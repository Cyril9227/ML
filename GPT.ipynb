{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "Largely inspired by [TinyTorch](https://mlsysbook.ai/tinytorch), the [associated course](https://mlsysbook.ai/assets/downloads/Machine-Learning-Systems.pdf), as well as this excellent [video serie from Karpathy](https://youtu.be/kCc8FmEb1nY) for his nano-gpt.\n",
    "\n",
    "Thanks Claude for the markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/harvard-edge/cs249r_book\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ac7aa-ffdd-4afa-8ce7-976a4a19cd07",
   "metadata": {},
   "source": [
    "To have a basic GPT model we need a bunch of components :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2ab6e-a29c-40c5-ae68-cbbdf00118e8",
   "metadata": {},
   "source": [
    "## Tokeniser\n",
    "\n",
    "There are tons of [different strategies](https://mlsysbook.ai/tinytorch/modules/10_tokenization_ABOUT.html), the simplest being a simple character lookup table from a known corpus.\n",
    "\n",
    "For our purpose, can just use one off the shelf if needed : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97936b3-e648-4cda-87d0-b249f5c102af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 995] ['Hello', 'Ġworld']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(\"GPT2\") \n",
    "encoded = tokenizer.encode(\"Hello world\")\n",
    "print(encoded.ids, encoded.tokens) # Ġ means that there is a space before this token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b27b7d-afa0-49fc-b41b-be306cf1be0f",
   "metadata": {},
   "source": [
    "## Embeddings \n",
    "\n",
    "Turns tokens ids into dense vectors carrying semantic information. Just a lookup table with randomly initialized vectors, that will be learnt as training goes. Different initializations schemes are available, the more popular are : \n",
    "\n",
    "#### Xavier Normal\n",
    "$$\n",
    "w \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}\\right)\n",
    "$$\n",
    "\n",
    "#### Xavier Uniform\n",
    "\n",
    "$$\n",
    "w \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
    "$$\n",
    "\n",
    "#### For Embeddings\n",
    "\n",
    "If you view the embedding layer as `embedd = one_hot @ W` then $n_{\\text{in}} = \\text{vocabsize}$ otherwise if you just view it as a lookup table then $n_{\\text{in}} = n_{\\text{out}} = n_{\\text{embed}}$ and the maths simplifies to : \n",
    "\n",
    "\n",
    "**Normal**: $w \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{\\text{embed}}}\\right)$\n",
    "\n",
    "**Uniform**: $w \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{3}{n_{\\text{embed}}}}, \\sqrt{\\frac{3}{n_{\\text{embed}}}}\\right)$\n",
    "\n",
    "Note : Can also directly use pretrained embeddings but it fell out of flavor for modern LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7698e80f-ef7c-47c4-8735-571e25140a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable embedding layer that maps token indices to dense vectors.\n",
    "\n",
    "    This is the fundamental building block for converting discrete tokens\n",
    "    into continuous representations that neural networks can process.\n",
    "\n",
    "    APPROACH:\n",
    "    1. Initialize embedding matrix with random weights (vocab_size, embed_dim)\n",
    "    2. Implement forward pass as matrix lookup using numpy indexing\n",
    "    3. Handle batch dimensions correctly\n",
    "    4. Return parameters for optimization\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> embed = Embedding(vocab_size=100, embed_dim=64)\n",
    "    >>> tokens = Tensor([[1, 2, 3], [4, 5, 6]])  # batch_size=2, seq_len=3\n",
    "    >>> output = embed.forward(tokens)\n",
    "    >>> print(output.shape)\n",
    "    (2, 3, 64)\n",
    "\n",
    "    HINTS:\n",
    "    - Use numpy advanced indexing for lookup: weight[indices]\n",
    "    - Embedding matrix shape: (vocab_size, embed_dim)\n",
    "    - Initialize with Xavier/Glorot uniform for stable gradients\n",
    "    - Handle multi-dimensional indices correctly\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, init: str = \"xavier_uniform\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        if init == \"xavier_uniform\":\n",
    "            # rand ~ U(0, 1) -> (b - a) * U(0, 1) ~ U(0, b - a) -> (b - a) * U(0, 1) + a ~ U(a, b)\n",
    "            rdm_uniform = torch.rand(vocab_size, embed_dim)\n",
    "            limit = math.sqrt(3 / embed_dim)\n",
    "            embeddings = 2 * limit * rdm_uniform - limit\n",
    "            \n",
    "        elif init == \"xavier_normal\":\n",
    "            # randn ~ N(0, 1) -> sqrt(a) * N(0, 1) + b ~ N(b, a)\n",
    "            rdm_normal = torch.randn(vocab_size, embed_dim)\n",
    "            limit = math.sqrt(1 / embed_dim)\n",
    "            embeddings = limit * rdm_normal\n",
    "            \n",
    "        elif init is None or init is False:\n",
    "            # N(0, 1) if not specified\n",
    "            embeddings = torch.randn(vocab_size, embed_dim)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Expected init in ['xavier_uniform', 'xavier_normal', None, False] but received {init}\")\n",
    "\n",
    "        self.embeddings = nn.Parameter(embeddings, requires_grad=True)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embeddings[tokens.long()]  # indexes must be int or bool.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f76e2-49bc-4d90-bac4-5fd59155cc55",
   "metadata": {},
   "source": [
    "## Positional Encodings\n",
    "\n",
    "Embeddings representing the position of the token in the sequence, as the position also carries meaning.\n",
    "\n",
    "#### Absolute encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7a2e71-1479-41be-8a1f-7446571a0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable positional encoding layer.\n",
    "\n",
    "    Adds trainable position-specific vectors to token embeddings,\n",
    "    allowing the model to learn positional patterns specific to the task.\n",
    "\n",
    "    APPROACH:\n",
    "    1. Create embedding matrix for positions: (max_seq_len, embed_dim)\n",
    "    2. Forward pass: lookup position embeddings and add to input\n",
    "    3. Handle different sequence lengths gracefully\n",
    "    4. Return parameters for training\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> pos_enc = PositionalEncoding(max_seq_len=512, embed_dim=64)\n",
    "    >>> embeddings = Tensor(np.random.randn(2, 10, 64))  # (batch, seq, embed)\n",
    "    >>> output = pos_enc.forward(embeddings)\n",
    "    >>> print(output.shape)\n",
    "    (2, 10, 64)  # Same shape, but now position-aware\n",
    "\n",
    "    HINTS:\n",
    "    - Position embeddings shape: (max_seq_len, embed_dim)\n",
    "    - Use slice [:seq_len] to handle variable lengths\n",
    "    - Add position encodings to input embeddings element-wise\n",
    "    - Initialize with smaller values than token embeddings (they're additive)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_seq_len: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        limit = math.sqrt(2 / embed_dim)  # smaller constant for inductive bias : embeddings > position\n",
    "        xavier_uniform = torch.empty(max_seq_len, embed_dim).uniform_(-limit, limit)  # idiomatic way to init\n",
    "        self.positional_encoding = nn.Parameter(xavier_uniform)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, embeddings, start_pos=0):\n",
    "        seq_len = embeddings.shape[1]\n",
    "        \n",
    "        # Check total length (current + history)\n",
    "        if start_pos + seq_len > self.max_seq_len:\n",
    "            raise ValueError(\n",
    "                f\"Sequence length {start_pos + seq_len} exceeds maximum {self.max_seq_len}\"\n",
    "            )\n",
    "            \n",
    "        # Add the SPECIFIC positions needed, not just 0 to seq_len\n",
    "        # We slice from start_pos to start_pos + current_length\n",
    "        # It is needed if we do K/V cache where the model only sees the last token and re-use cached matrixes\n",
    "        pos_slice = self.positional_encoding[start_pos : start_pos + seq_len]\n",
    "        \n",
    "        return embeddings + pos_slice\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc962692-efc6-4fce-b967-523f6b41b58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape : torch.Size([2, 3, 64]) | min : -0.215046226978302 - max 0.2109425812959671\n",
      "Positional Embeddings shape : torch.Size([2, 3, 64]) | min : -0.3751380443572998 - max 0.3606238067150116\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.Tensor([[1, 2, 3], [4, 5, 6]])  # 2 sequences of 3 tokens\n",
    "\n",
    "embed = EmbeddingLayer(vocab_size=100, embed_dim=64)\n",
    "embeddings = embed.forward(tokens)\n",
    "print(f\"Embeddings shape : {embeddings.shape} | min : {embeddings.min()} - max {embeddings.max()}\")\n",
    "\n",
    "pos = PositionalEncodingLayer(max_seq_len=1024, embed_dim=embeddings.shape[-1])\n",
    "pos_embeddings = pos.forward(embeddings, start_pos=0)\n",
    "\n",
    "print(f\"Positional Embeddings shape : {pos_embeddings.shape} | min : {pos_embeddings.min()} - max {pos_embeddings.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8030b7e-d352-4512-b824-4a6b7549e0e1",
   "metadata": {},
   "source": [
    "Absolute position encoding isn't used in practice because the encoding changes if you add tokens before the sentence and we're limited by max sequence length, making it hard to extrapolate to new seq lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b79ca-8fe9-44fa-a9d9-175308f7fd13",
   "metadata": {},
   "source": [
    "#### Relative encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff55d0-c2cf-4c8a-be2a-fb942e9a09d6",
   "metadata": {},
   "source": [
    "Modern systems use ROPE (or similar) encoding schemes (but not on the embeddings directly, here it's fine for the sake of the exercise). It encodes position as phase, token position scales the rotation angle, frequencies separate scales, and attention dot products convert phase differences into relative position awareness.\n",
    "\n",
    "- The first token is not rotated because position zero defines the reference frame.\n",
    "- Each embedding pair has a fixed angular frequency, shared across all tokens.\n",
    "- Tokens at later positions are rotated by an angle proportional to their position.\n",
    "- Different frequencies encode relative position at different spatial scales, and their combination allows robust relative positioning over long sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30566e4d-09ec-4421-a8d2-8b43dc75cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEncodingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Positional Encoding (RoPE) layer.\n",
    "\n",
    "    IMPORTANT CONCEPTUAL SHIFT:\n",
    "    ---------------------------\n",
    "    Unlike learned or sinusoidal positional encodings, RoPE:\n",
    "    - DOES NOT create a positional embedding matrix\n",
    "    - DOES NOT add anything to embeddings\n",
    "    - Instead, it *rotates* embedding vectors in a position-dependent way\n",
    "\n",
    "    RoPE encodes *relative position* by rotating pairs of embedding\n",
    "    dimensions using position-dependent angles.\n",
    "\n",
    "    This layer is typically applied to:\n",
    "    - Query (Q) and Key (K) tensors in attention\n",
    "    - NOT to token embeddings directly\n",
    "\n",
    "    However, for this exercise, we apply it to embeddings to understand\n",
    "    the mechanism in isolation.\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "\n",
    "    INPUT / OUTPUT SHAPE:\n",
    "    ---------------------\n",
    "    Input:\n",
    "        embeddings: Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    Output:\n",
    "        Tensor of same shape (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "\n",
    "    HIGH-LEVEL IDEA:\n",
    "    ----------------\n",
    "    1. Split embedding dimension into pairs:\n",
    "         (x_0, x_1), (x_2, x_3), ...\n",
    "\n",
    "    2. Each pair represents a 2D vector\n",
    "\n",
    "    3. For each position `pos`, rotate each 2D vector by an angle:\n",
    "         θ(pos, i) = pos / (base ** (2i / embed_dim))\n",
    "\n",
    "    4. Rotation is done via:\n",
    "         [cosθ  -sinθ]\n",
    "         [sinθ   cosθ]\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "\n",
    "    WHAT YOU NEED TO PRECOMPUTE:\n",
    "    ----------------------------\n",
    "    - A vector of inverse frequencies (inv_freq)\n",
    "        shape: (embed_dim // 2,)\n",
    "\n",
    "      Typical formula:\n",
    "        inv_freq[i] = 1 / (base ** (2i / embed_dim))\n",
    "\n",
    "      where base is usually 10000.\n",
    "\n",
    "    - Position indices:\n",
    "        pos = [0, 1, 2, ..., seq_len - 1]\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "\n",
    "    WHAT HAPPENS IN FORWARD():\n",
    "    --------------------------\n",
    "    1. Extract seq_len from embeddings\n",
    "    2. Create position indices [0..seq_len-1]\n",
    "    3. Compute angles = pos[:, None] * inv_freq[None, :]\n",
    "    4. Compute sin and cos of angles\n",
    "    5. Split embeddings into even / odd dimensions\n",
    "    6. Apply rotation:\n",
    "         x_even * cos - x_odd * sin\n",
    "         x_even * sin + x_odd * cos\n",
    "    7. Re-interleave dimensions\n",
    "    8. Return rotated embeddings\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "\n",
    "    KEY PROPERTIES OF RoPE:\n",
    "    -----------------------\n",
    "    - No learned parameters (usually)\n",
    "    - No max sequence length hard limit\n",
    "    - Encodes relative position naturally\n",
    "    - Enables extrapolation to longer sequences\n",
    "    - Position information survives dot products (Q·K)\n",
    "\n",
    "    EXAMPLE:\n",
    "    --------\n",
    "    >>> rope = PositionalEncodingLayer(embed_dim=64)\n",
    "    >>> x = torch.randn(2, 10, 64)\n",
    "    >>> y = rope(x)\n",
    "    >>> y.shape\n",
    "    torch.Size([2, 10, 64])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        if embed_dim % 2 != 0:\n",
    "            raise ValueError(f\"embed_dim must be divisible by 2 but received {embed_dim}\")\n",
    "        \n",
    "        i = torch.arange(0, embed_dim, 2)  # [0, 2, 4, ..., 62]\n",
    "        self.inv_freq = 1.0 / (base ** (i / embed_dim))\n",
    "        \n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply RoPE to embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Rotated embeddings: (batch_size, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = embeddings.shape\n",
    "        \n",
    "        # Step 1: Create position indices [0, 1, 2, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len)\n",
    "        \n",
    "        # Step 2: Compute rotation angles\n",
    "        # angles[pos, i] = pos * inv_freq[i]\n",
    "        angles = positions[:, None] * self.inv_freq[None, :]\n",
    "        \n",
    "        # Step 3: Compute sin and cos\n",
    "        cos_angles, sin_angles = torch.cos(angles), torch.sin(angles)  # (seq_len, embed_dim//2)\n",
    "        \n",
    "        # Step 4: Split embeddings into even and odd dimensions\n",
    "        x_even = embeddings[..., 0::2]  # (batch_size, seq_len, embed_dim//2)\n",
    "        x_odd = embeddings[..., 1::2]   # (batch_size, seq_len, embed_dim//2)\n",
    "        \n",
    "        # Step 5: Apply 2D rotation to each pair\n",
    "        # Rotation matrix: [cos  -sin]  applied to [x_even]\n",
    "        #                  [sin   cos]             [x_odd ]\n",
    "        x_even_rotated = x_even * cos_angles - x_odd * sin_angles\n",
    "        x_odd_rotated = x_even * sin_angles + x_odd * cos_angles\n",
    "        \n",
    "        # Step 6: Interleave back to original dimension order\n",
    "        # Stack along new dimension then flatten\n",
    "        x_out = torch.stack([x_even_rotated, x_odd_rotated], dim=-1)\n",
    "        x_out = x_out.flatten(-2)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1a06d8-de56-4702-9128-8affc9d2fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete embedding system combining token and positional embeddings.\n",
    "\n",
    "    This is the component that handles the full embedding\n",
    "    pipeline used in transformers and other sequence models.\n",
    "\n",
    "    APPROACH:\n",
    "    1. Combine token embedding + positional encoding\n",
    "    2. Support both learned and sinusoidal position encodings\n",
    "    3. Handle variable sequence lengths gracefully\n",
    "    4. Add optional embedding scaling (Transformer convention)\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> embed_layer = EmbeddingLayer(\n",
    "    ...     vocab_size=50000,\n",
    "    ...     embed_dim=512,\n",
    "    ...     max_seq_len=2048,\n",
    "    ...     pos_encoding='learned'\n",
    "    ... )\n",
    "    >>> tokens = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    >>> output = embed_layer.forward(tokens)\n",
    "    >>> print(output.shape)\n",
    "    (2, 3, 512)\n",
    "\n",
    "    HINTS:\n",
    "    - First apply token embedding, then add positional encoding\n",
    "    - Handle both 2D (batch, seq) and 1D (seq) inputs gracefully\n",
    "    - Scale embeddings by sqrt(embed_dim) if requested (transformer convention)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        max_seq_len: int = 512,\n",
    "        scale_embeddings: bool = False,\n",
    "        positional_embedding = \"absolute\"  # rotary\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.scale_embeddings = scale_embeddings\n",
    "        self.embedding = EmbeddingLayer(vocab_size, self.embed_dim)\n",
    "        if positional_embedding == \"absolute\":\n",
    "            self.positional_embedding = PositionalEncodingLayer(max_seq_len, self.embed_dim)\n",
    "        elif positional_embedding == \"rotary\":\n",
    "            self.positional_embedding = RotaryPositionalEncodingLayer(self.embed_dim, 10000)\n",
    "        elif positional_embedding is None or positional_embedding is False:\n",
    "            self.positional_embedding = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"positional_embedding only supports ['absolute', 'rotary', 'False', 'None'] but received {positional_embedding}\")\n",
    "\n",
    "    def forward(self, tokens, start_pos=0):\n",
    "        if len(tokens.shape) == 1:\n",
    "            tokens = tokens.unsqueeze(0)  # add batch dim\n",
    "\n",
    "        embeddings = self.embedding(tokens)\n",
    "        if self.scale_embeddings:\n",
    "            # transformer convention, helps with gradient stability\n",
    "            # and helps embedding dominate vs positions\n",
    "            embeddings *= math.sqrt(self.embed_dim) \n",
    "\n",
    "        return self.positional_embedding(embeddings, start_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed956c-ca46-408b-8cbb-3f4294fcc2e4",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "Now that we have a way to encode our words we can compute stuff\n",
    "\n",
    "### Core Intuition\n",
    "Attention allows each token to **selectively focus** on other tokens in the sequence. Instead of treating all positions equally, the model learns which positions are most relevant for processing each token.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given an input sequence of embeddings $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ where $n$ is sequence length and $d$ is embedding dimension:\n",
    "\n",
    "#### 1. **Linear Projections**\n",
    "Transform input into three representations:\n",
    "$$\\mathbf{Q} = \\mathbf{X}\\mathbf{W}^Q, \\quad \\mathbf{K} = \\mathbf{X}\\mathbf{W}^K, \\quad \\mathbf{V} = \\mathbf{X}\\mathbf{W}^V$$\n",
    "\n",
    "where $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{d \\times d_k}$ are learned weight matrices.\n",
    "\n",
    "- **Query** ($\\mathbf{Q}$): \"What am I looking for?\"\n",
    "- **Key** ($\\mathbf{K}$): \"What do I contain?\"\n",
    "- **Value** ($\\mathbf{V}$): \"What information do I carry?\"\n",
    "\n",
    "*Note : it's called self-attention because all 3 are computed from same input $X$*\n",
    "\n",
    "#### 2. **Attention Scores**\n",
    "Compute similarity between queries and keys:\n",
    "$$\\mathbf{S} = \\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\in \\mathbb{R}^{n \\times n}$$\n",
    "\n",
    "- Dot product measures compatibility between query $i$ and key $j$\n",
    "- Scaling by $\\sqrt{d_k}$ prevents extremely large values (stabilizes gradients)\n",
    "- Entry $S_{ij}$ = \"how much should token $i$ attend to token $j$?\"\n",
    "\n",
    "#### 3. **Attention Weights**\n",
    "Normalize scores into probabilities:\n",
    "$$\\mathbf{A} = \\text{softmax}(\\mathbf{S}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "where softmax is applied row-wise: $A_{ij} = \\frac{\\exp(S_{ij})}{\\sum_{k=1}^{n} \\exp(S_{ik})}$\n",
    "\n",
    "- Each row sums to 1: $\\sum_{j=1}^{n} A_{ij} = 1$\n",
    "- $A_{ij}$ = probability that token $i$ attends to token $j$\n",
    "\n",
    "#### 4. **Weighted Sum**\n",
    "Aggregate values using attention weights:\n",
    "$$\\mathbf{Z} = \\mathbf{A}\\mathbf{V} \\in \\mathbb{R}^{n \\times d_k}$$\n",
    "\n",
    "Each output $\\mathbf{z}_i$ is a weighted combination: $\\mathbf{z}_i = \\sum_{j=1}^{n} A_{ij} \\mathbf{v}_j$\n",
    "\n",
    "### Complete Formula (Scaled Dot-Product Attention)\n",
    "\n",
    "$$\\boxed{\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}}$$\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Split features into bags and run $h$ attention operations in parallel with different learned projections (1 attention per bag of features):\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(\\mathbf{X}\\mathbf{W}^Q_i, \\mathbf{X}\\mathbf{W}^K_i, \\mathbf{X}\\mathbf{W}^V_i)$$\n",
    "\n",
    "$$\\text{MultiHead}(\\mathbf{X}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}^O$$\n",
    "\n",
    "where $\\mathbf{W}^O \\in \\mathbb{R}^{hd_k \\times d}$ projects concatenated heads back to model dimension.\n",
    "\n",
    "**Benefit**: Each head can learn different attention patterns (e.g., syntax, semantics, positional relationships).\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- **Permutation Equivariant**: Without positional encodings, shuffling input tokens shuffles output identically\n",
    "- **Parallelizable**: All positions computed simultaneously (unlike RNNs)\n",
    "- **Long-Range Dependencies**: Any token can directly attend to any other token in $O(1)$ operations\n",
    "- **Complexity**: $O(n^2 d)$ time and $O(n^2)$ memory for sequence length $n$\n",
    "\n",
    "\n",
    "### Differentiable Hash-table (python dict)\n",
    "\n",
    "Can also view this attention mechanism as a \"soft\" python dictionary where the key doesn't need to match exactly and the dictionary returns a weighted blend of multiple values\n",
    "\n",
    "\n",
    "### Causal mask\n",
    "- Prevents each token from attending to future tokens by masking out positions to the right in the attention matrix.\n",
    "- It enforces autoregressive behavior: the model can only use past and present information when predicting the next token (typical for text generation).\n",
    "- Without it the model would “cheat” during training by seeing the future and would fail at generation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7a5fb-cea4-4dfa-a9f9-e1db1b46b7e7",
   "metadata": {},
   "source": [
    "One last thing, we use these kind of models for auto-regressive tasks (text completion etc.) which has redundant computations hence :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26031330-cfce-48e9-9ab6-6895895d6415",
   "metadata": {},
   "source": [
    "## K/V Cache in Autoregressive Inference\n",
    "\n",
    "### The Problem: Redundant Computation\n",
    "\n",
    "In autoregressive generation, we produce tokens sequentially where each new token depends on all previous tokens:\n",
    "\n",
    "$$p(x_t | x_1, x_2, \\ldots, x_{t-1})$$\n",
    "\n",
    "**Without caching**, at each step $t$ we recompute attention for *all* previous tokens:\n",
    "```\n",
    "Step 1: Compute attention for [x₁]\n",
    "Step 2: Compute attention for [x₁, x₂]           ← recomputes x₁\n",
    "Step 3: Compute attention for [x₁, x₂, x₃]       ← recomputes x₁, x₂\n",
    "Step 4: Compute attention for [x₁, x₂, x₃, x₄]   ← recomputes x₁, x₂, x₃\n",
    "```\n",
    "\n",
    "**Cost**: $O(n^2)$ operations per new token for sequence length $n$.\n",
    "\n",
    "**Key insight**: Once computed, $K$ and $V$ for token $x_i$ never change. Only $Q$ depends on the current token.\n",
    "\n",
    "### How K/V Cache Works\n",
    "\n",
    "**Step 1** (process prompt \"Hello world\"):\n",
    "```python\n",
    "Q = [Q_hello, Q_world]  # Queries for both tokens\n",
    "K = [K_hello, K_world]  # Compute and CACHE\n",
    "V = [V_hello, V_world]  # Compute and CACHE\n",
    "\n",
    "Output = Attention(Q, K, V)  # Use last position for generation\n",
    "```\n",
    "\n",
    "**Step 2** (generate \"!\"):\n",
    "```python\n",
    "Q = [Q_!]                                    # Only new query\n",
    "K = [K_hello, K_world, K_!]                 # Cached + new\n",
    "V = [V_hello, V_world, V_!]                 # Cached + new\n",
    "    └─────cached─────┘  └new┘\n",
    "\n",
    "Output = Attention(Q, K, V)  # Single query × all keys\n",
    "```\n",
    "\n",
    "**Step 3** (generate \"How\"):\n",
    "```python\n",
    "Q = [Q_how]                                      # Only new query\n",
    "K = [K_hello, K_world, K_!, K_how]              # Cached + new\n",
    "V = [V_hello, V_world, V_!, V_how]              # Cached + new\n",
    "    └──────────cached──────────┘  └new┘\n",
    "```\n",
    "\n",
    "### Computational Savings\n",
    "\n",
    "| Metric | Without Cache | With Cache |\n",
    "|--------|--------------|------------|\n",
    "| **Projections per step** | $O(n)$ tokens | $O(1)$ token |\n",
    "| **Attention scores** | $n \\times n$ matrix | $1 \\times n$ vector |\n",
    "| **Memory** | $O(n \\cdot d)$ | $O(n \\cdot d)$ |\n",
    "| **Time per token** | $O(n^2 \\cdot d)$ | $O(n \\cdot d)$ |\n",
    "\n",
    "For a 2000-token sequence, this means **2000× fewer projection operations** per generated token.\n",
    "\n",
    "### Implementation Pattern\n",
    "```python\n",
    "# Initialize cache\n",
    "cache = {\"K\": None, \"V\": None}\n",
    "\n",
    "# First pass: process prompt\n",
    "Q, K, V = compute_projections(prompt)\n",
    "cache = {\"K\": K, \"V\": V}\n",
    "output = attention(Q, K, V)\n",
    "\n",
    "# Generate tokens\n",
    "for step in range(max_tokens):\n",
    "    Q_new = compute_query(new_token)\n",
    "    K_new = compute_key(new_token)\n",
    "    V_new = compute_value(new_token)\n",
    "    \n",
    "    # Concatenate with cache\n",
    "    K_full = concat(cache[\"K\"], K_new)\n",
    "    V_full = concat(cache[\"V\"], V_new)\n",
    "    \n",
    "    # Update cache\n",
    "    cache = {\"K\": K_full, \"V\": V_full}\n",
    "    \n",
    "    # Attention with single query\n",
    "    output = attention(Q_new, K_full, V_full)\n",
    "```\n",
    "\n",
    "### Why No Causal Mask with Cache?\n",
    "\n",
    "**Training/First pass** ($N$ queries × $N$ keys):\n",
    "$$\\begin{bmatrix}\n",
    "q_1 \\\\ q_2 \\\\ q_3\n",
    "\\end{bmatrix} \\times \n",
    "\\begin{bmatrix}\n",
    "k_1 & k_2 & k_3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "✓ & ✗ & ✗ \\\\\n",
    "✓ & ✓ & ✗ \\\\\n",
    "✓ & ✓ & ✓\n",
    "\\end{bmatrix} \\leftarrow \\text{mask upper triangle}$$\n",
    "\n",
    "**Cached inference** (1 query × $N$ keys):\n",
    "$$\\begin{bmatrix}\n",
    "q_{\\text{new}}\n",
    "\\end{bmatrix} \\times \n",
    "\\begin{bmatrix}\n",
    "k_1 & k_2 & k_3 & k_{\\text{new}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "✓ & ✓ & ✓ & ✓\n",
    "\\end{bmatrix} \\leftarrow \\text{no future keys to mask}$$\n",
    "\n",
    "The single query can attend to all keys (all are from past or current position).\n",
    "\n",
    "\n",
    "### Why Not Caching Q ?\n",
    "\n",
    "If we understand attention as a dictionary it's immediately obvious that we don't need previous queries to find the key / values that match the query of the current token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d37a8c-c868-440b-89a7-c2cbe663ac63",
   "metadata": {},
   "source": [
    "tl;dr : \n",
    "\n",
    "![attention](ressources/cached_attention.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism.\n",
    "\n",
    "    Runs multiple attention heads in parallel, each learning different relationships.\n",
    "    \n",
    "    APPROACH:\n",
    "        1. Validate that embed_dim is divisible by num_heads\n",
    "        2. Calculate head_dim (embed_dim // num_heads)\n",
    "        3. Create linear layers for Q, K, V projections\n",
    "        4. Create output projection layer\n",
    "        5. Store configuration parameters\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Embedding dimension (d_model)\n",
    "            num_heads: Number of parallel attention heads\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> mha = MultiHeadAttention(embed_dim=512, num_heads=8)\n",
    "        >>> mha.head_dim  # 64 (512 / 8)\n",
    "        >>> len(mha.parameters())  # 4 linear layers * 2 params each = 8 tensors\n",
    "\n",
    "        HINTS:\n",
    "        - head_dim = embed_dim // num_heads must be integer\n",
    "        - Need 4 Linear layers: q_proj, k_proj, v_proj, out_proj\n",
    "        - Each projection maps embed_dim → embed_dim\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, causal: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Validate configuration\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\"\n",
    "            )\n",
    "        \n",
    "        # 2. Store parameters\n",
    "        self.causal = causal  # tokens can only attend to themselves or past tokens, no cheating !\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # 3. Linear projections for Q, K, V\n",
    "        # Each maps: (B, N, embed_dim) → (B, N, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        # 4. Output projection (after concatenating heads)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, k_v_cache=None):\n",
    "        B, N, _ = x.shape\n",
    "        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n",
    "    \n",
    "        if using_cache:\n",
    "            # inference: we only query the last token\n",
    "            # because the other queries have already been used and are irrelevant now\n",
    "            Q = self.q_proj(x[:, -1:, :])\n",
    "        else:\n",
    "            # training: full sequence\n",
    "            Q = self.q_proj(x)\n",
    "\n",
    "        if using_cache:\n",
    "            # only project new token\n",
    "            K_new = self.k_proj(x[:, -1:, :])\n",
    "            V_new = self.v_proj(x[:, -1:, :])\n",
    "\n",
    "            # append to projections we previously computer\n",
    "            K = torch.cat([k_v_cache[\"K\"], K_new], dim=1)\n",
    "            V = torch.cat([k_v_cache[\"V\"], V_new], dim=1)\n",
    "        else:\n",
    "            # training project everything\n",
    "            # inference project the initial sequence of an auto-regressive pass\n",
    "            K = self.k_proj(x)\n",
    "            V = self.v_proj(x)\n",
    "\n",
    "        # cache is never provided during training\n",
    "        if k_v_cache is not None:\n",
    "            k_v_cache[\"K\"] = K\n",
    "            k_v_cache[\"V\"] = V\n",
    "    \n",
    "        # split our features in num_heads chunks of head_dim features (embed dim // # heads)\n",
    "        # each head has its own Q/K/V projections and full softmax budget\n",
    "        # allowing different heads to attend to different subspaces, relationships, semantic patterns etc. in //\n",
    "        Q = Q.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2) \n",
    "        K = K.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # compute similarity scores for current query\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.head_dim)\n",
    "    \n",
    "        # during cached inference there is nothing to mask (single vector for current query)\n",
    "        # during training or non-cached inference we don't want tokens to attend to future (unseen) tokens\n",
    "        if self.causal and not using_cache:\n",
    "            mask = torch.triu(\n",
    "                torch.ones(N, N, device=x.device),\n",
    "                diagonal=1\n",
    "            ).bool()\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # final attention for current query\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        output = attn @ V\n",
    "        \n",
    "        output = (\n",
    "            output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(B, -1, self.embed_dim)\n",
    "        )\n",
    "        return self.out_proj(output), k_v_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a4b0d-1cce-41f2-a917-952d9b9a15a2",
   "metadata": {},
   "source": [
    "## Layer Normalization vs Batch Normalization\n",
    "\n",
    "### Why Layer Norm for Transformers?\n",
    "\n",
    "#### Normalization Axis Comparison\n",
    "\n",
    "**Batch Normalization**: Normalizes across the **batch dimension**\n",
    "$$\\mu_j = \\frac{1}{B}\\sum_{i=1}^{B} x_{ij}, \\quad \\sigma_j^2 = \\frac{1}{B}\\sum_{i=1}^{B} (x_{ij} - \\mu_j)^2$$\n",
    "\n",
    "For input shape `(B, N, D)` → computes statistics over `B` (batch) for each feature `j`\n",
    "\n",
    "**Layer Normalization**: Normalizes across the **feature dimension**\n",
    "$$\\mu_i = \\frac{1}{D}\\sum_{j=1}^{D} x_{ij}, \\quad \\sigma_i^2 = \\frac{1}{D}\\sum_{j=1}^{D} (x_{ij} - \\mu_i)^2$$\n",
    "\n",
    "For input shape `(B, N, D)` → computes statistics over `D` (features) for each sample `i`\n",
    "\n",
    "#### Visual Comparison\n",
    "```\n",
    "Input shape: (Batch=2, SeqLen=3, Features=4)\n",
    "\n",
    "Batch Norm:           Layer Norm:\n",
    "[[ 1  2  3  4]        [[ 1  2  3  4]\n",
    " [ 5  6  7  8]         [ 5  6  7  8]\n",
    " [ 9 10 11 12]         [ 9 10 11 12]\n",
    " [13 14 15 16]         [13 14 15 16]\n",
    " [17 18 19 20]         [17 18 19 20]\n",
    " [21 22 23 24]]        [21 22 23 24]]\n",
    " \n",
    " ↓ Normalize           → Normalize\n",
    " per column            per row\n",
    "```\n",
    "\n",
    "### Why Layer Norm Wins for Transformers\n",
    "\n",
    "| Issue | Batch Norm Problem | Layer Norm Solution |\n",
    "|-------|-------------------|---------------------|\n",
    "| **Variable Sequence Lengths** | Statistics change with padding | Each sequence normalized independently |\n",
    "| **Small Batch Sizes** | Unstable statistics (mean/var unreliable) | Statistics computed per sample |\n",
    "| **Train/Test Discrepancy** | Needs running statistics, different behavior | Same computation always |\n",
    "| **Sequential Processing** | Can't normalize one sample at inference | Works naturally for online inference |\n",
    "\n",
    "#### Key Insight\n",
    "> Transformers process **variable-length sequences** with **self-attention**. Each token's representation should be normalized based on **its own feature distribution**, not compared to other samples in the batch.\n",
    "\n",
    "### Biased vs Unbiased Variance\n",
    "\n",
    "#### The Math\n",
    "\n",
    "**Biased (Population) Variance**: $\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\mu)^2$\n",
    "\n",
    "**Unbiased (Sample) Variance**: $s^2 = \\frac{1}{N-1}\\sum_{i=1}^{N}(x_i - \\mu)^2$\n",
    "\n",
    "#### PyTorch Uses Biased Variance (`unbiased=False`)\n",
    "\n",
    "**Ratio**: $\\frac{\\sigma_{\\text{unbiased}}^2}{\\sigma_{\\text{biased}}^2} = \\frac{D}{D-1} = \\frac{512}{511} \\approx 1.002$\n",
    "\n",
    "**Practical Impact**:\n",
    "- For **large feature dimensions** (D=512, 768, 1024): difference is **<0.2%** → negligible\n",
    "- Biased estimator is the **population parameter** we actually want to normalize by\n",
    "- Using `unbiased=True` would slightly **over-normalize** (divide by slightly larger σ)\n",
    "\n",
    "#### Why Use Biased?\n",
    "\n",
    "1. **Consistency**: We're normalizing the **current data**, not estimating a population parameter\n",
    "2. **Stability**: Dividing by N (not N-1) is more stable when N is small\n",
    "3. **Standard Practice**: PyTorch's `nn.LayerNorm` uses biased variance\n",
    "4. **Negligible Difference**: For D ≫ 1, the correction factor D/(D-1) ≈ 1\n",
    "\n",
    "\n",
    "### Complete LayerNorm Formula\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "where:\n",
    "- $\\mu = \\frac{1}{D}\\sum_{j=1}^{D} x_j$ (mean over features)\n",
    "- $\\sigma^2 = \\frac{1}{D}\\sum_{j=1}^{D} (x_j - \\mu)^2$ (biased variance over features)\n",
    "- $\\gamma, \\beta$ are learnable parameters (initialized to 1 and 0)\n",
    "- $\\epsilon$ small value to prevent division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c83ee5-2a72-4c62-93a0-3a3c0773ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization for transformer blocks.\n",
    "\n",
    "    Normalizes across the feature dimension (last axis) for each sample independently,\n",
    "    unlike batch normalization which normalizes across the batch dimension.\n",
    "\n",
    "    Mathematical Formula:\n",
    "    output = (x - μ) / σ * γ + β\n",
    "    \n",
    "    where:\n",
    "      μ = mean(x, axis=features)     # Mean across feature dimension\n",
    "      σ = sqrt(var(x) + ε)          # Standard deviation + small epsilon\n",
    "      γ = learnable scale parameter  # Initialized to 1.0\n",
    "      β = learnable shift parameter  # Initialized to 0.0\n",
    "      \n",
    "    Why Layer Norm Works:\n",
    "    \n",
    "    Independence: Each sample normalized independently (good for variable batch sizes)\n",
    "    Stability: Prevents internal covariate shift that breaks training\n",
    "    Gradient Flow: Helps gradients flow better through deep networks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        \"\"\"\n",
    "        Initialize LayerNorm with learnable parameters.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Store the shape to normalize over (usually embed_dim)\n",
    "        2. Initialize learnable scale (gamma) and shift (beta) parameters\n",
    "        3. Set small epsilon for numerical stability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "\n",
    "        # Learnable parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply layer normalization.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Compute mean and variance across the last dimension\n",
    "        2. Normalize: (x - mean) / sqrt(variance + eps)\n",
    "        3. Apply learnable scale and shift: gamma * normalized + beta\n",
    "        \"\"\"\n",
    "\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        # var() divides by N-1 but torch implementation uses population var i.e divides by N\n",
    "        std = torch.sqrt(x.var(-1, keepdim=True, unbiased=False) + self.eps) # <=> np.sqrt(((x - mean) ** 2).mean(-1, keepdim=True))\n",
    "        x = (x - mean) / std\n",
    "        return x * self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065d472-f3eb-4ecd-946d-e83bdc21a83c",
   "metadata": {},
   "source": [
    "## MLP block\n",
    "\n",
    "### Attention\n",
    "Mixes information across tokens. Each token dynamically selects which other tokens to read from, based on content. This enables context, dependency modeling, and information routing over the sequence.\n",
    "\n",
    "### MLP (Feed-Forward Network)\n",
    "Mixes information within a token. It non-linearly recombines and transforms features of each token independently, increasing representational power.\n",
    "\n",
    "### Why both\n",
    "Attention provides communication between tokens; the MLP provides computation on token features.\n",
    "Without attention, tokens can’t interact. Without MLPs, the model is mostly linear and weak.\n",
    "\n",
    "#### Why GELU instead of ReLU?\n",
    "\n",
    "```\n",
    "# ReLU: Hard cutoff at 0\n",
    "relu(x) = max(0, x)\n",
    "\n",
    "# GELU: Smooth, probabilistic gating\n",
    "gelu(x) = x * Φ(x)  # Φ is standard normal CDF\n",
    "```\n",
    "*Benefits of GELU:*\n",
    "\n",
    "- Smoother gradients (no sharp corner at x=0)\n",
    "- Better performance in language models (empirically)\n",
    "- Can be thought of as \"stochastic regularization\"\n",
    "\n",
    "#### Why 4x expansion ? \n",
    "\n",
    "2x too small, 8x too slow, 4x works well\n",
    "\n",
    "\n",
    "#### Why No Activation After Second Layer?\n",
    "\n",
    "```\n",
    "# Transformer block structure:\n",
    "output = x + MLP(LayerNorm(x))  # Residual connection\n",
    "        ↑\n",
    "        └─ Need to preserve residual path\n",
    "```\n",
    "\n",
    "Adding GELU after linear2, would force all outputs to be non-negative, which:\n",
    "\n",
    "- Limits expressiveness of the residual stream\n",
    "- Can cause gradient flow issues\n",
    "- Breaks the \"smooth information highway\" of residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf4ba2d-402d-4eab-86e1-754f9faeaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (Feed-Forward Network) for transformer blocks.\n",
    "\n",
    "    Standard pattern: Linear -> GELU -> Linear with expansion ratio of 4:1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim=None, dropout_prob=0.1):\n",
    "        \"\"\"\n",
    "        Initialize MLP with two linear layers.\n",
    "\n",
    "        APPROACH:\n",
    "        1. First layer expands from embed_dim to hidden_dim (usually 4x larger)\n",
    "        2. Second layer projects back to embed_dim\n",
    "        3. Use GELU activation (smoother than ReLU, preferred in transformers)\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> mlp = MLP(512)  # Will create 512 -> 2048 -> 512 network\n",
    "        >>> x = Tensor(np.random.randn(2, 10, 512))\n",
    "        >>> output = mlp.forward(x)\n",
    "        >>> assert output.shape == (2, 10, 512)\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * embed_dim  # Standard transformer expansion\n",
    "\n",
    "        # params\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # actual layers, bias on by default, some architectures turn it off here\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.linear1(x))\n",
    "        x = self.linear2(x)  # No GELU here to not mess with residual paths x + MLP(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19c8d0-16aa-4cc0-b8f5-9e43d30bec58",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "Now that we have each component, we can code the actual computation block.\n",
    "\n",
    "Tl;dr :\n",
    "\n",
    "#### Transformer block\n",
    "Applies self-attention followed by an MLP, each wrapped with residual connections (and normalization).\n",
    "\n",
    "- Self-attention: lets tokens exchange information across the sequence.\n",
    "- MLP: non-linearly transforms each token’s features.\n",
    "- Residual connections: preserve information and stabilize training.\n",
    "\n",
    "Attention enables communication, the MLP enables computation, and residuals allow stacking many layers without losing signal.\n",
    "\n",
    "#### Why pre-norm\n",
    "Empirically found in a paper to be better for stability than applying LayerNorm after.\n",
    "\n",
    "#### Why stack many blocks\n",
    "Each block performs a small amount of communication (attention) and computation (MLP). Stacking many blocks lets the model build increasingly abstract, long-range, and hierarchical representations. One block is shallow; depth gives expressive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c04d073-6e3f-469d-98b4-5cda92115923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Block with self-attention, MLP, and residual connections.\n",
    "\n",
    "    This is the core building block of GPT and other transformer models.\n",
    "    Each block processes the input sequence and passes it to the next block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout_prob=0.1, causal=True):\n",
    "        \"\"\"\n",
    "        Initialize a complete transformer block.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Multi-head self-attention for sequence modeling\n",
    "        2. First layer normalization (pre-norm architecture)\n",
    "        3. MLP with specified expansion ratio\n",
    "        4. Second layer normalization\n",
    "\n",
    "        TRANSFORMER BLOCK ARCHITECTURE:\n",
    "        x → LayerNorm → MultiHeadAttention → + (residual) →\n",
    "            LayerNorm → MLP → + (residual) → output\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> block = TransformerBlock(embed_dim=512, num_heads=8)\n",
    "        >>> x = Tensor(np.random.randn(2, 10, 512))  # (batch, seq, embed)\n",
    "        >>> output = block.forward(x)\n",
    "        >>> assert output.shape == (2, 10, 512)\n",
    "\n",
    "        HINT: We use pre-norm architecture (LayerNorm before attention/MLP)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm1 = LayerNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, causal)  # causal = masking out tokens\n",
    "        self.layernorm2 = LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio * embed_dim, dropout_prob)\n",
    "\n",
    "    def forward(self, x, cache=None):\n",
    "        \"\"\"\n",
    "        Forward pass through transformer block.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Apply layer norm, then self-attention, then add residual\n",
    "        2. Apply layer norm, then MLP, then add residual\n",
    "        3. Return the transformed sequence\n",
    "\n",
    "        COMPUTATION FLOW:\n",
    "        x → ln1 → attention → + x → ln2 → mlp → + → output\n",
    "\n",
    "        RESIDUAL CONNECTIONS:\n",
    "        These are crucial for training deep networks - they allow gradients\n",
    "        to flow directly through the network during backpropagation.\n",
    "\n",
    "        HINT: \n",
    "        - Store intermediate results to add residual connections properly\n",
    "        - Don't forget masking\n",
    "        \"\"\"\n",
    "        x1 = self.layernorm1(x)\n",
    "        x2, cache = self.mha(x1, cache)  # will be used when generating tokens during inference\n",
    "        x2 = x2 + x  # residual path\n",
    "\n",
    "        x3 = self.layernorm2(x2)\n",
    "        x3 = self.mlp(x3) + x2  # residual path\n",
    "        return x3, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97cdca-b682-43ae-a24d-f08ab84b0673",
   "metadata": {},
   "source": [
    "# Putting it all together : GPT\n",
    "\n",
    "Words -> tokens -> embeddings + position -> stack of transformer blocks -> prediction layer\n",
    "\n",
    "One thing worth mentioning is the weight tying between the embedding table and the final linear layer. It is explicitely asked in the docstring but not actually implemented in the [official solution](https://hub.2i2c.mybinder.org/user/harvard-edge-cs249r_book-2qly0zim/doc/tree/tinytorch/modules/13_transformers/13_transformers.ipynb), however it is present in [nano-gpt](https://github.com/karpathy/nanoGPT/blob/3adf61e154c3fe3fca428ad6bc3818b27a3b8291/model.py#L138)\n",
    "\n",
    "I think it makes sense, you want the embedding table to perform token id -> embedding but also the reverse operation (if you transpose it) ie vector living in same semantic space -> ~~~token id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff39a6-d4bd-45e9-aeb9-ad57e7345221",
   "metadata": {},
   "source": [
    "### Temperature & Top-K Sampling (Intuition + Math)\n",
    "\n",
    "Let the model output logits $ z \\in \\mathbb{R}^{V}$ over a vocabulary of size $ V $.\n",
    "\n",
    "---\n",
    "\n",
    "#### Temperature Sampling\n",
    "\n",
    "We convert logits to probabilities using a temperature $ T > 0$:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{z_i / T}}{\\sum_{j=1}^{V} e^{z_j / T}}\n",
    "$$\n",
    "\n",
    "**Effect of temperature**\n",
    "- $T \\downarrow$ → sharper distribution → more deterministic\n",
    "- $T \\uparrow$ → flatter distribution → more randomness\n",
    "- Token ranking is **unchanged** (argmax stays the same)\n",
    "\n",
    "**Intuition**  \n",
    "Temperature rescales confidence:\n",
    "- Low $T$: model is confident, samples top token almost always\n",
    "- High $T$: model hesitates, alternatives become more likely\n",
    "\n",
    "Special cases:\n",
    "- $T \\to 0$: greedy decoding (argmax)\n",
    "- $T = 1$: normal softmax\n",
    "- $T \\to \\infty$: uniform distribution\n",
    "\n",
    "---\n",
    "\n",
    "#### Top-K Sampling\n",
    "\n",
    "Instead of sampling from all $V$ tokens, restrict to the top $K$ logits.\n",
    "\n",
    "Let:\n",
    "$$\n",
    "\\mathcal{K} = \\text{indices of the top-}K\\text{ logits}\n",
    "$$\n",
    "We mask all others:\n",
    "$$\n",
    "z_i' =\n",
    "\\begin{cases}\n",
    "z_i & i \\in \\mathcal{K} \\\\\n",
    "-\\infty & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "Then apply softmax:\n",
    "$$\n",
    "p_i = \\frac{e^{z_i' / T}}{\\sum_j e^{z_j' / T}}\n",
    "$$\n",
    "**Effect of Top-K**\n",
    "- Prevents low-probability garbage tokens\n",
    "- Keeps sampling focused on plausible continuations\n",
    "- Reduces entropy *by truncation*\n",
    "\n",
    "---\n",
    "\n",
    "#### Combined Intuition\n",
    "\n",
    "- **Temperature** controls *how much* we explore\n",
    "- **Top-K** controls *where* we are allowed to explore\n",
    "\n",
    "In practice:\n",
    "- Temperature flattens the distribution\n",
    "- Top-K cuts the tail\n",
    "- Together they give controlled, non-degenerate sampling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT (Generative Pre-trained Transformer) model.\n",
    "\n",
    "    This combines embeddings, positional encoding, multiple transformer blocks,\n",
    "    and a language modeling head for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, max_seq_len=1024):\n",
    "        \"\"\"\n",
    "        Initialize complete GPT model.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Token embedding layer to convert tokens to vectors\n",
    "        2. Positional embedding to add position information\n",
    "        3. Stack of transformer blocks (the main computation)\n",
    "        4. Final layer norm and language modeling head\n",
    "\n",
    "        GPT ARCHITECTURE:\n",
    "        tokens → embedding → + pos_embedding →\n",
    "                transformer_blocks → layer_norm → lm_head → logits\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> model = GPT(vocab_size=1000, embed_dim=256, num_layers=6, num_heads=8)\n",
    "        >>> tokens = Tensor(np.random.randint(0, 1000, (2, 10)))  # (batch, seq)\n",
    "        >>> logits = model.forward(tokens)\n",
    "        >>> assert logits.shape == (2, 10, 1000)  # (batch, seq, vocab)\n",
    "\n",
    "        HINTS:\n",
    "        - Positional embeddings are learned, not fixed sinusoidal\n",
    "        - Final layer norm stabilizes training\n",
    "        - Language modeling head shares weights with token embedding (tie_weights)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.embedding = Embedding(\n",
    "            vocab_size=self.vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            max_seq_len=self.max_seq_len,\n",
    "            scale_embeddings=True,\n",
    "            positional_embedding=\"absolute\"       \n",
    "        )\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.ln = LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        # mental naming lol\n",
    "        # but basically you want the embedding table to perform both operations words -> embed and embed -> prediction\n",
    "        # so we jointly learn last and first layer\n",
    "        # bias = false, to match the lookup table structure\n",
    "        self.embedding.embedding.embeddings = self.lm_head.weight\n",
    "       \n",
    "    def forward(self, tokens):\n",
    "        x = self.embedding(tokens)  # tokens -> emb + pos enc\n",
    "        for b in self.blocks:\n",
    "            x, _ = b(x)  # iteratively refines features from initial embeddings\n",
    "        features = self.ln(x)  # normalized to stabilize training\n",
    "        return self.lm_head(features)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 prompt_tokens,\n",
    "                 max_new_tokens=50,\n",
    "                 temperature=1.0,\n",
    "                 use_cache=True,\n",
    "                 use_top_k=False,\n",
    "                ):\n",
    "        self.eval()\n",
    "\n",
    "        tokens_out = prompt_tokens.clone()\n",
    "        current_tokens = prompt_tokens.clone()\n",
    "        cache = [{} if use_cache else None for _ in range(len(self.blocks))]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # position aware wrt to full sequence\n",
    "            start_pos = tokens_out.shape[1] - current_tokens.shape[1] if use_cache else 0\n",
    "            x = self.embedding(current_tokens, start_pos)\n",
    "            \n",
    "            for i, b in enumerate(self.blocks):\n",
    "                x, c_i = b(x, cache[i])\n",
    "                cache[i] = c_i\n",
    "            \n",
    "            features = self.ln(x)\n",
    "            logits = self.lm_head(features)\n",
    "                    \n",
    "            last_logits = logits[:, -1, :]\n",
    "    \n",
    "            if temperature > 0:\n",
    "                scaled_logits = last_logits / temperature\n",
    "                # Only sample from top k tokens to avoid garbage prediction derailing whole prediction\n",
    "                # We don't simply take max prob token to allow \"creativity\"\n",
    "                if use_top_k:\n",
    "                    # heuristic that is ok for toy project\n",
    "                    # most of probability mass in on a small amount of tokens\n",
    "                    k = min(max(5, int(0.01 * self.vocab_size)), 100)\n",
    "                    values, indices = torch.topk(scaled_logits, k)\n",
    "                    scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n",
    "                    scaled_logits.scatter_(1, indices, values)\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy decoding if temp is 0 (prevents division by zero)\n",
    "                next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
    "    \n",
    "            tokens_out = torch.cat([tokens_out, next_token], dim=1)\n",
    "\n",
    "            # If caching, we only need to feed the newest token next time, otherwise full sequence\n",
    "            current_tokens = next_token if use_cache else tokens_out\n",
    "       \n",
    "        return tokens_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78396c3-b63a-41f4-952a-8ebce18b74c0",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6d4dc62-d50c-4a53-a8f1-66e1203ab024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 4, 1000]) True\n",
      "tensor(0., grad_fn=<MeanBackward0>)\n",
      "tensor([[1, 2, 3, 4, 4, 4, 4, 4, 4]])\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT(vocab_size=1000, embed_dim=256, num_layers=6, num_heads=8)\n",
    "gpt.eval()\n",
    "\n",
    "tokens = torch.tensor([[1, 2, 3, 4]])\n",
    "logits = gpt(tokens)\n",
    "print(tokens.shape, logits.shape, gpt.lm_head.weight is gpt.embedding.embedding.embeddings)\n",
    "\n",
    "\n",
    "tokens2 = torch.tensor([[1, 2, 999, 4]])\n",
    "logits2 = gpt(tokens2)\n",
    "\n",
    "# need to be identical\n",
    "print(torch.abs(logits[:, :2] - logits2[:, :2]).mean())\n",
    "\n",
    "out = gpt.generate(tokens, 5, use_cache=False, temperature=0.)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec6e7e-9bce-467f-9c36-aadb01c2ec5f",
   "metadata": {},
   "source": [
    "## Full Training on Wikipedia\n",
    "\n",
    "Note : we re-use the tokenizer defined at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04a17c6-e96f-4943-827d-649214ae44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CONFIG #####\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "block_size = 256\n",
    "batch_size = 32\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b92cd5fa-1da3-4e79-8760-6c9faadd9516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size: 15068495 tokens\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_list = [\n",
    "    load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\"),\n",
    "    load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\"),\n",
    "    load_dataset(\"tiny_shakespeare\", split=\"train\"),\n",
    "    load_dataset(\"tiny_shakespeare\", split=\"validation\"),\n",
    "    load_dataset(\"bookcorpus\", split=\"train[:1%]\"),\n",
    "]\n",
    "\n",
    "\n",
    "all_text = []\n",
    "for ds in ds_list:\n",
    "    all_text.extend([t for t in ds[\"text\"] if t.strip()])\n",
    "\n",
    "\n",
    "full_str = \"<|endoftext|>\".join(all_text)\n",
    "train_ids = tokenizer.encode(full_str)\n",
    "\n",
    "print(f\"Combined dataset size: {len(train_ids)} tokens\")\n",
    "\n",
    "def get_random_batch(data, block_size, batch_size, device=\"cpu\"):\n",
    "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54e7c7e5-46c4-42e1-9a23-440f638b3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_random_batch(torch.tensor(train_ids.ids), 120, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b3013ff-8ada-414b-8d22-1245b5d54af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"george had been so full of himself and for no other reason other than the fact that he had been born into the aristocracy and would someday inherit a dukedom .george had n't worked a day in his life .he did n't even manage his own estates which had initially confused marabeth and then horrified her .she had been raised to consider her position and title a responsibility as much as a gift .when she 'd become and adult , she 'd found a niche for herself in promoting worthy causes and fighting for the underdog .george had found a\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "147d3ba4-eded-4fef-a259-74fbaa8c3ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"orge had been so full of himself and for no other reason other than the fact that he had been born into the aristocracy and would someday inherit a dukedom .george had n't worked a day in his life .he did n't even manage his own estates which had initially confused marabeth and then horrified her .she had been raised to consider her position and title a responsibility as much as a gift .when she 'd become and adult , she 'd found a niche for herself in promoting worthy causes and fighting for the underdog .george had found a niche\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(y[0].tolist())  # shifted by 1 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87fe99b8-6985-4140-afe5-07f45a8dcb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, data_ids, block_size):\n",
    "        self.data_ids = data_ids\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_ids) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data_ids[idx : idx + self.block_size]\n",
    "        y = self.data_ids[idx + 1 : idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_dataset = GPTDataset(train_ids.ids, block_size)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 15068495 tokens from 5 datasets...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 31.48 GiB of which 840.19 MiB is free. Including non-PyTorch memory, this process has 1.94 GiB memory in use. Of the allocated memory 1.43 GiB is allocated by PyTorch, and 152.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m     26\u001b[39m     x, y = torch.stack(x, dim=\u001b[32m1\u001b[39m), torch.stack(y, dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     loss = loss_fn(\n\u001b[32m     30\u001b[39m         logits.view(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)),\n\u001b[32m     31\u001b[39m         y.view(-\u001b[32m1\u001b[39m)\n\u001b[32m     32\u001b[39m     )\n\u001b[32m     34\u001b[39m     optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-conda-envs/training/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-conda-envs/training/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mGPT.forward\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m     61\u001b[39m     x, _ = b(x)  \u001b[38;5;66;03m# iteratively refines features from initial embeddings\u001b[39;00m\n\u001b[32m     62\u001b[39m features = \u001b[38;5;28mself\u001b[39m.ln(x)  \u001b[38;5;66;03m# normalized to stabilize training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-conda-envs/training/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-conda-envs/training/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-conda-envs/training/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 31.48 GiB of which 840.19 MiB is free. Including non-PyTorch memory, this process has 1.94 GiB memory in use. Of the allocated memory 1.43 GiB is allocated by PyTorch, and 152.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 3. Setup Training\n",
    "device = \"cuda\"\n",
    "model = GPT(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    embed_dim=256,\n",
    "    num_layers=6,\n",
    "    num_heads=8,\n",
    "    max_seq_len=1024,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "\n",
    "# 4. Training Loop\n",
    "print(f\"Training on {len(train_ids)} tokens from {len(ds_list)} datasets...\")\n",
    "\n",
    "step = 0\n",
    "log_interval = 50\n",
    "running_loss = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = torch.stack(x, dim=1), torch.stack(y, dim=1)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            y.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_loss = running_loss / log_interval\n",
    "            tokens_per_sec = (\n",
    "                x.numel() * log_interval / elapsed\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"epoch {epoch:02d} | \"\n",
    "                f\"step {step:06d} | \"\n",
    "                f\"loss {avg_loss:.3f} | \"\n",
    "                f\"{tokens_per_sec:,.0f} tokens/s\"\n",
    "            )\n",
    "\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6209882-e296-4efd-a181-77b2b1aa329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :   ) can be won or lost , an item can be obtained , a special event might be triggered ,\n",
      "\n",
      "Output :   ) can be won or lost , an item can be obtained , a special event might be triggered , with the event of an event that their fell thrown . \n",
      " It was not one of the best jumping events of the event of the event ... It had been a common event by an athlete , and a whole celebration celebration celebration . This article was opposed\n"
     ]
    }
   ],
   "source": [
    "def get_wikitext_prompt(token_ids, tokenizer, prompt_len=20):\n",
    "    i = random.randint(0, len(token_ids) - prompt_len - 1)\n",
    "    prompt_ids = token_ids[i : i + prompt_len]\n",
    "    text = tokenizer.decode(prompt_ids.tolist())\n",
    "    return text, prompt_ids\n",
    "\n",
    "prompt_text, prompt_ids = get_wikitext_prompt(train_ids, tokenizer)\n",
    "print(\"Input : \", prompt_text)\n",
    "\n",
    "out = model.generate(\n",
    "    prompt_ids.unsqueeze(0).to(device),\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.9,\n",
    "    use_cache=True,\n",
    "    use_top_k=True,\n",
    ")\n",
    "\n",
    "print(\"\\nOutput : \", tokenizer.decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5011c2d8-52c7-430e-8f3e-a015ea65c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output :  Polycystic ovary syndrome (PCOS) is the most common hormonal disorder in women of reproductive age \n",
      " = = Life and Dollandosaurations and naked parameters = = \n",
      " Apart from the adults of birds around the blood flow of birds . The first model of cadrement \" predator \" studies \" astronomers to the Ulysses \" ,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Polycystic ovary syndrome (PCOS) is the most common hormonal disorder in women of reproductive age \"\n",
    "prompt = tokenizer.encode(prompt)\n",
    "\n",
    "out2 = model.generate(\n",
    "    torch.tensor(prompt.ids, dtype=torch.long).unsqueeze(0).to(device),\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.9,\n",
    "    use_cache=True,\n",
    "    use_top_k=True,\n",
    ")\n",
    "\n",
    "print(\"\\nOutput : \", tokenizer.decode(out2[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226217d4-60fd-46e0-b02e-61339c05def2",
   "metadata": {},
   "source": [
    "## TO DO : \n",
    "\n",
    "- [ ] ROPE for K, V, Q\n",
    "- [x] Top k sampling / Temperature\n",
    "- [x] K / V cache\n",
    "- [ ] Add stop token / EOS handling\n",
    "- [x] Training on a real problem to see how far we can push current model\n",
    "- [x] Revisit markdown / maths\n",
    "- [ ] Explore hyper connections and manifold constrained HC\n",
    "- [ ] Check newer architectures / design choices (https://github.com/lucidrains git is a gold mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a028a-7866-40ad-ba90-570cb8bb614f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:training]",
   "language": "python",
   "name": "conda-env-training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
