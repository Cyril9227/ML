{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "Largely inspired by [TinyTorch](https://mlsysbook.ai/tinytorch), the [associated course](https://mlsysbook.ai/assets/downloads/Machine-Learning-Systems.pdf), as well as this excellent [video serie from Karpathy](https://youtu.be/kCc8FmEb1nY) for his nano-gpt.\n",
    "\n",
    "Thanks Claude for the markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/harvard-edge/cs249r_book\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ac7aa-ffdd-4afa-8ce7-976a4a19cd07",
   "metadata": {},
   "source": [
    "To have a basic GPT model we need a bunch of components :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2ab6e-a29c-40c5-ae68-cbbdf00118e8",
   "metadata": {},
   "source": [
    "## Tokeniser\n",
    "\n",
    "There are tons of [different strategies](https://mlsysbook.ai/tinytorch/modules/10_tokenization_ABOUT.html), the simplest being a simple character lookup table from a known corpus. \n",
    "\n",
    "For our purpose, can just use one off the shelf if needed : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e97936b3-e648-4cda-87d0-b249f5c102af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 995] ['Hello', 'Ġworld']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"GPT2\") \n",
    "encoded = tokenizer.encode(\"Hello world\")\n",
    "print(encoded.ids, encoded.tokens) # Ġ means that there is a space before this token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b27b7d-afa0-49fc-b41b-be306cf1be0f",
   "metadata": {},
   "source": [
    "## Embeddings \n",
    "\n",
    "Turns tokens ids into dense vectors carrying semantic information. Just a lookup table with randomly initialized vectors, that will be learnt as training go. Different initializations schemes are available, the more popular are : \n",
    "\n",
    "#### Xavier Normal\n",
    "$$\n",
    "w \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}\\right)\n",
    "$$\n",
    "\n",
    "#### Xavier Uniform\n",
    "\n",
    "$$\n",
    "w \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
    "$$\n",
    "\n",
    "#### For Embeddings\n",
    "\n",
    "If you view the embedding layer as `embedd = one_hot @ W` then $n_{\\text{in}} = \\text{vocabsize}$ otherwise if you just view it as a lookup table then $n_{\\text{in}} = n_{\\text{out}} = n_{\\text{embed}}$ and the maths simplifies to : \n",
    "\n",
    "\n",
    "**Normal**: $w \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{\\text{embed}}}\\right)$\n",
    "\n",
    "**Uniform**: $w \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{3}{n_{\\text{embed}}}}, \\sqrt{\\frac{3}{n_{\\text{embed}}}}\\right)$\n",
    "\n",
    "Note : Can also directly use pretrained embeddings but it fell out of flavor for modern LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7698e80f-ef7c-47c4-8735-571e25140a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable embedding layer that maps token indices to dense vectors.\n",
    "\n",
    "    This is the fundamental building block for converting discrete tokens\n",
    "    into continuous representations that neural networks can process.\n",
    "\n",
    "    APPROACH:\n",
    "    1. Initialize embedding matrix with random weights (vocab_size, embed_dim)\n",
    "    2. Implement forward pass as matrix lookup using numpy indexing\n",
    "    3. Handle batch dimensions correctly\n",
    "    4. Return parameters for optimization\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> embed = Embedding(vocab_size=100, embed_dim=64)\n",
    "    >>> tokens = Tensor([[1, 2, 3], [4, 5, 6]])  # batch_size=2, seq_len=3\n",
    "    >>> output = embed.forward(tokens)\n",
    "    >>> print(output.shape)\n",
    "    (2, 3, 64)\n",
    "\n",
    "    HINTS:\n",
    "    - Use numpy advanced indexing for lookup: weight[indices]\n",
    "    - Embedding matrix shape: (vocab_size, embed_dim)\n",
    "    - Initialize with Xavier/Glorot uniform for stable gradients\n",
    "    - Handle multi-dimensional indices correctly\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, init: str = \"xavier_uniform\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        if init == \"xavier_uniform\":\n",
    "            # rand ~ U(0, 1) -> (b - a) * U(0, 1) ~ U(0, b - a) -> (b - a) * U(0, 1) + a ~ U(a, b)\n",
    "            rdm_uniform = torch.rand(vocab_size, embed_dim)\n",
    "            limit = math.sqrt(3 / embed_dim)\n",
    "            embeddings = 2 * limit * rdm_uniform - limit\n",
    "            \n",
    "        elif init == \"xavier_normal\":\n",
    "            # randn ~ N(0, 1) -> sqrt(a) * N(0, 1) + b ~ N(b, a)\n",
    "            rdm_normal = torch.randn(vocab_size, embed_dim)\n",
    "            limit = math.sqrt(1 / embed_dim)\n",
    "            embeddings = limit * rdm_normal\n",
    "            \n",
    "        elif init is None or init is False:\n",
    "            # N(0, 1) if not specified\n",
    "            embeddings = torch.randn(vocab_size, embed_dim)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Expected init in ['xavier_uniform', 'xavier_normal', None, False] but received {init}\")\n",
    "\n",
    "        self.embeddings = nn.Parameter(embeddings, requires_grad=True)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embeddings[tokens.long()]  # indexes must be int or bool.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f76e2-49bc-4d90-bac4-5fd59155cc55",
   "metadata": {},
   "source": [
    "## Positional Encodings\n",
    "\n",
    "Embeddings representing the position of the token in the sequence, as the position also carries meaning.\n",
    "\n",
    "#### Absolute encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7a2e71-1479-41be-8a1f-7446571a0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable positional encoding layer.\n",
    "\n",
    "    Adds trainable position-specific vectors to token embeddings,\n",
    "    allowing the model to learn positional patterns specific to the task.\n",
    "\n",
    "    APPROACH:\n",
    "    1. Create embedding matrix for positions: (max_seq_len, embed_dim)\n",
    "    2. Forward pass: lookup position embeddings and add to input\n",
    "    3. Handle different sequence lengths gracefully\n",
    "    4. Return parameters for training\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> pos_enc = PositionalEncoding(max_seq_len=512, embed_dim=64)\n",
    "    >>> embeddings = Tensor(np.random.randn(2, 10, 64))  # (batch, seq, embed)\n",
    "    >>> output = pos_enc.forward(embeddings)\n",
    "    >>> print(output.shape)\n",
    "    (2, 10, 64)  # Same shape, but now position-aware\n",
    "\n",
    "    HINTS:\n",
    "    - Position embeddings shape: (max_seq_len, embed_dim)\n",
    "    - Use slice [:seq_len] to handle variable lengths\n",
    "    - Add position encodings to input embeddings element-wise\n",
    "    - Initialize with smaller values than token embeddings (they're additive)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_seq_len: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        limit = math.sqrt(2 / embed_dim)  # smaller constant for inductive bias : embeddings > position\n",
    "        xavier_uniform = torch.empty(max_seq_len, embed_dim).uniform_(-limit, limit)  # idiomatic way to init\n",
    "        self.positional_encoding = nn.Parameter(xavier_uniform)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        seq_len = embeddings.shape[1]\n",
    "        # early gpt / transformers used absolute pos encoding and required a fixed max length\n",
    "        if seq_len > self.max_seq_len:\n",
    "            raise ValueError(\n",
    "                    f\"Sequence length {seq_len} exceeds maximum {self.max_seq_len}\"\n",
    "                )\n",
    "        # Pytorch broadcasting is smart enough to figure out the missing batch dim\n",
    "        return embeddings + self.positional_encoding[:seq_len]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "cc962692-efc6-4fce-b967-523f6b41b58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape : torch.Size([2, 3, 64]) | min : -0.2159646600484848 - max 0.21605424582958221\n",
      "Positional Embeddings shape : torch.Size([2, 3, 64]) | min : -0.38585883378982544 - max 0.3753717243671417\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.Tensor([[1, 2, 3], [4, 5, 6]])  # 2 sequences of 3 tokens\n",
    "\n",
    "embed = EmbeddingLayer(vocab_size=100, embed_dim=64)\n",
    "embeddings = embed.forward(tokens)\n",
    "print(f\"Embeddings shape : {embeddings.shape} | min : {embeddings.min()} - max {embeddings.max()}\")\n",
    "\n",
    "pos = PositionalEncodingLayer(max_seq_len=1024, embed_dim=embeddings.shape[-1])\n",
    "pos_embeddings = pos.forward(embeddings)\n",
    "\n",
    "print(f\"Positional Embeddings shape : {pos_embeddings.shape} | min : {pos_embeddings.min()} - max {pos_embeddings.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8030b7e-d352-4512-b824-4a6b7549e0e1",
   "metadata": {},
   "source": [
    "Absolute position encoding isn't used in practice because the encoding changes if you add tokens before the sentence and we're limited by max sequence length, making it hard to extrapolate to new seq lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b79ca-8fe9-44fa-a9d9-175308f7fd13",
   "metadata": {},
   "source": [
    "#### Relative encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff55d0-c2cf-4c8a-be2a-fb942e9a09d6",
   "metadata": {},
   "source": [
    "Modern systems use ROPE (or similar) encoding schemes (but not on the embeddings directly, here it's fine for the sake of the exercise). It encodes position as phase, token position scales the rotation angle, frequencies separate scales, and attention dot products convert phase differences into relative position awareness.\n",
    "\n",
    "- The first token is not rotated because position zero defines the reference frame.\n",
    "- Each embedding pair has a fixed angular frequency, shared across all tokens.\n",
    "- Tokens at later positions are rotated by an angle proportional to their position.\n",
    "- Different frequencies encode relative position at different spatial scales, and their combination allows robust relative positioning over long sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30566e4d-09ec-4421-a8d2-8b43dc75cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEncodingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Positional Encoding (RoPE) layer.\n",
    "\n",
    "    IMPORTANT CONCEPTUAL SHIFT:\n",
    "    ---------------------------\n",
    "    Unlike learned or sinusoidal positional encodings, RoPE:\n",
    "    - DOES NOT create a positional embedding matrix\n",
    "    - DOES NOT add anything to embeddings\n",
    "    - Instead, it *rotates* embedding vectors in a position-dependent way\n",
    "\n",
    "    RoPE encodes *relative position* by rotating pairs of embedding\n",
    "    dimensions using position-dependent angles.\n",
    "\n",
    "    This layer is typically applied to:\n",
    "    - Query (Q) and Key (K) tensors in attention\n",
    "    - NOT to token embeddings directly\n",
    "\n",
    "    However, for this exercise, we apply it to embeddings to understand\n",
    "    the mechanism in isolation.\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "\n",
    "    INPUT / OUTPUT SHAPE:\n",
    "    ---------------------\n",
    "    Input:\n",
    "        embeddings: Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    Output:\n",
    "        Tensor of same shape (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "\n",
    "    HIGH-LEVEL IDEA:\n",
    "    ----------------\n",
    "    1. Split embedding dimension into pairs:\n",
    "         (x_0, x_1), (x_2, x_3), ...\n",
    "\n",
    "    2. Each pair represents a 2D vector\n",
    "\n",
    "    3. For each position `pos`, rotate each 2D vector by an angle:\n",
    "         θ(pos, i) = pos / (base ** (2i / embed_dim))\n",
    "\n",
    "    4. Rotation is done via:\n",
    "         [cosθ  -sinθ]\n",
    "         [sinθ   cosθ]\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "\n",
    "    WHAT YOU NEED TO PRECOMPUTE:\n",
    "    ----------------------------\n",
    "    - A vector of inverse frequencies (inv_freq)\n",
    "        shape: (embed_dim // 2,)\n",
    "\n",
    "      Typical formula:\n",
    "        inv_freq[i] = 1 / (base ** (2i / embed_dim))\n",
    "\n",
    "      where base is usually 10000.\n",
    "\n",
    "    - Position indices:\n",
    "        pos = [0, 1, 2, ..., seq_len - 1]\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "\n",
    "    WHAT HAPPENS IN FORWARD():\n",
    "    --------------------------\n",
    "    1. Extract seq_len from embeddings\n",
    "    2. Create position indices [0..seq_len-1]\n",
    "    3. Compute angles = pos[:, None] * inv_freq[None, :]\n",
    "    4. Compute sin and cos of angles\n",
    "    5. Split embeddings into even / odd dimensions\n",
    "    6. Apply rotation:\n",
    "         x_even * cos - x_odd * sin\n",
    "         x_even * sin + x_odd * cos\n",
    "    7. Re-interleave dimensions\n",
    "    8. Return rotated embeddings\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "\n",
    "    KEY PROPERTIES OF RoPE:\n",
    "    -----------------------\n",
    "    - No learned parameters (usually)\n",
    "    - No max sequence length hard limit\n",
    "    - Encodes relative position naturally\n",
    "    - Enables extrapolation to longer sequences\n",
    "    - Position information survives dot products (Q·K)\n",
    "\n",
    "    EXAMPLE:\n",
    "    --------\n",
    "    >>> rope = PositionalEncodingLayer(embed_dim=64)\n",
    "    >>> x = torch.randn(2, 10, 64)\n",
    "    >>> y = rope(x)\n",
    "    >>> y.shape\n",
    "    torch.Size([2, 10, 64])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        if embed_dim % 2 != 0:\n",
    "            raise ValueError(f\"embed_dim must be divisible by 2 but received {embed_dim}\")\n",
    "        \n",
    "        i = torch.arange(0, embed_dim, 2)  # [0, 2, 4, ..., 62]\n",
    "        self.inv_freq = 1.0 / (base ** (i / embed_dim))\n",
    "        \n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply RoPE to embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Rotated embeddings: (batch_size, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = embeddings.shape\n",
    "        \n",
    "        # Step 1: Create position indices [0, 1, 2, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len)\n",
    "        \n",
    "        # Step 2: Compute rotation angles\n",
    "        # angles[pos, i] = pos * inv_freq[i]\n",
    "        angles = positions[:, None] * self.inv_freq[None, :]\n",
    "        \n",
    "        # Step 3: Compute sin and cos\n",
    "        cos_angles, sin_angles = torch.cos(angles), torch.sin(angles)  # (seq_len, embed_dim//2)\n",
    "        \n",
    "        # Step 4: Split embeddings into even and odd dimensions\n",
    "        x_even = embeddings[..., 0::2]  # (batch_size, seq_len, embed_dim//2)\n",
    "        x_odd = embeddings[..., 1::2]   # (batch_size, seq_len, embed_dim//2)\n",
    "        \n",
    "        # Step 5: Apply 2D rotation to each pair\n",
    "        # Rotation matrix: [cos  -sin]  applied to [x_even]\n",
    "        #                  [sin   cos]             [x_odd ]\n",
    "        # Result:\n",
    "        #   x_even_rotated = x_even * cos - x_odd * sin\n",
    "        #   x_odd_rotated  = x_even * sin + x_odd * cos\n",
    "        x_even_rotated = x_even * cos_angles - x_odd * sin_angles\n",
    "        x_odd_rotated = x_even * sin_angles + x_odd * cos_angles\n",
    "        \n",
    "        # Step 6: Interleave back to original dimension order\n",
    "        # Stack along new dimension then flatten\n",
    "        x_out = torch.stack([x_even_rotated, x_odd_rotated], dim=-1)\n",
    "        x_out = x_out.flatten(-2)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1a06d8-de56-4702-9128-8affc9d2fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete embedding system combining token and positional embeddings.\n",
    "\n",
    "    This is the component that handles the full embedding\n",
    "    pipeline used in transformers and other sequence models.\n",
    "\n",
    "    APPROACH:\n",
    "    1. Combine token embedding + positional encoding\n",
    "    2. Support both learned and sinusoidal position encodings\n",
    "    3. Handle variable sequence lengths gracefully\n",
    "    4. Add optional embedding scaling (Transformer convention)\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> embed_layer = EmbeddingLayer(\n",
    "    ...     vocab_size=50000,\n",
    "    ...     embed_dim=512,\n",
    "    ...     max_seq_len=2048,\n",
    "    ...     pos_encoding='learned'\n",
    "    ... )\n",
    "    >>> tokens = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    >>> output = embed_layer.forward(tokens)\n",
    "    >>> print(output.shape)\n",
    "    (2, 3, 512)\n",
    "\n",
    "    HINTS:\n",
    "    - First apply token embedding, then add positional encoding\n",
    "    - Handle both 2D (batch, seq) and 1D (seq) inputs gracefully\n",
    "    - Scale embeddings by sqrt(embed_dim) if requested (transformer convention)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        max_seq_len: int = 512,\n",
    "        scale_embeddings: bool = False,\n",
    "        positional_embedding = \"absolute\"  # rotary\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.scale_embeddings = scale_embeddings\n",
    "        self.embedding = EmbeddingLayer(vocab_size, self.embed_dim)\n",
    "        if positional_embedding == \"absolute\":\n",
    "            self.positional_embedding = PositionalEncodingLayer(max_seq_len, self.embed_dim)\n",
    "        elif positional_embedding == \"rotary\":\n",
    "            self.positional_embedding = RotaryPositionalEncodingLayer(self.embed_dim, 10000)\n",
    "        elif positional_embedding is None or positional_embedding is False:\n",
    "            self.positional_embedding = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"positional_embedding only supports ['absolute', 'rotary', 'False', 'None'] but received {positional_embedding}\")\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        if len(tokens.shape) == 1:\n",
    "            tokens = tokens.unsqueeze(0)  # add batch dim\n",
    "\n",
    "        embeddings = self.embedding(tokens)\n",
    "        if self.scale_embeddings:\n",
    "            # transformer convention, helps with gradient stability\n",
    "            # and helps embedding dominate vs positions\n",
    "            embeddings *= math.sqrt(self.embed_dim) \n",
    "\n",
    "        return self.positional_embedding(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed956c-ca46-408b-8cbb-3f4294fcc2e4",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "Now that we have a way to encode our words we can compute stuff\n",
    "\n",
    "### Core Intuition\n",
    "Attention allows each token to **selectively focus** on other tokens in the sequence. Instead of treating all positions equally, the model learns which positions are most relevant for processing each token.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given an input sequence of embeddings $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ where $n$ is sequence length and $d$ is embedding dimension:\n",
    "\n",
    "#### 1. **Linear Projections**\n",
    "Transform input into three representations:\n",
    "$$\\mathbf{Q} = \\mathbf{X}\\mathbf{W}^Q, \\quad \\mathbf{K} = \\mathbf{X}\\mathbf{W}^K, \\quad \\mathbf{V} = \\mathbf{X}\\mathbf{W}^V$$\n",
    "\n",
    "where $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{d \\times d_k}$ are learned weight matrices.\n",
    "\n",
    "- **Query** ($\\mathbf{Q}$): \"What am I looking for?\"\n",
    "- **Key** ($\\mathbf{K}$): \"What do I contain?\"\n",
    "- **Value** ($\\mathbf{V}$): \"What information do I carry?\"\n",
    "\n",
    "*Note : it's called self-attention because all 3 are computed from same input $X$*\n",
    "\n",
    "#### 2. **Attention Scores**\n",
    "Compute similarity between queries and keys:\n",
    "$$\\mathbf{S} = \\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\in \\mathbb{R}^{n \\times n}$$\n",
    "\n",
    "- Dot product measures compatibility between query $i$ and key $j$\n",
    "- Scaling by $\\sqrt{d_k}$ prevents extremely large values (stabilizes gradients)\n",
    "- Entry $S_{ij}$ = \"how much should token $i$ attend to token $j$?\"\n",
    "\n",
    "#### 3. **Attention Weights**\n",
    "Normalize scores into probabilities:\n",
    "$$\\mathbf{A} = \\text{softmax}(\\mathbf{S}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "where softmax is applied row-wise: $A_{ij} = \\frac{\\exp(S_{ij})}{\\sum_{k=1}^{n} \\exp(S_{ik})}$\n",
    "\n",
    "- Each row sums to 1: $\\sum_{j=1}^{n} A_{ij} = 1$\n",
    "- $A_{ij}$ = probability that token $i$ attends to token $j$\n",
    "\n",
    "#### 4. **Weighted Sum**\n",
    "Aggregate values using attention weights:\n",
    "$$\\mathbf{Z} = \\mathbf{A}\\mathbf{V} \\in \\mathbb{R}^{n \\times d_k}$$\n",
    "\n",
    "Each output $\\mathbf{z}_i$ is a weighted combination: $\\mathbf{z}_i = \\sum_{j=1}^{n} A_{ij} \\mathbf{v}_j$\n",
    "\n",
    "### Complete Formula (Scaled Dot-Product Attention)\n",
    "\n",
    "$$\\boxed{\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}}$$\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Split features into bags and run $h$ attention operations in parallel with different learned projections (1 attention per bag of features):\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(\\mathbf{X}\\mathbf{W}^Q_i, \\mathbf{X}\\mathbf{W}^K_i, \\mathbf{X}\\mathbf{W}^V_i)$$\n",
    "\n",
    "$$\\text{MultiHead}(\\mathbf{X}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}^O$$\n",
    "\n",
    "where $\\mathbf{W}^O \\in \\mathbb{R}^{hd_k \\times d}$ projects concatenated heads back to model dimension.\n",
    "\n",
    "**Benefit**: Each head can learn different attention patterns (e.g., syntax, semantics, positional relationships).\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- **Permutation Equivariant**: Without positional encodings, shuffling input tokens shuffles output identically\n",
    "- **Parallelizable**: All positions computed simultaneously (unlike RNNs)\n",
    "- **Long-Range Dependencies**: Any token can directly attend to any other token in $O(1)$ operations\n",
    "- **Complexity**: $O(n^2 d)$ time and $O(n^2)$ memory for sequence length $n$\n",
    "\n",
    "\n",
    "### Differentiable Hash-table (python dict)\n",
    "\n",
    "Can also view this attention mechanism as a \"soft\" python dictionary where the key doesn't need to match exactly and the dictionary returns a weighted blend of multiple values\n",
    "\n",
    "\n",
    "### Causal mask\n",
    "- Prevents each token from attending to future tokens by masking out positions to the right in the attention matrix.\n",
    "- It enforces autoregressive behavior: the model can only use past and present information when predicting the next token (typical for text generation).\n",
    "- Without it the model would “cheat” during training by seeing the future and would fail at generation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism.\n",
    "\n",
    "    Runs multiple attention heads in parallel, each learning different relationships.\n",
    "    \n",
    "    APPROACH:\n",
    "        1. Validate that embed_dim is divisible by num_heads\n",
    "        2. Calculate head_dim (embed_dim // num_heads)\n",
    "        3. Create linear layers for Q, K, V projections\n",
    "        4. Create output projection layer\n",
    "        5. Store configuration parameters\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Embedding dimension (d_model)\n",
    "            num_heads: Number of parallel attention heads\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> mha = MultiHeadAttention(embed_dim=512, num_heads=8)\n",
    "        >>> mha.head_dim  # 64 (512 / 8)\n",
    "        >>> len(mha.parameters())  # 4 linear layers * 2 params each = 8 tensors\n",
    "\n",
    "        HINTS:\n",
    "        - head_dim = embed_dim // num_heads must be integer\n",
    "        - Need 4 Linear layers: q_proj, k_proj, v_proj, out_proj\n",
    "        - Each projection maps embed_dim → embed_dim\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, causal: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Validate configuration\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\"\n",
    "            )\n",
    "        \n",
    "        # 2. Store parameters\n",
    "        self.causal = causal  # mask futur tokens ?\n",
    "        self.embed_dim = embed_dim          # d_model\n",
    "        self.num_heads = num_heads          # H\n",
    "        self.head_dim = embed_dim // num_heads  # d_head\n",
    "\n",
    "        # 3. Linear projections for Q, K, V\n",
    "        # Each maps: (B, N, embed_dim) → (B, N, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        # 4. Output projection (after concatenating heads)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, _ = x.shape\n",
    "        # split features into num_heads bags of head_dim features\n",
    "        Q = self.q_proj(x).reshape(B, N, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(x).reshape(B, N, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(x).reshape(B, N, self.num_heads, self.head_dim)\n",
    "\n",
    "        # transpose to (batch, num_heads, seq, head_dim) for parallel processing\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.head_dim)  # (batch, num_heads, seq, seq)\n",
    "        \n",
    "        if self.causal:\n",
    "            auto_regressive_mask = torch.triu(\n",
    "                torch.ones(N, N, device=x.device),\n",
    "                diagonal=1\n",
    "            ).bool()\n",
    "            # fill upper triangle with -inf value -> 0 after softmax\n",
    "            scores = scores.masked_fill(auto_regressive_mask, float('-inf'))\n",
    "\n",
    "        # softmax along keys\n",
    "        scores = torch.softmax(scores, -1)\n",
    "        output = scores @ V\n",
    "        \n",
    "        # concatenate the heads and final proj\n",
    "        return self.out_proj(output.transpose(1, 2).contiguous().view(B, N, self.embed_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a4b0d-1cce-41f2-a917-952d9b9a15a2",
   "metadata": {},
   "source": [
    "## Layer Normalization vs Batch Normalization\n",
    "\n",
    "### Why Layer Norm for Transformers?\n",
    "\n",
    "#### Normalization Axis Comparison\n",
    "\n",
    "**Batch Normalization**: Normalizes across the **batch dimension**\n",
    "$$\\mu_j = \\frac{1}{B}\\sum_{i=1}^{B} x_{ij}, \\quad \\sigma_j^2 = \\frac{1}{B}\\sum_{i=1}^{B} (x_{ij} - \\mu_j)^2$$\n",
    "\n",
    "For input shape `(B, N, D)` → computes statistics over `B` (batch) for each feature `j`\n",
    "\n",
    "**Layer Normalization**: Normalizes across the **feature dimension**\n",
    "$$\\mu_i = \\frac{1}{D}\\sum_{j=1}^{D} x_{ij}, \\quad \\sigma_i^2 = \\frac{1}{D}\\sum_{j=1}^{D} (x_{ij} - \\mu_i)^2$$\n",
    "\n",
    "For input shape `(B, N, D)` → computes statistics over `D` (features) for each sample `i`\n",
    "\n",
    "#### Visual Comparison\n",
    "```\n",
    "Input shape: (Batch=2, SeqLen=3, Features=4)\n",
    "\n",
    "Batch Norm:           Layer Norm:\n",
    "[[ 1  2  3  4]        [[ 1  2  3  4]\n",
    " [ 5  6  7  8]         [ 5  6  7  8]\n",
    " [ 9 10 11 12]         [ 9 10 11 12]\n",
    " [13 14 15 16]         [13 14 15 16]\n",
    " [17 18 19 20]         [17 18 19 20]\n",
    " [21 22 23 24]]        [21 22 23 24]]\n",
    " \n",
    " ↓ Normalize           → Normalize\n",
    " per column            per row\n",
    "```\n",
    "\n",
    "### Why Layer Norm Wins for Transformers\n",
    "\n",
    "| Issue | Batch Norm Problem | Layer Norm Solution |\n",
    "|-------|-------------------|---------------------|\n",
    "| **Variable Sequence Lengths** | Statistics change with padding | Each sequence normalized independently |\n",
    "| **Small Batch Sizes** | Unstable statistics (mean/var unreliable) | Statistics computed per sample |\n",
    "| **Train/Test Discrepancy** | Needs running statistics, different behavior | Same computation always |\n",
    "| **Sequential Processing** | Can't normalize one sample at inference | Works naturally for online inference |\n",
    "\n",
    "#### Key Insight\n",
    "> Transformers process **variable-length sequences** with **self-attention**. Each token's representation should be normalized based on **its own feature distribution**, not compared to other samples in the batch.\n",
    "\n",
    "### Biased vs Unbiased Variance\n",
    "\n",
    "#### The Math\n",
    "\n",
    "**Biased (Population) Variance**: $\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\mu)^2$\n",
    "\n",
    "**Unbiased (Sample) Variance**: $s^2 = \\frac{1}{N-1}\\sum_{i=1}^{N}(x_i - \\mu)^2$\n",
    "\n",
    "#### PyTorch Uses Biased Variance (`unbiased=False`)\n",
    "\n",
    "**Ratio**: $\\frac{\\sigma_{\\text{unbiased}}^2}{\\sigma_{\\text{biased}}^2} = \\frac{D}{D-1} = \\frac{512}{511} \\approx 1.002$\n",
    "\n",
    "**Practical Impact**:\n",
    "- For **large feature dimensions** (D=512, 768, 1024): difference is **<0.2%** → negligible\n",
    "- Biased estimator is the **population parameter** we actually want to normalize by\n",
    "- Using `unbiased=True` would slightly **over-normalize** (divide by slightly larger σ)\n",
    "\n",
    "#### Why Use Biased?\n",
    "\n",
    "1. **Consistency**: We're normalizing the **current data**, not estimating a population parameter\n",
    "2. **Stability**: Dividing by N (not N-1) is more stable when N is small\n",
    "3. **Standard Practice**: PyTorch's `nn.LayerNorm` uses biased variance\n",
    "4. **Negligible Difference**: For D ≫ 1, the correction factor D/(D-1) ≈ 1\n",
    "\n",
    "\n",
    "### Complete LayerNorm Formula\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "where:\n",
    "- $\\mu = \\frac{1}{D}\\sum_{j=1}^{D} x_j$ (mean over features)\n",
    "- $\\sigma^2 = \\frac{1}{D}\\sum_{j=1}^{D} (x_j - \\mu)^2$ (biased variance over features)\n",
    "- $\\gamma, \\beta$ are learnable parameters (initialized to 1 and 0)\n",
    "- $\\epsilon$ small value to prevent division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c83ee5-2a72-4c62-93a0-3a3c0773ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization for transformer blocks.\n",
    "\n",
    "    Normalizes across the feature dimension (last axis) for each sample independently,\n",
    "    unlike batch normalization which normalizes across the batch dimension.\n",
    "\n",
    "    Mathematical Formula:\n",
    "    output = (x - μ) / σ * γ + β\n",
    "    \n",
    "    where:\n",
    "      μ = mean(x, axis=features)     # Mean across feature dimension\n",
    "      σ = sqrt(var(x) + ε)          # Standard deviation + small epsilon\n",
    "      γ = learnable scale parameter  # Initialized to 1.0\n",
    "      β = learnable shift parameter  # Initialized to 0.0\n",
    "      \n",
    "    Why Layer Norm Works:\n",
    "    \n",
    "    Independence: Each sample normalized independently (good for variable batch sizes)\n",
    "    Stability: Prevents internal covariate shift that breaks training\n",
    "    Gradient Flow: Helps gradients flow better through deep networks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        \"\"\"\n",
    "        Initialize LayerNorm with learnable parameters.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Store the shape to normalize over (usually embed_dim)\n",
    "        2. Initialize learnable scale (gamma) and shift (beta) parameters\n",
    "        3. Set small epsilon for numerical stability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "\n",
    "        # Learnable parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply layer normalization.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Compute mean and variance across the last dimension\n",
    "        2. Normalize: (x - mean) / sqrt(variance + eps)\n",
    "        3. Apply learnable scale and shift: gamma * normalized + beta\n",
    "        \"\"\"\n",
    "\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        # var() divides by N-1 but torch implementation uses population var i.e divides by N\n",
    "        std = torch.sqrt(x.var(-1, keepdim=True, unbiased=False) + self.eps) # <=> np.sqrt(((x - mean) ** 2).mean(-1, keepdim=True))\n",
    "        x = (x - mean) / std\n",
    "        return x * self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065d472-f3eb-4ecd-946d-e83bdc21a83c",
   "metadata": {},
   "source": [
    "## MLP block\n",
    "\n",
    "### Attention\n",
    "Mixes information across tokens. Each token dynamically selects which other tokens to read from, based on content. This enables context, dependency modeling, and information routing over the sequence.\n",
    "\n",
    "### MLP (Feed-Forward Network)\n",
    "Mixes information within a token. It non-linearly recombines and transforms features of each token independently, increasing representational power.\n",
    "\n",
    "### Why both\n",
    "Attention provides communication between tokens; the MLP provides computation on token features.\n",
    "Without attention, tokens can’t interact. Without MLPs, the model is mostly linear and weak.\n",
    "\n",
    "#### Why GELU instead of ReLU?\n",
    "\n",
    "```\n",
    "# ReLU: Hard cutoff at 0\n",
    "relu(x) = max(0, x)\n",
    "\n",
    "# GELU: Smooth, probabilistic gating\n",
    "gelu(x) = x * Φ(x)  # Φ is standard normal CDF\n",
    "```\n",
    "*Benefits of GELU:*\n",
    "\n",
    "- Smoother gradients (no sharp corner at x=0)\n",
    "- Better performance in language models (empirically)\n",
    "- Can be thought of as \"stochastic regularization\"\n",
    "\n",
    "#### Why 4x expansion ? \n",
    "\n",
    "2x too small, 8x too slow, 4x works well\n",
    "\n",
    "\n",
    "#### Why No Activation After Second Layer?\n",
    "\n",
    "```\n",
    "# Transformer block structure:\n",
    "output = x + MLP(LayerNorm(x))  # Residual connection\n",
    "        ↑\n",
    "        └─ Need to preserve residual path\n",
    "```\n",
    "\n",
    "Adding GELU after linear2, would force all outputs to be non-negative, which:\n",
    "\n",
    "- Limits expressiveness of the residual stream\n",
    "- Can cause gradient flow issues\n",
    "- Breaks the \"smooth information highway\" of residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf4ba2d-402d-4eab-86e1-754f9faeaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (Feed-Forward Network) for transformer blocks.\n",
    "\n",
    "    Standard pattern: Linear -> GELU -> Linear with expansion ratio of 4:1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim=None, dropout_prob=0.1):\n",
    "        \"\"\"\n",
    "        Initialize MLP with two linear layers.\n",
    "\n",
    "        APPROACH:\n",
    "        1. First layer expands from embed_dim to hidden_dim (usually 4x larger)\n",
    "        2. Second layer projects back to embed_dim\n",
    "        3. Use GELU activation (smoother than ReLU, preferred in transformers)\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> mlp = MLP(512)  # Will create 512 -> 2048 -> 512 network\n",
    "        >>> x = Tensor(np.random.randn(2, 10, 512))\n",
    "        >>> output = mlp.forward(x)\n",
    "        >>> assert output.shape == (2, 10, 512)\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * embed_dim  # Standard transformer expansion\n",
    "\n",
    "        # params\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # actual layers, bias on by default, some architectures turn it off here\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.linear1(x))\n",
    "        x = self.linear2(x)  # No GELU here to not mess with residual paths x + MLP(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19c8d0-16aa-4cc0-b8f5-9e43d30bec58",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "Now that we have each component, we can code the actual computation block.\n",
    "\n",
    "Tl;dr :\n",
    "\n",
    "#### Transformer block\n",
    "Applies self-attention followed by an MLP, each wrapped with residual connections (and normalization).\n",
    "\n",
    "- Self-attention: lets tokens exchange information across the sequence.\n",
    "- MLP: non-linearly transforms each token’s features.\n",
    "- Residual connections: preserve information and stabilize training.\n",
    "\n",
    "Attention enables communication, the MLP enables computation, and residuals allow stacking many layers without losing signal.\n",
    "\n",
    "#### Why pre-norm\n",
    "Empirically found in a paper to be better for stability than applying LayerNorm after.\n",
    "\n",
    "#### Why stack many blocks\n",
    "Each block performs a small amount of communication (attention) and computation (MLP). Stacking many blocks lets the model build increasingly abstract, long-range, and hierarchical representations. One block is shallow; depth gives expressive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c04d073-6e3f-469d-98b4-5cda92115923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Block with self-attention, MLP, and residual connections.\n",
    "\n",
    "    This is the core building block of GPT and other transformer models.\n",
    "    Each block processes the input sequence and passes it to the next block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout_prob=0.1, causal=True):\n",
    "        \"\"\"\n",
    "        Initialize a complete transformer block.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Multi-head self-attention for sequence modeling\n",
    "        2. First layer normalization (pre-norm architecture)\n",
    "        3. MLP with specified expansion ratio\n",
    "        4. Second layer normalization\n",
    "\n",
    "        TRANSFORMER BLOCK ARCHITECTURE:\n",
    "        x → LayerNorm → MultiHeadAttention → + (residual) →\n",
    "            LayerNorm → MLP → + (residual) → output\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> block = TransformerBlock(embed_dim=512, num_heads=8)\n",
    "        >>> x = Tensor(np.random.randn(2, 10, 512))  # (batch, seq, embed)\n",
    "        >>> output = block.forward(x)\n",
    "        >>> assert output.shape == (2, 10, 512)\n",
    "\n",
    "        HINT: We use pre-norm architecture (LayerNorm before attention/MLP)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm1 = LayerNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, causal)  # causal = masking out tokens\n",
    "        self.layernorm2 = LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio * embed_dim, dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through transformer block.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Apply layer norm, then self-attention, then add residual\n",
    "        2. Apply layer norm, then MLP, then add residual\n",
    "        3. Return the transformed sequence\n",
    "\n",
    "        COMPUTATION FLOW:\n",
    "        x → ln1 → attention → + x → ln2 → mlp → + → output\n",
    "\n",
    "        RESIDUAL CONNECTIONS:\n",
    "        These are crucial for training deep networks - they allow gradients\n",
    "        to flow directly through the network during backpropagation.\n",
    "\n",
    "        HINT: \n",
    "        - Store intermediate results to add residual connections properly\n",
    "        - Don't forget masking\n",
    "        \"\"\"\n",
    "        x1 = self.layernorm1(x)\n",
    "        x2 = self.mha(x1) + x  # residual path\n",
    "\n",
    "        x3 = self.layernorm2(x2)\n",
    "        x3 = self.mlp(x3) + x2  # residual path\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97cdca-b682-43ae-a24d-f08ab84b0673",
   "metadata": {},
   "source": [
    "# Putting it all together : GPT\n",
    "\n",
    "Words -> tokens -> embeddings + position -> stack of transformer blocks -> prediction layer\n",
    "\n",
    "One thing worth mentioning is the weight tying between the embedding table and the final linear layer. It is explicitely asked in the docstring but not actually implemented in the [official solution](https://hub.2i2c.mybinder.org/user/harvard-edge-cs249r_book-2qly0zim/doc/tree/tinytorch/modules/13_transformers/13_transformers.ipynb), however it is present in [nano-gpt](https://github.com/karpathy/nanoGPT/blob/3adf61e154c3fe3fca428ad6bc3818b27a3b8291/model.py#L138)\n",
    "\n",
    "I think it makes sense, you want the embedding table to perform token id -> embedding but also the reverse operation (if you transpose it) ie vector living in same embedding space -> ~~~token id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT (Generative Pre-trained Transformer) model.\n",
    "\n",
    "    This combines embeddings, positional encoding, multiple transformer blocks,\n",
    "    and a language modeling head for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, max_seq_len=1024):\n",
    "        \"\"\"\n",
    "        Initialize complete GPT model.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Token embedding layer to convert tokens to vectors\n",
    "        2. Positional embedding to add position information\n",
    "        3. Stack of transformer blocks (the main computation)\n",
    "        4. Final layer norm and language modeling head\n",
    "\n",
    "        GPT ARCHITECTURE:\n",
    "        tokens → embedding → + pos_embedding →\n",
    "                transformer_blocks → layer_norm → lm_head → logits\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> model = GPT(vocab_size=1000, embed_dim=256, num_layers=6, num_heads=8)\n",
    "        >>> tokens = Tensor(np.random.randint(0, 1000, (2, 10)))  # (batch, seq)\n",
    "        >>> logits = model.forward(tokens)\n",
    "        >>> assert logits.shape == (2, 10, 1000)  # (batch, seq, vocab)\n",
    "\n",
    "        HINTS:\n",
    "        - Positional embeddings are learned, not fixed sinusoidal\n",
    "        - Final layer norm stabilizes training\n",
    "        - Language modeling head shares weights with token embedding (tie_weights)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.embedding = Embedding(\n",
    "            vocab_size=self.vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            max_seq_len=self.max_seq_len,\n",
    "            scale_embeddings=True,\n",
    "            positional_embedding=\"absolute\"       \n",
    "        )\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.ln = LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        # mental naming lol\n",
    "        # but basically you want the embedding table to perform both operations words -> embed and embed -> prediction\n",
    "        # so we jointly learn last and first layer\n",
    "        # bias = false, to match the lookup table structure\n",
    "        self.embedding.embedding.embeddings = self.lm_head.weight\n",
    "       \n",
    "    def forward(self, tokens):\n",
    "        x = self.embedding(tokens)  # tokens -> emb + pos enc\n",
    "        for b in self.blocks:\n",
    "            x = b(x)  # iteratively refines features from initial embeddings\n",
    "        features = self.ln(x)  # normalized to stabilize training\n",
    "        return self.lm_head(features)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate text autoregressively.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Start with prompt tokens\n",
    "        2. For each new position:\n",
    "           - Run forward pass to get logits\n",
    "           - Sample next token from logits\n",
    "           - Append to sequence\n",
    "        3. Return generated sequence\n",
    "\n",
    "        AUTOREGRESSIVE GENERATION:\n",
    "        At each step, the model predicts the next token based on all\n",
    "        previous tokens. This is how GPT generates coherent text.\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> model = GPT(vocab_size=100, embed_dim=64, num_layers=2, num_heads=4)\n",
    "        >>> prompt = Tensor([[1, 2, 3]])  # Some token sequence\n",
    "        >>> generated = model.generate(prompt, max_new_tokens=5)\n",
    "        >>> assert generated.shape[1] == 3 + 5  # original + new tokens\n",
    "\n",
    "        HINT: Use np.random.choice with temperature for sampling\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        current_tokens = prompt_tokens.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Forward pass\n",
    "                logits = self(current_tokens)          # (B, T, V)\n",
    "        \n",
    "                # Last token logits\n",
    "                last_logits = logits[:, -1, :]         # (B, V)\n",
    "        \n",
    "                # Temperature\n",
    "                scaled_logits = last_logits / temperature\n",
    "        \n",
    "                # Softmax\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "                # Sample next token (Torch-native)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "        \n",
    "                # Append\n",
    "                current_tokens = torch.cat([current_tokens, next_token], dim=1)\n",
    "        \n",
    "        return current_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78396c3-b63a-41f4-952a-8ebce18b74c0",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "e6d4dc62-d50c-4a53-a8f1-66e1203ab024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 4, 1000]) True\n",
      "tensor(0., grad_fn=<MeanBackward0>)\n",
      "tensor([[ 1,  2,  3,  4, 10, 10, 10, 10, 10]])\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT(vocab_size=1000, embed_dim=256, num_layers=6, num_heads=8)\n",
    "logits = gpt(tokens)\n",
    "print(tokens.shape, logits.shape, gpt.lm_head.weight is gpt.embedding.embedding.embeddings)\n",
    "\n",
    "gpt.eval()\n",
    "tokens = torch.tensor([[1, 2, 3, 4]])\n",
    "logits = gpt(tokens)\n",
    "\n",
    "tokens2 = torch.tensor([[1, 2, 999, 4]])\n",
    "logits2 = gpt(tokens2)\n",
    "\n",
    "# need to be identical\n",
    "print(torch.abs(logits[:, :2] - logits2[:, :2]).mean())\n",
    "\n",
    "out = model.generate(tokens, 5)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "745e0a14-8a09-43de-86a7-721dacea5e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00 | loss 0.4297\n",
      "epoch 01 | loss 0.1400\n",
      "epoch 02 | loss 0.1117\n",
      "epoch 03 | loss 0.0660\n",
      "epoch 04 | loss 0.0630\n",
      "epoch 05 | loss 0.0636\n",
      "epoch 06 | loss 0.0562\n",
      "epoch 07 | loss 0.0564\n",
      "epoch 08 | loss 0.0556\n",
      "epoch 09 | loss 0.0524\n",
      "epoch 10 | loss 0.0526\n",
      "epoch 11 | loss 0.0526\n",
      "epoch 12 | loss 0.0529\n",
      "epoch 13 | loss 0.0509\n",
      "epoch 14 | loss 0.1056\n",
      "epoch 15 | loss 0.0718\n",
      "epoch 16 | loss 0.0555\n",
      "epoch 17 | loss 0.0712\n",
      "epoch 18 | loss 0.0698\n",
      "epoch 19 | loss 0.0614\n",
      "epoch 20 | loss 0.0557\n",
      "epoch 21 | loss 0.0516\n",
      "epoch 22 | loss 0.0524\n",
      "epoch 23 | loss 0.0519\n",
      "epoch 24 | loss 0.0517\n",
      "epoch 25 | loss 0.0510\n",
      "epoch 26 | loss 0.0501\n",
      "epoch 27 | loss 0.0524\n",
      "epoch 28 | loss 0.0514\n",
      "epoch 29 | loss 0.0502\n",
      "epoch 30 | loss 0.0523\n",
      "epoch 31 | loss 0.0501\n",
      "epoch 32 | loss 0.0507\n",
      "epoch 33 | loss 0.0497\n",
      "epoch 34 | loss 0.0503\n",
      "epoch 35 | loss 0.0506\n",
      "epoch 36 | loss 0.0516\n",
      "epoch 37 | loss 0.0503\n",
      "epoch 38 | loss 0.0491\n",
      "epoch 39 | loss 0.0505\n",
      "epoch 40 | loss 0.0499\n",
      "epoch 41 | loss 0.0506\n",
      "epoch 42 | loss 0.0494\n",
      "epoch 43 | loss 0.0510\n",
      "epoch 44 | loss 0.0506\n",
      "epoch 45 | loss 0.0504\n",
      "epoch 46 | loss 0.0513\n",
      "epoch 47 | loss 0.0500\n",
      "epoch 48 | loss 0.0501\n",
      "epoch 49 | loss 0.0508\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, block_size):\n",
    "        self.block_size = block_size\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "        self.data = torch.tensor([self.stoi[c] for c in text], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.block_size]\n",
    "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x)                  # (B, T, V)\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            y.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text = \"hello world\\n\" * 100\n",
    "block_size = 8\n",
    "batch_size = 32\n",
    "\n",
    "dataset = CharDataset(text, block_size)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embed_dim=128,\n",
    "    num_layers=8,\n",
    "    num_heads=4,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for epoch in range(50):\n",
    "    loss = train(model, loader, optimizer, device)\n",
    "    print(f\"epoch {epoch:02d} | loss {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bbe9144-0cb3-486e-86f2-4998fdb7c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "helllorlo\n"
     ]
    }
   ],
   "source": [
    "start_token = dataset.stoi[\"h\"]\n",
    "\n",
    "prompt = torch.tensor(\n",
    "    [[start_token]],\n",
    "    dtype=torch.long,\n",
    "    device=device\n",
    ")\n",
    "out = model.generate(prompt, max_new_tokens=20, temperature=0.7)\n",
    "tokens = out[0].tolist()\n",
    "text = \"\".join(dataset.itos[i] for i in tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cafb8b1-6799-4a5e-a207-d046e270f6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h → hello world\n",
      "e → ello world\n",
      "\n",
      "l → lo world\n",
      "he\n"
     ]
    }
   ],
   "source": [
    "# We're good !\n",
    "for c in \"hel\":\n",
    "    prompt = torch.tensor([[dataset.stoi[c]]], device=device)\n",
    "    print(c, \"→\", \"\".join(dataset.itos[i] for i in model.generate(prompt, 10, temperature=0.7)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f55e4-352a-42a7-a2e6-740818434fff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebcde1e6-6a9d-4270-86f9-ab525799a97d",
   "metadata": {},
   "source": [
    "## TO DO : \n",
    "\n",
    "- ROPE for K, V, Q\n",
    "- Top k sampling\n",
    "- Training on a real problem to see how far we can push current model\n",
    "- Revisit markdown / maths\n",
    "- Check newer architectures / design choices (https://github.com/lucidrains git is a gold mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7aaa1-d123-48b5-a41c-a221ebd5ae01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:training]",
   "language": "python",
   "name": "conda-env-training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
