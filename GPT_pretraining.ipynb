{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "First notebook was written from 'scratch', this one leverages existing libraries to experiment with actual training and inference.\n",
    "\n",
    "I also added some improvements over baseline model : \n",
    "- Moved attention computation to optimized `F.scaled_dot_product_attention`\n",
    "- Moved `LayerNorm` to `RMSNorm`, which is the standard now\n",
    "- Moved `GELU` to `SWIGLU`\n",
    "- Moved positional encoding to `RoPE` on `Q` and `K`\n",
    "- Disabled bias in every linear layers\n",
    "- Grouped `Q`, `V`, `K` projections into 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import csv\n",
    "import inspect\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment config\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "from datasets import concatenate_datasets, load_dataset, Dataset as ds\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "\n",
    "# Custom\n",
    "from utils import clean_columns\n",
    "\n",
    "# Torch runtime config\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, causal: bool = True, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "        \n",
    "        self.causal = causal\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout_p = dropout\n",
    "        \n",
    "        # Fused QKV projection: 3x the output size\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        \n",
    "        self.rotary_emb = RotaryEmbedding(dim=self.head_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, k_v_cache=None):\n",
    "        B, T, _ = x.shape\n",
    "        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n",
    "    \n",
    "        # 1. Single fused projection\n",
    "        if using_cache:\n",
    "            x_q = x[:, -1:, :]\n",
    "            qkv = self.qkv_proj(x_q)  # (B, 1, 3 x embed_dim)\n",
    "        else:\n",
    "            qkv = self.qkv_proj(x)  # (B, T, 3 x embed_dim)\n",
    "        \n",
    "        # 2. Split into Q, K, V\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)  # Each is (B, T, embed_dim)\n",
    "        \n",
    "        def split_heads(t):\n",
    "            return t.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 3. Split heads -> (B, H, T, D_head)\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "    \n",
    "        # 4. Apply RoPE \n",
    "        if using_cache:\n",
    "            past_len = k_v_cache[\"K\"].shape[-2]\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q, offset=past_len)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K, offset=past_len)\n",
    "            \n",
    "            K = torch.cat([k_v_cache[\"K\"], K], dim=-2)\n",
    "            V = torch.cat([k_v_cache[\"V\"], V], dim=-2)\n",
    "        else:\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K)\n",
    "    \n",
    "        # 5. Update cache\n",
    "        if k_v_cache is not None:\n",
    "            k_v_cache[\"K\"] = K\n",
    "            k_v_cache[\"V\"] = V\n",
    "    \n",
    "        # 6. Attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            query=Q,\n",
    "            key=K,\n",
    "            value=V,\n",
    "            attn_mask=None, \n",
    "            dropout_p=self.dropout_p if self.training else 0.0,\n",
    "            is_causal=self.causal\n",
    "        )\n",
    "        \n",
    "        # 7. Merge heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, -1, self.embed_dim)\n",
    "\n",
    "        # 8. Linear projection\n",
    "        return self.out_proj(out), k_v_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "caf4ba2d-402d-4eab-86e1-754f9faeaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim=None, dropout_prob=0.1, use_swiglu=True):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * embed_dim\n",
    "        \n",
    "        self.use_swiglu = use_swiglu\n",
    "        \n",
    "        if use_swiglu:\n",
    "            # Adjust hidden_dim for param count matching\n",
    "            hidden_dim = int(2 * hidden_dim / 3)\n",
    "            self.gate_proj = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.up_proj = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.act = nn.GELU()\n",
    "            self.linear2 = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_swiglu:\n",
    "            return self.dropout(self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x)))\n",
    "        else:\n",
    "            return self.dropout(self.linear2(self.act(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c04d073-6e3f-469d-98b4-5cda92115923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 causal=True,\n",
    "                 use_swiglu=True,\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Initialize a complete transformer block.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Multi-head self-attention for sequence modeling\n",
    "        2. 1st Normalization (pre-norm architecture)\n",
    "        3. MLP with specified expansion ratio\n",
    "        4. 2nd Normalization\n",
    "    \n",
    "        TRANSFORMER BLOCK ARCHITECTURE:\n",
    "        x → Norm → MultiHeadAttention → + (residual) →\n",
    "            Norm → MLP → + (residual) → output\n",
    "    \n",
    "        NB: We use pre-norm architecture (before attention/MLP)\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "        self.norm1 = nn.RMSNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, causal, dropout_prob)  # causal = masking out tokens\n",
    "        self.norm2 = nn.RMSNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio * embed_dim, dropout_prob, use_swiglu)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        x1 = self.norm1(x)\n",
    "        x2, cache = self.mha(x1, cache)  # will be used when generating tokens during inference\n",
    "        x2 = x2 + x  # residual path\n",
    "    \n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.mlp(x3) + x2  # residual path\n",
    "        return x3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT (Generative Pre-trained Transformer) model.\n",
    "\n",
    "    This combines embeddings, positional encoding, multiple transformer blocks,\n",
    "    and a language modeling head for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 is_causal=True,\n",
    "                 use_swiglu=True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize complete GPT model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout_prob, is_causal, use_swiglu) for _ in range(num_layers)])\n",
    "        self.norm = nn.RMSNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        # weight tying\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        # below shamefully stolen from nano-gpt\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            # dont forget swiglu case, oops\n",
    "            if pn.endswith((\"c_proj.weight\", \"out_proj.weight\", \"down_proj.weight\")):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.num_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "       \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.embedding(tokens)\n",
    "        x = self.dropout(embeddings)\n",
    "        for b in self.blocks:\n",
    "            x, _ = b(x)  # iteratively refines features from initial embeddings\n",
    "        features = self.norm(x)  # normalized to stabilize training\n",
    "        return self.lm_head(features)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 prompt_tokens,\n",
    "                 max_new_tokens=50,\n",
    "                 temperature=1.0,\n",
    "                 use_cache=True,\n",
    "                 use_top_k=False,\n",
    "                ):\n",
    "        self.eval()\n",
    "\n",
    "        tokens_out = prompt_tokens.clone()\n",
    "        current_tokens = prompt_tokens.clone()\n",
    "        tokens_out = tokens_out.to(self.device)\n",
    "        current_tokens = current_tokens.to(self.device)\n",
    "        cache = [{} if use_cache else None for _ in range(len(self.blocks))]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            x = self.embedding(current_tokens)\n",
    "            for i, b in enumerate(self.blocks):\n",
    "                x, c_i = b(x, cache[i])\n",
    "                cache[i] = c_i\n",
    "            \n",
    "            features = self.norm(x)\n",
    "            logits = self.lm_head(features)\n",
    "                    \n",
    "            last_logits = logits[:, -1, :]\n",
    "    \n",
    "            if temperature > 0:\n",
    "                scaled_logits = last_logits / temperature\n",
    "                # Only sample from top k tokens to avoid garbage prediction derailing whole prediction\n",
    "                # We don't simply take max prob token to allow \"creativity\"\n",
    "                if use_top_k:\n",
    "                    # heuristic that is ok for toy project\n",
    "                    # most of probability mass in on a small amount of tokens\n",
    "                    k = min(max(5, int(0.01 * self.vocab_size)), 100)\n",
    "                    values, indices = torch.topk(scaled_logits, k)\n",
    "                    scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n",
    "                    scaled_logits.scatter_(1, indices, values)\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy decoding if temp is 0 (prevents division by zero)\n",
    "                next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
    "\n",
    "            # Stop generating if model thinks the \"document\" is finished\n",
    "            if next_token.item() == eot_id:\n",
    "                break\n",
    "            \n",
    "            tokens_out = torch.cat([tokens_out, next_token], dim=1)\n",
    "\n",
    "            # If caching, we only need to feed the newest token next time, otherwise full sequence\n",
    "            current_tokens = next_token if use_cache else tokens_out\n",
    "       \n",
    "        return tokens_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec6e7e-9bce-467f-9c36-aadb01c2ec5f",
   "metadata": {},
   "source": [
    "## Full Training\n",
    "\n",
    "Note : we re-use the tokenizer defined at the beginning\n",
    "\n",
    "As I do not have unlimited compute, our datasets need to be as clean as possible and high signal. C4 / FineWeb will have massive overlap with OpenWebText (from same common crawl), wikipedia is probably going to be oversampled, which should be fine, bookcorpus is mostly self-published fictions, should have less overlap.\n",
    "\n",
    "I thought adding maths and code would be a good idea but it introduces big issues with the tokenizer and learning new semantics / syntax etc. might be too ambitious for a small(er) scale model. Arxiv is definitely a no, open-web-math might be fine as it's more informal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c04a17c6-e96f-4943-827d-649214ae44d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_bf16_supported() True\n"
     ]
    }
   ],
   "source": [
    "#### CONFIG #####\n",
    "\n",
    "# Basically GPT-2 Small\n",
    "block_size = 1024  # Can also do 512 for faster convergence then 1024 to finish training\n",
    "batch_size = 16\n",
    "embed_dim = 768\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "dropout_prob = 0.1  # usually set as 0 but my model being relatively smaller, should help\n",
    "mlp_ratio = 4  # standard 4x expansion\n",
    "\n",
    "\n",
    "# Training\n",
    "MAX_STEPS = 600000       # Total number of micro-batches to process\n",
    "GRAD_ACCUM_STEPS = 40    # Accumulate gradients over 40 batches\n",
    "LOG_INTERVAL = 500       # Log every 500 micro-batches\n",
    "num_workers = 4          # For data loading\n",
    "prefetch = 4\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "model_path = f\"gpt_model_maths_{block_size}_final.pt\"  # where do we store trained model\n",
    "log_file = \"training_log_pretraining.csv\"  # where do we store training logs\n",
    "print(\"torch.cuda.is_bf16_supported()\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b92cd5fa-1da3-4e79-8760-6c9faadd9516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & cleaning up datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5df355483ed4b4693433ae2adecab5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dd7be9d5cd445b941e3b751cd3c8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded 5 datasets\n",
      "  Dataset fineweb-edu: 9,672,101 examples\n",
      "  Dataset wiki-train: 36,718 examples\n",
      "  Dataset wiki-validation: 3,760 examples\n",
      "  Dataset bookcorpus: 7,400,423 examples\n",
      "  Dataset maths-qa: 200,035 examples\n",
      "Concatenating...\n",
      "Success! Final Train Size: 17313037 rows\n",
      "Tokenizing and chunking (this may take a moment)...\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import login\n",
    "# login(\"your token\")  # faster dl\n",
    "\n",
    "# Setup Tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"GPT2\")\n",
    "eot_id = tokenizer.token_to_id(\"<|endoftext|>\")\n",
    "assert eot_id is not None\n",
    "\n",
    "\n",
    "# Clean and Concatenate\n",
    "print(\"Loading & cleaning up datasets...\")\n",
    "cleaned_datasets = {\n",
    "    # main dataset, high quality web crawl\n",
    "    \"fineweb-edu\": load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\"),\n",
    "    # might have some overlap with above but should be fine\n",
    "    \"wiki-train\": load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\"),\n",
    "    \"wiki-validation\": load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\"),\n",
    "    # for flavor / diversity\n",
    "    \"bookcorpus\": load_dataset(\"bookcorpus\", split=\"train[:10%]\"),\n",
    "    # pretty good Q/A maths dataset (tiny though, might not do much)\n",
    "    \"maths-qa\": load_dataset(\"microsoft/orca-math-word-problems-200k\", split=\"train\")  \n",
    "}\n",
    "cleaned_datasets = {n: clean_columns(d) for n, d in cleaned_datasets.items()}\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(cleaned_datasets)} datasets\")\n",
    "for n, ds in cleaned_datasets.items():\n",
    "    print(f\"  Dataset {n}: {len(ds):,} examples\")\n",
    "\n",
    "print(\"Concatenating...\")\n",
    "train_ds = concatenate_datasets(cleaned_datasets.values())\n",
    "\n",
    "# Shuffle to make sure the model doesn't train for hours on wikipedia then suddenly code etc.\n",
    "train_ds = train_ds.shuffle(seed=42)  # usually bad idea but our data is small enough\n",
    "print(f\"Success! Final Train Size: {len(train_ds)} rows\")\n",
    "\n",
    "\n",
    "def process_batch(examples):\n",
    "    \"\"\"\n",
    "    1. Tokenizes text.\n",
    "    2. Appends EOT token to EVERY document.\n",
    "    3. Flattens into a 1D stream.\n",
    "    4. Chunks into block_size + 1 (to allow for shifting).\n",
    "    \"\"\"\n",
    "    all_token_ids = []\n",
    "    \n",
    "    # Tokenize and add EOT (Document Boundary)\n",
    "    for text in examples[\"text\"]:\n",
    "        # Skip empty strings\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Encode\n",
    "        ids = tokenizer.encode(text).ids\n",
    "        \n",
    "        # Append EOT (Crucial for GPT context separation)\n",
    "        ids.append(eot_id)\n",
    "        all_token_ids.extend(ids)\n",
    "    \n",
    "    # We need chunks of length block_size + 1 (bcs of shifting -> block_size later)\n",
    "    chunk_len = block_size + 1\n",
    "    \n",
    "    # Truncate remainder\n",
    "    total_len = (len(all_token_ids) // chunk_len) * chunk_len\n",
    "    \n",
    "    # Reshape into list of lists\n",
    "    chunks = [\n",
    "        all_token_ids[i : i + chunk_len] \n",
    "        for i in range(0, total_len, chunk_len)\n",
    "    ]\n",
    "    \n",
    "    # Return dict for HF Dataset\n",
    "    return {\"chunk_ids\": chunks}\n",
    "\n",
    "\n",
    "# Apply the processing\n",
    "print(\"Tokenizing and chunking (this may take a moment)...\")\n",
    "train_tokenized = train_ds.map(\n",
    "    process_batch, \n",
    "    batched=True, \n",
    "    batch_size=1000, \n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    "    remove_columns=train_ds.column_names,  # Remove 'text' to free up RAM.\n",
    "    desc=\"Processing Train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9b9c5e5-977f-4759-854e-06604ef30c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Final Train Batches: 617609\n",
      "Input x shape: torch.Size([16, 1024])\n",
      "Target y shape: torch.Size([16, 1024])\n",
      "\n",
      "Sanity Check (Shifting):\n",
      "x[0, -5:]: [198, 13295, 13019, 25, 314]\n",
      "y[0, -5:]: [13295, 13019, 25, 314, 423]\n"
     ]
    }
   ],
   "source": [
    "class GPTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.ds = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the chunk of size BLOCK_SIZE + 1\n",
    "        chunk = self.ds[idx][\"chunk_ids\"].long()\n",
    "        # Shift target, the model needs to learn token_t -> token_t+1\n",
    "        return chunk[:-1], chunk[1:]\n",
    "\n",
    "\n",
    "# Alternatively can set format to numpy and use from_numpy() in the Dataset (zero-copy, much faster than tensor())\n",
    "train_tokenized.set_format(type=\"torch\", columns=[\"chunk_ids\"])\n",
    "train_dataset = GPTDataset(train_tokenized)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=prefetch,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"-\" * 20)\n",
    "print(f\"Final Train Batches: {len(train_loader)}\")\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"Input x shape: {x.shape}\")  # Should be [Batch, Block_Size]\n",
    "print(f\"Target y shape: {y.shape}\") # Should be [Batch, Block_Size]\n",
    "\n",
    "print(\"\\nSanity Check (Shifting):\")\n",
    "print(f\"x[0, -5:]: {x[0, -5:].tolist()}\") # End of input\n",
    "print(f\"y[0, -5:]: {y[0, -5:].tolist()}\") # End of target\n",
    "del x\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f2698c4-2604-4801-a70b-496ef8e6df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with config : \n",
      "{'dropout_prob': 0.1,\n",
      " 'embed_dim': 768,\n",
      " 'is_causal': True,\n",
      " 'mlp_ratio': 4,\n",
      " 'num_heads': 12,\n",
      " 'num_layers': 12,\n",
      " 'use_swiglu': True,\n",
      " 'vocab_size': 50257}\n"
     ]
    }
   ],
   "source": [
    "# --- Model ---\n",
    "model_config = {\n",
    "    \"vocab_size\": tokenizer.get_vocab_size(),\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"mlp_ratio\": mlp_ratio,\n",
    "    \"dropout_prob\": dropout_prob,\n",
    "    \"is_causal\": True,\n",
    "    \"use_swiglu\": True,\n",
    "}\n",
    "\n",
    "print(\"Initializing model with config : \")\n",
    "pprint(model_config)\n",
    "\n",
    "model = GPT(**model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e9f7153-2d7a-44fd-aa7e-628eaa16c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled !\n"
     ]
    }
   ],
   "source": [
    "model = torch.compile(model)   # can take a while\n",
    "model.train()\n",
    "print(\"Model compiled !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ceac27-4430-4319-96ae-d53bf4b15e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # TODO : implement resume from checkpoint (i.e optimizer state, step count etc.)\n",
    "# ckpt_path = \"ckpts/...\"\n",
    "\n",
    "# state_dict = torch.load(ckpt_path, map_location=device)\n",
    "# model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b4254-6a67-4537-90f4-a47fa740da21",
   "metadata": {},
   "source": [
    "Using standard practice :\n",
    "- linear warmup + cosine schedule\n",
    "- big weight decay (empirically proven to be beneficial)\n",
    "- gradient accumulation to emulate large batch size\n",
    "\n",
    "Nb : hearing contradicting statements about label_smoothing so disabled for now, we also only decay linear weights unlike nanogpt that also decay embeddings (NanoGPT does not follow modern best practices here apparently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2e581bf-67ec-4db3-9bcd-a1516f736f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decay params: 60, No decay params: 26\n",
      "Total Micro-batches: 600000\n",
      "Gradient Accumulation: 40\n",
      "Total Optimizer Updates: 15000\n",
      "Using fused AdamW :  True\n"
     ]
    }
   ],
   "source": [
    "# --- Optimizer ---\n",
    "# Separate parameters into decay and no-decay groups\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        continue\n",
    "    \n",
    "    # Don't apply weight decay to:\n",
    "    # - Norm parameters (scale/weight etc.)\n",
    "    # - Embedding table (which is tied to lm_head)\n",
    "    # - Any bias terms if present\n",
    "    if any(keyword in name.lower() for keyword in ['norm', 'bias', 'embed', 'embedding', 'lm_head']):\n",
    "        no_decay_params.append(param)\n",
    "    else:\n",
    "        decay_params.append(param)\n",
    "\n",
    "print(f\"\\nDecay params: {len(decay_params)}, No decay params: {len(no_decay_params)}\")\n",
    "\n",
    "total_optim_steps = MAX_STEPS // GRAD_ACCUM_STEPS\n",
    "print(f\"Total Micro-batches: {MAX_STEPS}\")\n",
    "print(f\"Gradient Accumulation: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Total Optimizer Updates: {total_optim_steps}\")\n",
    "\n",
    "# Perform ADAM update with a single kernel\n",
    "fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "use_fused = fused_available and device == 'cuda'\n",
    "extra_args = dict(fused=True) if use_fused else dict()\n",
    "print(\"Using fused AdamW : \", use_fused)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params, 'weight_decay': 0.1},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "], \n",
    "    lr=3e-4, \n",
    "    betas=(0.9, 0.95),\n",
    "    **extra_args\n",
    ")\n",
    "\n",
    "warmup_steps = int(total_optim_steps * 0.05)  # 5% of total optim steps\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer, \n",
    "    start_factor=0.1,  # Start at 3e-5\n",
    "    total_iters=warmup_steps\n",
    ")\n",
    "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=total_optim_steps - warmup_steps, \n",
    "    eta_min=1e-5\n",
    ")\n",
    "scheduler = SequentialLR(\n",
    "    optimizer, \n",
    "    schedulers=[warmup_scheduler, cosine_scheduler], \n",
    "    milestones=[warmup_steps]\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 000500 | opt_step 0012 | loss 9.782 | lr 3.43e-05 | 78,354 tok/s\n"
     ]
    }
   ],
   "source": [
    "# --- CSV Logger ---\n",
    "file_exists = os.path.isfile(log_file)\n",
    "with open(log_file, \"a\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    if not file_exists:\n",
    "        writer.writerow([\"micro_step\", \"optim_step\", \"loss\", \"lr\", \"tokens_seen\", \"tokens_per_sec\", \"timestamp\"])\n",
    "\n",
    "# --- Training Loop ---\n",
    "micro_step = 0      # Counts every batch seen\n",
    "optim_step = 0      # Counts every weight update\n",
    "tokens_seen = 0\n",
    "running_loss = 0.0\n",
    "start_time = time.time()\n",
    "start_training = time.time()\n",
    "\n",
    "# Initialize gradients once before starting\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# Train until we've seen enough tokens\n",
    "while micro_step < MAX_STEPS:\n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        B, T = x.shape\n",
    "        tokens_seen += B * T\n",
    "\n",
    "        # 1. Forward\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        # 2. Scale Loss for Backward (but keep original for logging!)\n",
    "        current_loss_val = loss.item() \n",
    "        scaled_loss = loss / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        # 3. Backward\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        # 4. Step (only every GRAD_ACCUM_STEPS micro-steps)\n",
    "        if (micro_step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            # avoids exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            optim_step += 1\n",
    "\n",
    "        # 5. Bookkeeping\n",
    "        running_loss += current_loss_val\n",
    "        micro_step += 1\n",
    "\n",
    "        # 6. Logging\n",
    "        if micro_step % LOG_INTERVAL == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_loss = running_loss / LOG_INTERVAL\n",
    "            tokens_per_sec = (B * T * LOG_INTERVAL) / elapsed\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            print(\n",
    "                f\"step {micro_step:06d} | \"\n",
    "                f\"opt_step {optim_step:04d} | \"\n",
    "                f\"loss {avg_loss:.3f} | \"\n",
    "                f\"lr {current_lr:.2e} | \"\n",
    "                f\"{tokens_per_sec:,.0f} tok/s\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                with open(log_file, \"a\", newline=\"\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([micro_step, optim_step, f\"{avg_loss:.4f}\", f\"{current_lr:.2e}\", tokens_seen, int(tokens_per_sec), timestamp])\n",
    "            except Exception as e:\n",
    "                print(f\"CSV Error: {e}\")\n",
    "\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "\n",
    "        if micro_step % 50000 == 0:\n",
    "            mid_model_path = model_path.replace(\".pt\", f\"_{micro_step}.pt\")\n",
    "            print(f\"Saving intermediate model in {mid_model_path}\")\n",
    "            torch.save(model.state_dict(), mid_model_path)\n",
    "        \n",
    "        if micro_step >= MAX_STEPS:\n",
    "            elapsed = int(time.time() - start_training)\n",
    "            h = elapsed // 3600\n",
    "            m = (elapsed % 3600) // 60\n",
    "            s = elapsed % 60\n",
    "            print(f\"\\nProcessed {tokens_seen:,} tokens in {h:02d}:{m:02d}:{s:02d}\")\n",
    "            print(f\"Saving final model in {model_path}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47d3a1ef-1db0-4658-acdd-a68757294434",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Test prompt I need to sample somewhere \"\n",
    "x = torch.tensor(tokenizer.encode(prompt).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a028a-7866-40ad-ba90-570cb8bb614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "out = model.generate(\n",
    "    x.unsqueeze(0).to(\"cuda\"),\n",
    "    max_new_tokens=250,\n",
    "    temperature=0.9,\n",
    "    use_cache=True,\n",
    "    use_top_k=True,\n",
    ")\n",
    "\n",
    "print(\"\\nOutput : \", tokenizer.decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d1452-0749-4415-a850-6138ae435283",
   "metadata": {},
   "source": [
    "## TO DO : \n",
    "\n",
    "- [x] ROPE for K, V, Q\n",
    "- [ ] ROPE for K, V cache, should be correct but need double check\n",
    "- [x] Top k sampling / Temperature\n",
    "- [x] K / V cache\n",
    "- [x] Add stop token / EOS handling\n",
    "- [x] Training on a real problem to see how far we can push current model\n",
    "- [x] Clean up / Revisit markdown / maths\n",
    "- [ ] Explore hyper connections and manifold constrained HC\n",
    "- [x] Check newer architectures / design choices (https://github.com/lucidrains git is a gold mine)\n",
    "- [ ] MUON optimizer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7b86e-2803-4838-840f-e22e311cb940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
