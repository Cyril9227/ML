{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "First notebook was written from 'scratch', this one leverages existing libraries to experiment with actual training and inference.\n",
    "\n",
    "I also added some improvements over baseline model : \n",
    "- Moved attention computation to optimized `F.scaled_dot_product_attention`\n",
    "- Moved `LayerNorm` to `RMSNorm`, which is the standard now\n",
    "- Moved `GELU` to `SWIGLU`\n",
    "- Moved positional encoding to RoPE on Q, K\n",
    "- Disabled bias in every linear layers\n",
    "- Grouped Q, V, K projections into 1\n",
    "- Used `MUON`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import csv\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment config\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Torch runtime config\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, causal: bool = True, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "        \n",
    "        self.causal = causal\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout_p = dropout\n",
    "        \n",
    "        # Fused QKV projection: 3x the output size\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        \n",
    "        self.rotary_emb = RotaryEmbedding(dim=self.head_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, k_v_cache=None):\n",
    "        B, T, _ = x.shape\n",
    "        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n",
    "    \n",
    "        # 1. Single fused projection\n",
    "        if using_cache:\n",
    "            x_q = x[:, -1:, :]\n",
    "            qkv = self.qkv_proj(x_q)  # (B, 1, 3*embed_dim)\n",
    "        else:\n",
    "            qkv = self.qkv_proj(x)  # (B, T, 3*embed_dim)\n",
    "        \n",
    "        # 2. Split into Q, K, V\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)  # Each is (B, T, embed_dim)\n",
    "        \n",
    "        def split_heads(t):\n",
    "            return t.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 3. Split heads -> (B, H, T, D_head)\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "    \n",
    "        # 4. Apply RoPE \n",
    "        if using_cache:\n",
    "            past_len = k_v_cache[\"K\"].shape[-2]\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q, offset=past_len)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K, offset=past_len)\n",
    "            \n",
    "            K = torch.cat([k_v_cache[\"K\"], K], dim=-2)\n",
    "            V = torch.cat([k_v_cache[\"V\"], V], dim=-2)\n",
    "        else:\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K)\n",
    "    \n",
    "        # 5. Update cache\n",
    "        if k_v_cache is not None:\n",
    "            k_v_cache[\"K\"] = K\n",
    "            k_v_cache[\"V\"] = V\n",
    "    \n",
    "        # 6. Attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            query=Q,\n",
    "            key=K,\n",
    "            value=V,\n",
    "            attn_mask=None, \n",
    "            dropout_p=self.dropout_p if self.training else 0.0,\n",
    "            is_causal=self.causal and (Q.shape[-2] > 1)\n",
    "        )\n",
    "        \n",
    "        # 7. Merge heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, -1, self.embed_dim)\n",
    "    \n",
    "        return self.out_proj(out), k_v_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf4ba2d-402d-4eab-86e1-754f9faeaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim=None, dropout_prob=0.1, use_swiglu=True):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * embed_dim\n",
    "        \n",
    "        self.use_swiglu = use_swiglu\n",
    "        \n",
    "        if use_swiglu:\n",
    "            # Adjust hidden_dim for param count matching\n",
    "            hidden_dim = int(2 * hidden_dim / 3)\n",
    "            self.gate_proj = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.up_proj = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.act = nn.GELU()\n",
    "            self.linear2 = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_swiglu:\n",
    "            return self.dropout(self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x)))\n",
    "        else:\n",
    "            return self.dropout(self.linear2(self.act(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c04d073-6e3f-469d-98b4-5cda92115923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 causal=True,\n",
    "                 use_swiglu=True,\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Initialize a complete transformer block.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Multi-head self-attention for sequence modeling\n",
    "        2. 1st Normalization (pre-norm architecture)\n",
    "        3. MLP with specified expansion ratio\n",
    "        4. 2nd Normalization\n",
    "    \n",
    "        TRANSFORMER BLOCK ARCHITECTURE:\n",
    "        x → Norm → MultiHeadAttention → + (residual) →\n",
    "            Norm → MLP → + (residual) → output\n",
    "    \n",
    "        NB: We use pre-norm architecture (before attention/MLP)\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "        self.norm1 = nn.RMSNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, causal, dropout_prob)  # causal = masking out tokens\n",
    "        self.norm2 = nn.RMSNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio * embed_dim, dropout_prob, use_swiglu)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        x1 = self.norm1(x)\n",
    "        x2, cache = self.mha(x1, cache)  # will be used when generating tokens during inference\n",
    "        x2 = x2 + x  # residual path\n",
    "    \n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.mlp(x3) + x2  # residual path\n",
    "        return x3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT (Generative Pre-trained Transformer) model.\n",
    "\n",
    "    This combines embeddings, positional encoding, multiple transformer blocks,\n",
    "    and a language modeling head for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 use_swiglu=True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize complete GPT model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout_prob, use_swiglu) for _ in range(num_layers)])\n",
    "        self.norm = nn.RMSNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        # weight tying\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        # below shamefully stolen from nano-gpt\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * self.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"Number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.embedding.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "       \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.embedding(tokens)\n",
    "        x = self.dropout(embeddings)\n",
    "        for b in self.blocks:\n",
    "            x, _ = b(x)  # iteratively refines features from initial embeddings\n",
    "        features = self.norm(x)  # normalized to stabilize training\n",
    "        return self.lm_head(features)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 prompt_tokens,\n",
    "                 max_new_tokens=50,\n",
    "                 temperature=1.0,\n",
    "                 use_cache=True,\n",
    "                 use_top_k=False,\n",
    "                ):\n",
    "        self.eval()\n",
    "\n",
    "        tokens_out = prompt_tokens.clone()\n",
    "        current_tokens = prompt_tokens.clone()\n",
    "        tokens_out = tokens_out.to(self.device)\n",
    "        current_tokens = current_tokens.to(self.device)\n",
    "        cache = [{} if use_cache else None for _ in range(len(self.blocks))]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            x = self.embedding(current_tokens)\n",
    "            for i, b in enumerate(self.blocks):\n",
    "                x, c_i = b(x, cache[i])\n",
    "                cache[i] = c_i\n",
    "            \n",
    "            features = self.norm(x)\n",
    "            logits = self.lm_head(features)\n",
    "                    \n",
    "            last_logits = logits[:, -1, :]\n",
    "    \n",
    "            if temperature > 0:\n",
    "                scaled_logits = last_logits / temperature\n",
    "                # Only sample from top k tokens to avoid garbage prediction derailing whole prediction\n",
    "                # We don't simply take max prob token to allow \"creativity\"\n",
    "                if use_top_k:\n",
    "                    # heuristic that is ok for toy project\n",
    "                    # most of probability mass in on a small amount of tokens\n",
    "                    k = min(max(5, int(0.01 * self.vocab_size)), 100)\n",
    "                    values, indices = torch.topk(scaled_logits, k)\n",
    "                    scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n",
    "                    scaled_logits.scatter_(1, indices, values)\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy decoding if temp is 0 (prevents division by zero)\n",
    "                next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
    "    \n",
    "            tokens_out = torch.cat([tokens_out, next_token], dim=1)\n",
    "\n",
    "            # If caching, we only need to feed the newest token next time, otherwise full sequence\n",
    "            current_tokens = next_token if use_cache else tokens_out\n",
    "       \n",
    "        return tokens_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec6e7e-9bce-467f-9c36-aadb01c2ec5f",
   "metadata": {},
   "source": [
    "## Full Training on Wikipedia\n",
    "\n",
    "Note : we re-use the tokenizer defined at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04a17c6-e96f-4943-827d-649214ae44d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_bf16_supported() True\n"
     ]
    }
   ],
   "source": [
    "#### CONFIG #####\n",
    "\n",
    "# Basically GPT-2 Small\n",
    "block_size = 1024  # 512 for faster convergence then 1024 to finish training\n",
    "batch_size = 16\n",
    "embed_dim = 768\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "dropout_prob = 0.1\n",
    "mlp_ratio = 4  # standard 4x expansion\n",
    "\n",
    "\n",
    "# Training\n",
    "MAX_STEPS = 500000       # Total number of micro-batches to process\n",
    "GRAD_ACCUM_STEPS = 40    # Accumulate gradients over 40 batches\n",
    "LOG_INTERVAL = 500       # Log every 500 micro-batches\n",
    "num_workers = 4          # For data loading\n",
    "prefetch = 4\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "model_path = f\"gpt_model_{block_size}_final.pt\"  # where do we store trained model\n",
    "print(\"torch.cuda.is_bf16_supported()\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92cd5fa-1da3-4e79-8760-6c9faadd9516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81aa1a8512d40c880bb4e745814cc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af10e3285dfc4eff84132052e5cfc76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Train Size: 9039896 rows\n",
      "Tokenizing and chunking (this may take a moment)...\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()  # faster dl\n",
    "\n",
    "\n",
    "# --- 1. Setup Tokenizer (Your specific implementation) ---\n",
    "tokenizer = Tokenizer.from_pretrained(\"GPT2\")\n",
    "eot_id = tokenizer.token_to_id(\"<|endoftext|>\")\n",
    "assert eot_id is not None\n",
    "\n",
    "# --- 2. Load and Normalize Datasets ---\n",
    "# We need to make sure all datasets have a 'text' column and nothing else\n",
    "def clean_columns(ds):\n",
    "    # Some datasets use 'line' or 'sentence', we rename to 'text'\n",
    "    if 'line' in ds.column_names:\n",
    "        ds = ds.rename_column('line', 'text')\n",
    "    # Keep only the 'text' column to ensure concatenation works\n",
    "    return ds.select_columns(['text'])\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Train List\n",
    "train_raw = [\n",
    "    load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\"),\n",
    "    load_dataset(\"tiny_shakespeare\", split=\"train\"),\n",
    "    load_dataset(\"bookcorpus\", split=\"train[:10%]\"),\n",
    "    load_dataset(\"openwebtext\", split=\"train[:20%]\"),\n",
    "]\n",
    "# Clean and Concatenate\n",
    "train_ds = concatenate_datasets([clean_columns(d) for d in train_raw])\n",
    "\n",
    "# Validation List\n",
    "val_raw = [\n",
    "    load_dataset(\"tiny_shakespeare\", split=\"validation\"),\n",
    "    load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\"),\n",
    "]\n",
    "val_ds = concatenate_datasets([clean_columns(d) for d in val_raw])\n",
    "\n",
    "print(f\"Raw Train Size: {len(train_ds)} rows\")\n",
    "\n",
    "# --- 3. Optimized Processing Pipeline ---\n",
    "\n",
    "def process_batch(examples):\n",
    "    \"\"\"\n",
    "    1. Tokenizes text.\n",
    "    2. Appends EOT token to EVERY document.\n",
    "    3. Flattens into a 1D stream.\n",
    "    4. Chunks into block_size + 1 (to allow for shifting).\n",
    "    \"\"\"\n",
    "    all_token_ids = []\n",
    "    \n",
    "    # 1. Tokenize and add EOT (Document Boundary)\n",
    "    # Note: We use tokenizer.encode_batch for speed if possible, \n",
    "    # but here we loop to append EOT easily.\n",
    "    for text in examples[\"text\"]:\n",
    "        # Skip empty strings\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Encode\n",
    "        ids = tokenizer.encode(text).ids\n",
    "        \n",
    "        # Append EOT (Crucial for GPT context separation)\n",
    "        ids.append(eot_id)\n",
    "        all_token_ids.extend(ids)\n",
    "    \n",
    "    # 2. Chunking\n",
    "    # We need chunks of length (block_size + 1)\n",
    "    chunk_len = block_size + 1\n",
    "    \n",
    "    # Truncate remainder\n",
    "    total_len = (len(all_token_ids) // chunk_len) * chunk_len\n",
    "    \n",
    "    # Reshape into list of lists\n",
    "    chunks = [\n",
    "        all_token_ids[i : i + chunk_len] \n",
    "        for i in range(0, total_len, chunk_len)\n",
    "    ]\n",
    "    \n",
    "    # Return dict for HF Dataset\n",
    "    return {\"chunk_ids\": chunks}\n",
    "\n",
    "# Apply the processing\n",
    "# We remove 'text' immediately to free up RAM.\n",
    "# num_proc uses multiple CPU cores to tokenize faster.\n",
    "print(\"Tokenizing and chunking (this may take a moment)...\")\n",
    "train_tokenized = train_ds.map(\n",
    "    process_batch, \n",
    "    batched=True, \n",
    "    batch_size=1000, \n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    "    remove_columns=train_ds.column_names,\n",
    "    desc=\"Processing Train\"\n",
    ")\n",
    "\n",
    "val_tokenized = val_ds.map(\n",
    "    process_batch, \n",
    "    batched=True, \n",
    "    batch_size=1000, \n",
    "    num_proc=max(1, multiprocessing.cpu_count() // 2),\n",
    "    remove_columns=val_ds.column_names,\n",
    "    desc=\"Processing Val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9b9c5e5-977f-4759-854e-06604ef30c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Final Train Batches: 117275\n",
      "Input x shape: torch.Size([16, 1024])\n",
      "Target y shape: torch.Size([16, 1024])\n",
      "\n",
      "Sanity Check (Shifting):\n",
      "x[0, -5:]: [355, 326, 3290, 11, 290]\n",
      "y[0, -5:]: [326, 3290, 11, 290, 6451]\n"
     ]
    }
   ],
   "source": [
    "class GPTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.ds = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the chunk of size BLOCK_SIZE + 1\n",
    "        chunk = self.ds[idx][\"chunk_ids\"]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        data = torch.tensor(chunk, dtype=torch.long)\n",
    "\n",
    "        # Shift target, the model needs to learn token_t -> token_t+1\n",
    "        x = data[:-1]\n",
    "        y = data[1:]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Set format to pytorch (optional, but good practice for speed)\n",
    "train_tokenized.set_format(type=\"numpy\", columns=[\"chunk_ids\"])\n",
    "val_tokenized.set_format(type=\"numpy\", columns=[\"chunk_ids\"])\n",
    "\n",
    "train_dataset = GPTDataset(train_tokenized)\n",
    "val_dataset = GPTDataset(val_tokenized)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, # Shuffle chunks\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=prefetch,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=prefetch,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"-\" * 20)\n",
    "print(f\"Final Train Batches: {len(train_loader)}\")\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"Input x shape: {x.shape}\")  # Should be [Batch, Block_Size]\n",
    "print(f\"Target y shape: {y.shape}\") # Should be [Batch, Block_Size]\n",
    "\n",
    "print(\"\\nSanity Check (Shifting):\")\n",
    "print(f\"x[0, -5:]: {x[0, -5:].tolist()}\") # End of input\n",
    "print(f\"y[0, -5:]: {y[0, -5:].tolist()}\") # End of target\n",
    "del x\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f2698c4-2604-4801-a70b-496ef8e6df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with config : \n",
      "{'dropout_prob': 0.1,\n",
      " 'embed_dim': 768,\n",
      " 'mlp_ratio': 4,\n",
      " 'num_heads': 12,\n",
      " 'num_layers': 12,\n",
      " 'use_swiglu': True,\n",
      " 'vocab_size': 50257}\n",
      "Number of parameters: 84.95M\n"
     ]
    }
   ],
   "source": [
    "# --- Model & Optimizer ---\n",
    "from pprint import pprint\n",
    "model_config = {\n",
    "    \"vocab_size\": tokenizer.get_vocab_size(),\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"mlp_ratio\": mlp_ratio,\n",
    "    \"dropout_prob\": dropout_prob,\n",
    "    \"use_swiglu\": True,\n",
    "}\n",
    "\n",
    "print(\"Initializing model with config : \")\n",
    "pprint(model_config)\n",
    "\n",
    "model = GPT(**model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e9f7153-2d7a-44fd-aa7e-628eaa16c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled !\n"
     ]
    }
   ],
   "source": [
    "model = torch.compile(model)   # can take a while\n",
    "model.train()\n",
    "print(\"Model compiled !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ceac27-4430-4319-96ae-d53bf4b15e97",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['_orig_mod.blocks.0.norm1.weight', '_orig_mod.blocks.0.mha.qkv_proj.weight', '_orig_mod.blocks.0.norm2.weight', '_orig_mod.blocks.0.mlp.gate_proj.weight', '_orig_mod.blocks.0.mlp.up_proj.weight', '_orig_mod.blocks.0.mlp.down_proj.weight', '_orig_mod.blocks.1.norm1.weight', '_orig_mod.blocks.1.mha.qkv_proj.weight', '_orig_mod.blocks.1.norm2.weight', '_orig_mod.blocks.1.mlp.gate_proj.weight', '_orig_mod.blocks.1.mlp.up_proj.weight', '_orig_mod.blocks.1.mlp.down_proj.weight', '_orig_mod.blocks.2.norm1.weight', '_orig_mod.blocks.2.mha.qkv_proj.weight', '_orig_mod.blocks.2.norm2.weight', '_orig_mod.blocks.2.mlp.gate_proj.weight', '_orig_mod.blocks.2.mlp.up_proj.weight', '_orig_mod.blocks.2.mlp.down_proj.weight', '_orig_mod.blocks.3.norm1.weight', '_orig_mod.blocks.3.mha.qkv_proj.weight', '_orig_mod.blocks.3.norm2.weight', '_orig_mod.blocks.3.mlp.gate_proj.weight', '_orig_mod.blocks.3.mlp.up_proj.weight', '_orig_mod.blocks.3.mlp.down_proj.weight', '_orig_mod.blocks.4.norm1.weight', '_orig_mod.blocks.4.mha.qkv_proj.weight', '_orig_mod.blocks.4.norm2.weight', '_orig_mod.blocks.4.mlp.gate_proj.weight', '_orig_mod.blocks.4.mlp.up_proj.weight', '_orig_mod.blocks.4.mlp.down_proj.weight', '_orig_mod.blocks.5.norm1.weight', '_orig_mod.blocks.5.mha.qkv_proj.weight', '_orig_mod.blocks.5.norm2.weight', '_orig_mod.blocks.5.mlp.gate_proj.weight', '_orig_mod.blocks.5.mlp.up_proj.weight', '_orig_mod.blocks.5.mlp.down_proj.weight', '_orig_mod.blocks.6.norm1.weight', '_orig_mod.blocks.6.mha.qkv_proj.weight', '_orig_mod.blocks.6.norm2.weight', '_orig_mod.blocks.6.mlp.gate_proj.weight', '_orig_mod.blocks.6.mlp.up_proj.weight', '_orig_mod.blocks.6.mlp.down_proj.weight', '_orig_mod.blocks.7.norm1.weight', '_orig_mod.blocks.7.mha.qkv_proj.weight', '_orig_mod.blocks.7.norm2.weight', '_orig_mod.blocks.7.mlp.gate_proj.weight', '_orig_mod.blocks.7.mlp.up_proj.weight', '_orig_mod.blocks.7.mlp.down_proj.weight', '_orig_mod.blocks.8.norm1.weight', '_orig_mod.blocks.8.mha.qkv_proj.weight', '_orig_mod.blocks.8.norm2.weight', '_orig_mod.blocks.8.mlp.gate_proj.weight', '_orig_mod.blocks.8.mlp.up_proj.weight', '_orig_mod.blocks.8.mlp.down_proj.weight', '_orig_mod.blocks.9.norm1.weight', '_orig_mod.blocks.9.mha.qkv_proj.weight', '_orig_mod.blocks.9.norm2.weight', '_orig_mod.blocks.9.mlp.gate_proj.weight', '_orig_mod.blocks.9.mlp.up_proj.weight', '_orig_mod.blocks.9.mlp.down_proj.weight', '_orig_mod.blocks.10.norm1.weight', '_orig_mod.blocks.10.mha.qkv_proj.weight', '_orig_mod.blocks.10.norm2.weight', '_orig_mod.blocks.10.mlp.gate_proj.weight', '_orig_mod.blocks.10.mlp.up_proj.weight', '_orig_mod.blocks.10.mlp.down_proj.weight', '_orig_mod.blocks.11.norm1.weight', '_orig_mod.blocks.11.mha.qkv_proj.weight', '_orig_mod.blocks.11.norm2.weight', '_orig_mod.blocks.11.mlp.gate_proj.weight', '_orig_mod.blocks.11.mlp.up_proj.weight', '_orig_mod.blocks.11.mlp.down_proj.weight', '_orig_mod.norm.weight'], unexpected_keys=['_orig_mod.ln.weight', '_orig_mod.ln.bias', '_orig_mod.blocks.0.layernorm1.weight', '_orig_mod.blocks.0.layernorm1.bias', '_orig_mod.blocks.0.layernorm2.weight', '_orig_mod.blocks.0.layernorm2.bias', '_orig_mod.blocks.0.mha.q_proj.weight', '_orig_mod.blocks.0.mha.k_proj.weight', '_orig_mod.blocks.0.mha.v_proj.weight', '_orig_mod.blocks.0.mha.out_proj.bias', '_orig_mod.blocks.0.mlp.linear1.weight', '_orig_mod.blocks.0.mlp.linear1.bias', '_orig_mod.blocks.0.mlp.linear2.weight', '_orig_mod.blocks.0.mlp.linear2.bias', '_orig_mod.blocks.1.layernorm1.weight', '_orig_mod.blocks.1.layernorm1.bias', '_orig_mod.blocks.1.layernorm2.weight', '_orig_mod.blocks.1.layernorm2.bias', '_orig_mod.blocks.1.mha.q_proj.weight', '_orig_mod.blocks.1.mha.k_proj.weight', '_orig_mod.blocks.1.mha.v_proj.weight', '_orig_mod.blocks.1.mha.out_proj.bias', '_orig_mod.blocks.1.mlp.linear1.weight', '_orig_mod.blocks.1.mlp.linear1.bias', '_orig_mod.blocks.1.mlp.linear2.weight', '_orig_mod.blocks.1.mlp.linear2.bias', '_orig_mod.blocks.2.layernorm1.weight', '_orig_mod.blocks.2.layernorm1.bias', '_orig_mod.blocks.2.layernorm2.weight', '_orig_mod.blocks.2.layernorm2.bias', '_orig_mod.blocks.2.mha.q_proj.weight', '_orig_mod.blocks.2.mha.k_proj.weight', '_orig_mod.blocks.2.mha.v_proj.weight', '_orig_mod.blocks.2.mha.out_proj.bias', '_orig_mod.blocks.2.mlp.linear1.weight', '_orig_mod.blocks.2.mlp.linear1.bias', '_orig_mod.blocks.2.mlp.linear2.weight', '_orig_mod.blocks.2.mlp.linear2.bias', '_orig_mod.blocks.3.layernorm1.weight', '_orig_mod.blocks.3.layernorm1.bias', '_orig_mod.blocks.3.layernorm2.weight', '_orig_mod.blocks.3.layernorm2.bias', '_orig_mod.blocks.3.mha.q_proj.weight', '_orig_mod.blocks.3.mha.k_proj.weight', '_orig_mod.blocks.3.mha.v_proj.weight', '_orig_mod.blocks.3.mha.out_proj.bias', '_orig_mod.blocks.3.mlp.linear1.weight', '_orig_mod.blocks.3.mlp.linear1.bias', '_orig_mod.blocks.3.mlp.linear2.weight', '_orig_mod.blocks.3.mlp.linear2.bias', '_orig_mod.blocks.4.layernorm1.weight', '_orig_mod.blocks.4.layernorm1.bias', '_orig_mod.blocks.4.layernorm2.weight', '_orig_mod.blocks.4.layernorm2.bias', '_orig_mod.blocks.4.mha.q_proj.weight', '_orig_mod.blocks.4.mha.k_proj.weight', '_orig_mod.blocks.4.mha.v_proj.weight', '_orig_mod.blocks.4.mha.out_proj.bias', '_orig_mod.blocks.4.mlp.linear1.weight', '_orig_mod.blocks.4.mlp.linear1.bias', '_orig_mod.blocks.4.mlp.linear2.weight', '_orig_mod.blocks.4.mlp.linear2.bias', '_orig_mod.blocks.5.layernorm1.weight', '_orig_mod.blocks.5.layernorm1.bias', '_orig_mod.blocks.5.layernorm2.weight', '_orig_mod.blocks.5.layernorm2.bias', '_orig_mod.blocks.5.mha.q_proj.weight', '_orig_mod.blocks.5.mha.k_proj.weight', '_orig_mod.blocks.5.mha.v_proj.weight', '_orig_mod.blocks.5.mha.out_proj.bias', '_orig_mod.blocks.5.mlp.linear1.weight', '_orig_mod.blocks.5.mlp.linear1.bias', '_orig_mod.blocks.5.mlp.linear2.weight', '_orig_mod.blocks.5.mlp.linear2.bias', '_orig_mod.blocks.6.layernorm1.weight', '_orig_mod.blocks.6.layernorm1.bias', '_orig_mod.blocks.6.layernorm2.weight', '_orig_mod.blocks.6.layernorm2.bias', '_orig_mod.blocks.6.mha.q_proj.weight', '_orig_mod.blocks.6.mha.k_proj.weight', '_orig_mod.blocks.6.mha.v_proj.weight', '_orig_mod.blocks.6.mha.out_proj.bias', '_orig_mod.blocks.6.mlp.linear1.weight', '_orig_mod.blocks.6.mlp.linear1.bias', '_orig_mod.blocks.6.mlp.linear2.weight', '_orig_mod.blocks.6.mlp.linear2.bias', '_orig_mod.blocks.7.layernorm1.weight', '_orig_mod.blocks.7.layernorm1.bias', '_orig_mod.blocks.7.layernorm2.weight', '_orig_mod.blocks.7.layernorm2.bias', '_orig_mod.blocks.7.mha.q_proj.weight', '_orig_mod.blocks.7.mha.k_proj.weight', '_orig_mod.blocks.7.mha.v_proj.weight', '_orig_mod.blocks.7.mha.out_proj.bias', '_orig_mod.blocks.7.mlp.linear1.weight', '_orig_mod.blocks.7.mlp.linear1.bias', '_orig_mod.blocks.7.mlp.linear2.weight', '_orig_mod.blocks.7.mlp.linear2.bias', '_orig_mod.blocks.8.layernorm1.weight', '_orig_mod.blocks.8.layernorm1.bias', '_orig_mod.blocks.8.layernorm2.weight', '_orig_mod.blocks.8.layernorm2.bias', '_orig_mod.blocks.8.mha.q_proj.weight', '_orig_mod.blocks.8.mha.k_proj.weight', '_orig_mod.blocks.8.mha.v_proj.weight', '_orig_mod.blocks.8.mha.out_proj.bias', '_orig_mod.blocks.8.mlp.linear1.weight', '_orig_mod.blocks.8.mlp.linear1.bias', '_orig_mod.blocks.8.mlp.linear2.weight', '_orig_mod.blocks.8.mlp.linear2.bias', '_orig_mod.blocks.9.layernorm1.weight', '_orig_mod.blocks.9.layernorm1.bias', '_orig_mod.blocks.9.layernorm2.weight', '_orig_mod.blocks.9.layernorm2.bias', '_orig_mod.blocks.9.mha.q_proj.weight', '_orig_mod.blocks.9.mha.k_proj.weight', '_orig_mod.blocks.9.mha.v_proj.weight', '_orig_mod.blocks.9.mha.out_proj.bias', '_orig_mod.blocks.9.mlp.linear1.weight', '_orig_mod.blocks.9.mlp.linear1.bias', '_orig_mod.blocks.9.mlp.linear2.weight', '_orig_mod.blocks.9.mlp.linear2.bias', '_orig_mod.blocks.10.layernorm1.weight', '_orig_mod.blocks.10.layernorm1.bias', '_orig_mod.blocks.10.layernorm2.weight', '_orig_mod.blocks.10.layernorm2.bias', '_orig_mod.blocks.10.mha.q_proj.weight', '_orig_mod.blocks.10.mha.k_proj.weight', '_orig_mod.blocks.10.mha.v_proj.weight', '_orig_mod.blocks.10.mha.out_proj.bias', '_orig_mod.blocks.10.mlp.linear1.weight', '_orig_mod.blocks.10.mlp.linear1.bias', '_orig_mod.blocks.10.mlp.linear2.weight', '_orig_mod.blocks.10.mlp.linear2.bias', '_orig_mod.blocks.11.layernorm1.weight', '_orig_mod.blocks.11.layernorm1.bias', '_orig_mod.blocks.11.layernorm2.weight', '_orig_mod.blocks.11.layernorm2.bias', '_orig_mod.blocks.11.mha.q_proj.weight', '_orig_mod.blocks.11.mha.k_proj.weight', '_orig_mod.blocks.11.mha.v_proj.weight', '_orig_mod.blocks.11.mha.out_proj.bias', '_orig_mod.blocks.11.mlp.linear1.weight', '_orig_mod.blocks.11.mlp.linear1.bias', '_orig_mod.blocks.11.mlp.linear2.weight', '_orig_mod.blocks.11.mlp.linear2.bias'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckpt_path = \"ckpts/gpt_model_1024_final.pt\"\n",
    "\n",
    "# state_dict = torch.load(ckpt_path, map_location=device)\n",
    "# model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2e581bf-67ec-4db3-9bcd-a1516f736f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Micro-batches: 500000\n",
      "Gradient Accumulation: 40\n",
      "Total Optimizer Updates: 12500\n"
     ]
    }
   ],
   "source": [
    "# Calculate how many actual updates we will do (for the scheduler)\n",
    "\n",
    "total_optim_steps = MAX_STEPS // GRAD_ACCUM_STEPS\n",
    "print(f\"Total Micro-batches: {MAX_STEPS}\")\n",
    "print(f\"Gradient Accumulation: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Total Optimizer Updates: {total_optim_steps}\")\n",
    "\n",
    "\n",
    "# T_max is now based on actual optimizer updates, not total loops\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_optim_steps, eta_min=1e-5)\n",
    "\n",
    "# --- CSV Logger ---\n",
    "log_file = \"training_log.csv\"\n",
    "file_exists = os.path.isfile(log_file)\n",
    "with open(log_file, \"a\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    if not file_exists:\n",
    "        writer.writerow([\"micro_step\", \"optim_step\", \"loss\", \"lr\", \"tokens_seen\", \"tokens_per_sec\", \"timestamp\"])\n",
    "\n",
    "# --- Training Loop ---\n",
    "micro_step = 0      # Counts every batch seen\n",
    "optim_step = 0      # Counts every weight update\n",
    "tokens_seen = 0\n",
    "running_loss = 0.0\n",
    "start_time = time.time()\n",
    "start_training = time.time()\n",
    "\n",
    "# Initialize gradients once before starting\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n",
    "# Train until we've seen enough tokens\n",
    "while micro_step < MAX_STEPS:\n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        B, T = x.shape\n",
    "        tokens_seen += B * T\n",
    "\n",
    "        # 1. Forward\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        # 2. Scale Loss for Backward (but keep original for logging!)\n",
    "        current_loss_val = loss.item() \n",
    "        scaled_loss = loss / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        # 3. Backward\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        # 4. Step (only every 40 micro-steps)\n",
    "        if (micro_step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            # avoids exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            optim_step += 1\n",
    "\n",
    "        # 5. Bookkeeping\n",
    "        running_loss += current_loss_val\n",
    "        micro_step += 1\n",
    "\n",
    "        # 6. Logging\n",
    "        if micro_step % LOG_INTERVAL == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_loss = running_loss / LOG_INTERVAL\n",
    "            tokens_per_sec = (B * T * LOG_INTERVAL) / elapsed\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            print(\n",
    "                f\"step {micro_step:06d} | \"\n",
    "                f\"opt_step {optim_step:04d} | \"\n",
    "                f\"loss {avg_loss:.3f} | \"\n",
    "                f\"lr {current_lr:.2e} | \"\n",
    "                f\"{tokens_per_sec:,.0f} tok/s\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                with open(log_file, \"a\", newline=\"\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([micro_step, optim_step, f\"{avg_loss:.4f}\", f\"{current_lr:.2e}\", tokens_seen, int(tokens_per_sec), timestamp])\n",
    "            except Exception as e:\n",
    "                print(f\"CSV Error: {e}\")\n",
    "\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "\n",
    "        if micro_step % 60000 == 0:\n",
    "            mid_model_path = model_path.replace(\".pt\", f\"_{micro_step}.pt\")\n",
    "            print(f\"Saving intermediate model in {mid_model_path}\")\n",
    "            torch.save(model.state_dict(), mid_model_path)\n",
    "        \n",
    "        if micro_step >= MAX_STEPS:\n",
    "            elapsed = int(time.time() - start_training)\n",
    "            h = elapsed // 3600\n",
    "            m = (elapsed % 3600) // 60\n",
    "            s = elapsed % 60\n",
    "            print(f\"\\nProcessed {tokens_seen:,} tokens in {h:02d}:{m:02d}:{s:02d}\")\n",
    "            print(f\"Saving final model in {model_path}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31b28c-f177-4b1e-b889-b462ca79e964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba29a505-812a-470b-aed9-1157488fffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = val_dataset.__getitem__(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c47b15f-d640-41d4-b042-a7588dba2f1e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atres . Phase I of the construction of Meridian Crossroads , a 375 @,@ 000 @-@ square @-@ foot ( 34 @,@ 800 m2 ) shopping center in the Bonita Lakes area , was completed in November 2007 , providing a major boost to retail in the area . Also , the shopping district on North Hills Street has continued to expand , and in March 2007 , additional retail and office space was opened near the Highway 19 Walmart Supercenter . \n",
      " The area is also served by two military facilities , Naval Air Station Meridian and Key Field , which supply over 4 @,@ 000 jobs to residents of the surrounding area . NAS Meridian provides training for naval carrier pilots and other enlisted personnel . Also housed at the base is the Regional Counter @-@ Drug Training Academy ( RCTA ) , which provides narcotics training for law enforcement in many southeastern states . Containing the first local Department of Homeland Security in the state , the city is the leader in a nine county regional response team and a twenty @-@ nine county regional response task force . Key Field is the site of the famous flight by brothers Fred and Al Key , who set a world endurance flight record in 1935 . Key Field is now home to the 186th Air Refueling Wing of the Air National Guard and a support facility for the 185th Aviation Brigade of the Army National Guard . The site also contains an exhibit reviewing the history of aviation , and is the home of Meridian 's Aviation Museum . \n",
      " The total manufacturing employment of Lauderdale County in April 2010 was 2 @,@ 850 people . Peavey Electronics Corporation , which has manufactured guitars , amplifiers , and sound equipment since 1965 , operates its headquarters in the city . Other businesses in the area include Avery Dennison , Structural Steel Services , Sara Lee , Tower Automotive , and Teikuro Corporation . The city is also home to four industrial parks . \n",
      " In downtown , the MSU Riley Center provides revenue from tourism , arts , and entertainment sales . The Riley Center attracts more than 60 @,@ 000 visitors to downtown Meridian annually for conferences , meetings , and performances . Loeb 's Department Store on Front St has remained a Mississippi clothing landmark , having passed through four generations of family ownership . The store has been selling fine men 's and women 's clothing since 1887 , when the store was first opened by Alex Loeb . \n",
      " = = Culture = = \n",
      " = = = Arts = = = \n",
      " Known for more than a century of arts , Meridian contains many art and cultural organizations and hosts many cultural events . One of the first art organizations in the city , The Meridian Art League , was established in February 1933 . Art exhibitions were originally held in Lamar Hotel in downtown Meridian , but after a name change to Meridian Art Association in 1949 , exhibitions were held at various locations around the city . After the Carnegie library at 25th Ave and 7th St was closed , the Art Association remodelled the building into the Meridian Museum of Art to serve as a permanent home for exhibits . The museum was opened in 1970 and has since featured rotating exhibitions as well as many educational programs for both students and adults . Over thirty exhibitions are held annually , ranging from traditional decorative arts to ethnographic and tribal materials , photography , crafts , and many other works of art . The collection also includes 18th and 19th century portraits , 20th century photography , and several sculptures . \n",
      " The Meridian Council for the Arts ( MCA ) was founded as Meridian 's and Lauderdale County 's official arts agency in 1978 . MCA operates its Community Art Grants program , the annual Threefoot Festival , several workshops , and other special events each year . MCA is partnered with many arts organizations in the city and county including the Meridian Museum of Art , the Meridian Little Theatre , and the Meridian Symphony Orchestra . Meridian Little Theatre , one of the South 's oldest subscription @-@ based community theatres , was built in 1932 and currently provides entertainment to residents of and visitors to Meridian and Lauderdale County , entertaining over 22 @,@ 000 guests each season , making it Mississippi 's most @-@ attended community theatre . The Meridian Symphony Orchestra ( MSO ) – founded in 1961 – played its first concert in 1962 and its first full season in 1963 . In 1965 the MSO booked its first international soloist , Elena Nikolaidi , to perform with the orchestra . The Orchestra helped the Meridian Public School District develop its own orchestra and strings programs and also helped develop the Meridian Symphony Chorus . The current conductor is Dr. Claire Fox Hillard , who has been with the orchestra since 1991 . The MSO will celebrate its 50th anniversary in February 2011 with a performance from Itzhak Perlman . \n",
      " The city 's former Grand Opera House was built in 1889 by two half brothers , Israel Marks and Levi Rothenberg . During its operation the opera house hosted many famous artists and works , the first being a German company 's rendition of Johann Strauss II 's \" The Gypsy Baron\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(x.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47d3a1ef-1db0-4658-acdd-a68757294434",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The 2021 Masters (officially the 2021 Betfred Masters) was a professional non-ranking snooker tournament that took place from 10 to 17 January 2021 at the Marshall Arena in Milton Keynes, England\"\n",
    "x = torch.tensor(tokenizer.encode(prompt).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d4a028a-7866-40ad-ba90-570cb8bb614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output :  The 2021 Masters (officially the 2021 Betfred Masters) was a professional non-ranking snooker tournament that took place from 10 to 17 January 2021 at the Marshall Arena in Milton Keynes, England.\n",
      "\n",
      "The competition was scheduled from 1 April to 12 April, with the rest of the UK qualifying for an additional round to 1 April, with the prize money divided between £10,000 and £100,000.\n",
      "\n",
      "The £10,000 was to be split into £10,000 and £100,000, respectively.\n",
      "\n",
      "The event was contested on 3 May-June 2021 at the Milton Keynes Stadium in Milton Keynes, England\n",
      "\n",
      "The draw gave the North London Women a total of £858,000 to be sold, with a total prize of £2,300,000 added to the £50,000 prize money\n",
      "\n",
      "The final draw was for the second round of the men’s tournament in Boston, England. They have been on the run since the World Championships, but have not been on the Tour.\n",
      "\n",
      "The winner of the bid is the runner up and will be invited to attend the competition.\n",
      "\n",
      "(Image: Getty)\n",
      "\n",
      "The winner of the third round of the women’s world cup has just been announced\n",
      "\n",
      "Worst winner of the final draw of the women’s semi-final in Moscow, Russia\n",
      "\n",
      "FIGHTING UP\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "out = model.generate(\n",
    "    x.unsqueeze(0).to(\"cuda\"),\n",
    "    max_new_tokens=250,\n",
    "    temperature=0.9,\n",
    "    use_cache=True,\n",
    "    use_top_k=True,\n",
    ")\n",
    "\n",
    "print(\"\\nOutput : \", tokenizer.decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d1452-0749-4415-a850-6138ae435283",
   "metadata": {},
   "source": [
    "## TO DO : \n",
    "\n",
    "- [x] ROPE for K, V, Q\n",
    "- [x] Top k sampling / Temperature\n",
    "- [x] K / V cache\n",
    "- [x] Add stop token / EOS handling\n",
    "- [x] Training on a real problem to see how far we can push current model\n",
    "- [ ] Clean up / Revisit markdown / maths\n",
    "- [ ] Explore hyper connections and manifold constrained HC\n",
    "- [ ] Check newer architectures / design choices (https://github.com/lucidrains git is a gold mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7b86e-2803-4838-840f-e22e311cb940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
