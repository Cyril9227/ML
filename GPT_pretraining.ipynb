{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "First notebook was written from 'scratch', this one leverages existing libraries to experiment with actual training and inference.\n",
    "\n",
    "I also added some improvements over baseline model : \n",
    "- Moved attention computation to optimized `F.scaled_dot_product_attention`\n",
    "- Moved `LayerNorm` to `RMSNorm`, which is the standard now\n",
    "- Moved `GELU` to `SWIGLU`\n",
    "- Moved positional encoding to `RoPE` on `Q` and `K`\n",
    "- Disabled bias in every linear layers\n",
    "- Grouped `Q`, `V`, `K` projections into 1\n",
    "- Padded embedding to nearest % of 128 for GPU optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cu130\n",
      "torch.cuda.is_bf16_supported() -> True\n",
      "\n",
      "Thu Jan 22 13:26:28 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 5000 Ada Gene...    Off |   00000000:17:00.0 Off |                  Off |\n",
      "| 30%   42C    P2             51W /  250W |     369MiB /  32760MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         2158493      C   /opt/conda/bin/python                   360MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import csv\n",
    "import inspect\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment config\n",
    "from huggingface_hub import login\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "if os.path.isfile(\".env\"):\n",
    "    with open(\".env\") as f:\n",
    "        for line in f:\n",
    "            key, value = line.strip().split(\"=\")\n",
    "            os.environ[key] = value\n",
    "\n",
    "    HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "    login(HF_TOKEN)\n",
    "\n",
    "\n",
    "# Third-party\n",
    "import tiktoken\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from datasets import concatenate_datasets, load_dataset, Dataset as ds  # 3.6.0 to avoid issues with load_dataset\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "\n",
    "# Custom\n",
    "from utils import clean_columns, round_up, strip_compile_prefix\n",
    "\n",
    "# Torch runtime config\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "dtype = torch.bfloat16  \n",
    "print(f\"torch.cuda.is_bf16_supported() -> {torch.cuda.is_bf16_supported()}\\n\")\n",
    "\n",
    "torch.empty(\n",
    "    1, device=f\"cuda:{os.environ.get('LOCAL_RANK', 0)}\", requires_grad=True\n",
    ").backward()  # prevents a bug on some systems\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 rotary_emb: RotaryEmbedding,\n",
    "                 causal: bool = True,\n",
    "                 dropout: float = 0.1\n",
    "                ):\n",
    "        super().__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "        \n",
    "        self.causal = causal\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout_p = dropout\n",
    "        \n",
    "        # Fused QKV projection: 3x the output size\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        \n",
    "        # Shared rotary embedding (passed from GPT model)\n",
    "        self.rotary_emb = rotary_emb\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, k_v_cache=None):\n",
    "        B, T, _ = x.shape\n",
    "        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n",
    "    \n",
    "        # 1. Single fused projection\n",
    "        if using_cache:\n",
    "            x_q = x[:, -1:, :]\n",
    "            qkv = self.qkv_proj(x_q)  # (B, 1, 3 x embed_dim)\n",
    "        else:\n",
    "            qkv = self.qkv_proj(x)  # (B, T, 3 x embed_dim)\n",
    "        \n",
    "        # 2. Split into Q, K, V\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)  # Each is (B, T, embed_dim)\n",
    "        \n",
    "        def split_heads(t):\n",
    "            return t.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 3. Split heads -> (B, H, T, D_head)\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "    \n",
    "        # 4. Apply RoPE \n",
    "        if using_cache:\n",
    "            past_len = k_v_cache[\"K\"].shape[-2]\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q, offset=past_len)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K, offset=past_len)\n",
    "            \n",
    "            K = torch.cat([k_v_cache[\"K\"], K], dim=-2)\n",
    "            V = torch.cat([k_v_cache[\"V\"], V], dim=-2)\n",
    "            # When using the cache, the \"causality\" is already enforced by the fact that we are passing 1 query token against all valid past keys \n",
    "            # We don't need a mask as we want the current token to attend to everything in the history\n",
    "            is_causal_step = False\n",
    "        else:\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K)\n",
    "            is_causal_step = self.causal\n",
    "    \n",
    "        # 5. Update cache\n",
    "        if k_v_cache is not None:\n",
    "            k_v_cache[\"K\"] = K.detach()  # we will never .backward on these\n",
    "            k_v_cache[\"V\"] = V.detach()  \n",
    "    \n",
    "        # 6. Attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            query=Q,\n",
    "            key=K,\n",
    "            value=V,\n",
    "            attn_mask=None, \n",
    "            dropout_p=self.dropout_p if self.training else 0.0,\n",
    "            is_causal=is_causal_step\n",
    "        )\n",
    "        \n",
    "        # 7. Merge heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, -1, self.embed_dim)\n",
    "\n",
    "        # 8. Linear projection\n",
    "        return self.out_proj(out), k_v_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf4ba2d-402d-4eab-86e1-754f9faeaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim=None, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * embed_dim\n",
    "        \n",
    "        # Adjust hidden_dim for param count matching\n",
    "        # For perf it's important to be % 8\n",
    "        hidden_dim = round_up(2 * hidden_dim // 3, 8)\n",
    "\n",
    "        # Fused projection\n",
    "        self.gate_up_proj = nn.Linear(embed_dim, 2 * hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "\n",
    "        # Manual implementation would be this\n",
    "        # self.gate_proj = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        # self.up_proj = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        # self.down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        # forward pass : return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        # For performance sake we should maybe use xformers fused kernel instead, specially opt for bf16\n",
    "        # Actually for very small tensors the manual implementation is almost 2x faster\n",
    "        # But in real setting i.e 16-32 sequences of 1024 tokens, this implementation is ~ 25% faster (%timeit)\n",
    "        # HOWEVER : compile + bf16 + manual FASTER then bf16 + xformers.swiglu (cant compile without errors)\n",
    "        # Bottom line : we can keep manual for the purpose of this notebook\n",
    "        # from xformers.ops import SwiGLU\n",
    "        # self.swiglu = SwiGLU(\n",
    "        #     in_features=embed_dim,\n",
    "        #     hidden_features=hidden_dim,\n",
    "        #     out_features=embed_dim,\n",
    "        #     bias=False,\n",
    "        #     _pack_weights=True,  # on by default but for clarity\n",
    "        # )\n",
    "        # forward pass : return self.dropout(self.swiglu(x))\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        gate_up = self.gate_up_proj(x)\n",
    "        gate, up = gate_up.chunk(2, dim=-1)\n",
    "        return self.dropout(self.down_proj(F.silu(gate) * up))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Fused Cross-Entropy Loss (Triton)\n",
    "# ============================================================================\n",
    "#\n",
    "# Stolen err adapted from modded-nanogpt\n",
    "# Idea is that computing Softmax CE on large vocab is super expensive so we try to fuse ops together\n",
    "# TODO: Implement SoftCap on logits\n",
    "#\n",
    "# Benefits: \n",
    "# - No need to materialize full [B*T, vocab_size] probability tensor\n",
    "# - Single kernel pass instead of softmax + nll_loss\n",
    "# - ~2-3x faster for large vocabularies\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def fused_cross_entropy_fwd_kernel(\n",
    "    logits_ptr, losses_ptr, lse_ptr, targets_ptr,\n",
    "    stride_logits_row,\n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    \"\"\"\n",
    "    Fused forward pass: computes loss = -logits[target] + logsumexp(logits)\n",
    "    without materializing softmax probabilities.\n",
    "    \"\"\"\n",
    "    row_idx = tl.program_id(0).to(tl.int64)\n",
    "    logits_row_ptr = logits_ptr + row_idx * stride_logits_row\n",
    "    \n",
    "    # Compute logsumexp in numerically stable way (online algorithm)\n",
    "    max_val = -float('inf')\n",
    "    sum_exp = 0.0\n",
    "    \n",
    "    for off in range(0, n_cols, BLOCK_SIZE):\n",
    "        cols = off + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = cols < n_cols\n",
    "        logits = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)\n",
    "        \n",
    "        # Update running max and sum for logsumexp\n",
    "        curr_max = tl.max(logits, axis=0)\n",
    "        new_max = tl.maximum(max_val, curr_max)\n",
    "        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(logits - new_max), axis=0)\n",
    "        max_val = new_max\n",
    "    \n",
    "    lse = max_val + tl.log(sum_exp)\n",
    "    tl.store(lse_ptr + row_idx, lse)\n",
    "    \n",
    "    # Load target and compute loss\n",
    "    target = tl.load(targets_ptr + row_idx).to(tl.int64)\n",
    "    target_logit = tl.load(logits_row_ptr + target).to(tl.float32)\n",
    "    loss = lse - target_logit\n",
    "    \n",
    "    tl.store(losses_ptr + row_idx, loss)\n",
    "\n",
    "\n",
    "@triton.jit  \n",
    "def fused_cross_entropy_bwd_kernel(\n",
    "    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr,\n",
    "    stride_logits_row, stride_grad_row,\n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    \"\"\"\n",
    "    Fused backward pass: grad[i] = softmax[i] - (1 if i==target else 0)\n",
    "    Recomputes softmax from logits and lse, avoiding memory overhead.\n",
    "    \"\"\"\n",
    "    row_idx = tl.program_id(0).to(tl.int64)\n",
    "    logits_row_ptr = logits_ptr + row_idx * stride_logits_row\n",
    "    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_row\n",
    "    \n",
    "    lse = tl.load(lse_ptr + row_idx)\n",
    "    grad_loss = tl.load(grad_output_ptr + row_idx)\n",
    "    target = tl.load(targets_ptr + row_idx).to(tl.int64)\n",
    "    \n",
    "    for off in range(0, n_cols, BLOCK_SIZE):\n",
    "        cols = off + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = cols < n_cols\n",
    "        \n",
    "        logits = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)\n",
    "        # softmax = exp(logits - lse)\n",
    "        probs = tl.exp(logits - lse)\n",
    "        # gradient = grad_loss * (probs - one_hot(target))\n",
    "        is_target = (cols == target).to(tl.float32)\n",
    "        grad = grad_loss * (probs - is_target)\n",
    "        \n",
    "        tl.store(grad_row_ptr + cols, grad.to(tl.bfloat16), mask=mask)\n",
    "\n",
    "\n",
    "class FusedCrossEntropyLoss(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Memory-efficient cross-entropy that doesn't materialize the full probability matrix.\n",
    "    For vocab=50k, batch=16, seq=1024: saves ~3GB of memory per forward pass.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits, targets):\n",
    "        n_rows, n_cols = logits.shape\n",
    "        \n",
    "        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)\n",
    "        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)\n",
    "        \n",
    "        logits = logits.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "        \n",
    "        # One thread block per row (token)\n",
    "        grid = (n_rows,)\n",
    "        fused_cross_entropy_fwd_kernel[grid](\n",
    "            logits, losses, lse, targets,\n",
    "            logits.stride(0),\n",
    "            n_cols,\n",
    "            BLOCK_SIZE=1024,\n",
    "            num_warps=8,\n",
    "        )\n",
    "        \n",
    "        ctx.save_for_backward(logits, targets, lse)\n",
    "        return losses\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        logits, targets, lse = ctx.saved_tensors\n",
    "        n_rows, n_cols = logits.shape\n",
    "        \n",
    "        grad_input = torch.empty_like(logits, dtype=torch.bfloat16)\n",
    "        grad_output = grad_output.contiguous()\n",
    "        \n",
    "        grid = (n_rows,)\n",
    "        fused_cross_entropy_bwd_kernel[grid](\n",
    "            grad_input, grad_output, lse, logits, targets,\n",
    "            logits.stride(0), grad_input.stride(0),\n",
    "            n_cols,\n",
    "            BLOCK_SIZE=1024,\n",
    "            num_warps=8,\n",
    "        )\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "def fused_cross_entropy(logits, targets):\n",
    "    \"\"\"Drop-in replacement for F.cross_entropy with fused kernel.\"\"\"\n",
    "    return FusedCrossEntropyLoss.apply(logits, targets).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c04d073-6e3f-469d-98b4-5cda92115923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 rotary_emb,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 causal=True,\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Initialize a complete transformer block.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Multi-head self-attention for sequence modeling\n",
    "        2. 1st Normalization (pre-norm architecture)\n",
    "        3. MLP with specified expansion ratio\n",
    "        4. 2nd Normalization\n",
    "    \n",
    "        TRANSFORMER BLOCK ARCHITECTURE:\n",
    "        x → Norm → MultiHeadAttention → + (residual) →\n",
    "            Norm → MLP → + (residual) → output\n",
    "    \n",
    "        NB: We use pre-norm architecture (before attention/MLP)\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "        self.norm1 = nn.RMSNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, rotary_emb, causal, dropout_prob)  # causal = masking out tokens\n",
    "        self.norm2 = nn.RMSNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio * embed_dim, dropout_prob)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        x1 = self.norm1(x)\n",
    "        x2, cache = self.mha(x1, cache)  # will be used when generating tokens during inference\n",
    "        x2 = x2 + x  # residual path\n",
    "    \n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.mlp(x3) + x2  # residual path\n",
    "        return x3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT (Generative Pre-trained Transformer) model.\n",
    "\n",
    "    This combines embeddings, positional encoding, multiple transformer blocks,\n",
    "    and a language modeling head for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 is_causal=True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize complete GPT model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Shared rotary embedding across all layers (more efficient for compilation)\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.rotary_emb = RotaryEmbedding(dim=head_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, self.rotary_emb, mlp_ratio, dropout_prob, is_causal) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.RMSNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # weight tying\n",
    "\n",
    "        # below shamefully stolen from nano-gpt\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        # don't forget swiglu variant !\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith((\"out_proj.weight\", \"down_proj.weight\")):\n",
    "                # Residual projections: scale down to prevent variance explosion\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.num_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "       \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.embedding(tokens)\n",
    "        x = self.dropout(embeddings)\n",
    "        for b in self.blocks:\n",
    "            x, _ = b(x)\n",
    "        features = self.norm(x)  # normalized to stabilize training\n",
    "        return self.lm_head(features)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 prompt_tokens,\n",
    "                 max_new_tokens=50,\n",
    "                 temperature=1.0,\n",
    "                 top_k=0,\n",
    "                 top_p=0.0,\n",
    "                 use_cache=True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Auto-regressive text generation loop\n",
    "\n",
    "        prompt_tokens : tensor with input tokens, shape (1, n_tokens)\n",
    "        max_new_tokens : how many new tokens we want to generate\n",
    "        temperature : controls expressivity (lower = less, higher = more funky, degenerate cases : 0 = argmax, +inf = random guess)\n",
    "        top_k : restrict prediction to top_k tokens to avoid sampling low prob garbage, set to 0 to disable, top_k ∈ [0, vocab_size]\n",
    "        top_p : sample from smallest set with cumulative prob >= top_p (adapts to model confidence, usually top_k OR top_p), top_p ∈ [0, 1]\n",
    "        use_cache : set to True to avoid re-computing expensive K, V matrices\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.eval()\n",
    "\n",
    "        tokens_out = prompt_tokens.clone()\n",
    "        current_tokens = prompt_tokens.clone()\n",
    "        tokens_out = tokens_out.to(self.device)\n",
    "        current_tokens = current_tokens.to(self.device)\n",
    "        cache = [{} if use_cache else None for _ in range(len(self.blocks))]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            x = self.embedding(current_tokens)\n",
    "            for i, b in enumerate(self.blocks):\n",
    "                x, c_i = b(x, cache[i])\n",
    "                cache[i] = c_i\n",
    "            \n",
    "            features = self.norm(x)\n",
    "            logits = self.lm_head(features)    \n",
    "            last_logits = logits[:, -1, :]\n",
    "    \n",
    "            if temperature == 0:\n",
    "                # Greedy decoding if temp is 0\n",
    "                next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                # \"reshape the distribution\", i.e crushing logits before softmax ~ uniform distribution etc.\n",
    "                scaled_logits = last_logits / temperature\n",
    "                \n",
    "                # Only sample from top k tokens to avoid garbage prediction derailing whole prediction\n",
    "                if int(top_k) > 0:\n",
    "                    # most of probability mass in on a small amount of tokens, maybe 50 ?\n",
    "                    values, indices = torch.topk(scaled_logits, top_k)\n",
    "                    scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n",
    "                    scaled_logits.scatter_(1, indices, values)\n",
    "\n",
    "                # TODO : DISABLE top_k + top_p ? Modern implementation *usually* only expose top_p\n",
    "                if top_p > 0.0 and top_p < 1.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True, dim=-1)\n",
    "                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    # Remove tokens with cumulative probability above the threshold\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    # Shift right to keep at least one token (the first one)\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "                    # Set logits to -inf for tokens we want to remove\n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                    scaled_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # logits -> distribution probability\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                # Sample from prob distribution, nb : we don't simply take max prob token to allow \"creativity\"\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Stop generating if model thinks the \"document\" is finished\n",
    "            # eot_id = tokenizer.eot_token\n",
    "            if next_token.item() == eot_id:\n",
    "                break\n",
    "            \n",
    "            tokens_out = torch.cat([tokens_out, next_token], dim=1)\n",
    "\n",
    "            # If caching, we only need to feed the newest token next time, otherwise full sequence\n",
    "            current_tokens = next_token if use_cache else tokens_out\n",
    "       \n",
    "        return tokens_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec6e7e-9bce-467f-9c36-aadb01c2ec5f",
   "metadata": {},
   "source": [
    "## Full Training\n",
    "\n",
    "As I do not have unlimited compute, our datasets need to be as clean as possible and high signal. C4 / FineWeb will have massive overlap with OpenWebText (from same common crawl), wikipedia is probably going to be oversampled, which should be fine, bookcorpus is mostly self-published fictions, should have less overlap but it has known issues.\n",
    "\n",
    "I thought adding maths and code would be a good idea but it introduces some issues with learning new semantics / syntax etc. might be too ambitious for a small(er) scale model. Arxiv is definitely a no-no, open-web-math might be fine as it's more informal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04a17c6-e96f-4943-827d-649214ae44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CONFIG #####\n",
    "\n",
    "# Basically GPT-2 Small\n",
    "block_size = 1024  # 512 for debug, 2048 realistically too slow\n",
    "batch_size = 32    # sweet spot, have some room to increase\n",
    "embed_dim = 768\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "dropout_prob = 0.05  # usually set as 0 but my model being relatively smaller, should help\n",
    "mlp_ratio = 4  # standard 4x expansion\n",
    "\n",
    "\n",
    "# Training\n",
    "MAX_STEPS = 700000                         # Total number of micro-batches to process\n",
    "GRAD_ACCUM_STEPS = 20                      # Better for GPU ?\n",
    "LOG_INTERVAL = MAX_STEPS // 1000           # Log every xxx micro-batches\n",
    "num_workers = 4                            # For data loading\n",
    "prefetch = 8\n",
    "device = \"cuda\"\n",
    "model_path = f\"gpt_full_run.pt\"            # where do we store trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92cd5fa-1da3-4e79-8760-6c9faadd9516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "✓ Loaded tiktoken gpt2\n",
      "  Vocab size: 50,304\n",
      "  EOT token ID: 50256\n",
      "\n",
      "Loading & cleaning up datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bced1367fa9422eac359e26b75eb137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f3ba64e9bb4acbacb5a2c278b321af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc87e3174784a96850eef9d0b266469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded 4 datasets\n",
      "  Dataset fineweb-edu: 9,672,101 examples\n",
      "  Dataset wiki: 640,781 examples\n",
      "  Dataset bookcorpus: 7,400,423 examples\n",
      "  Dataset maths-qa: 200,035 examples\n",
      "\n",
      "Concatenating datasets...\n",
      "  Combined size: 17,913,340 examples\n",
      "Shuffling...\n",
      "✓ Final Train Size: 17,913,340 rows\n",
      "\n",
      "Cleaning special tokens from datasets...\n",
      "\n",
      "Tokenizing and packing documents (this may take a few minutes)...\n",
      "\n",
      "✓ Tokenization complete!\n",
      "  Total chunks: 10,450,143\n",
      "  Chunk size: 1025 tokens\n",
      "  Approx total tokens: 10,700,946,432\n",
      "\n",
      "✓ DataLoader ready\n",
      "  Batches per epoch: 326,566\n",
      "  Tokens per batch: 32,768\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Setup Tokenizer (tiktoken gpt2 - 50,257 vocab, much better for small models)\n",
    "# ============================================================================\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "eot_id = tokenizer.eot_token\n",
    "# original gpt2 tokenizer has an awkward dim for GPUs, pad it to nearest 128 multiple, speeds things up decently\n",
    "vocab_size = round_up(tokenizer.n_vocab, 128)\n",
    "\n",
    "\n",
    "print(f\"✓ Loaded tiktoken gpt2\")\n",
    "print(f\"  Vocab size: {vocab_size:,}\")\n",
    "print(f\"  EOT token ID: {eot_id}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load and Clean Datasets\n",
    "# ============================================================================\n",
    "print(\"\\nLoading & cleaning up datasets...\")\n",
    "cleaned_datasets = {\n",
    "    # Main dataset, high quality web crawl\n",
    "    \"fineweb-edu\": load_dataset(\n",
    "        \"HuggingFaceFW/fineweb-edu\", \n",
    "        name=\"sample-10BT\", \n",
    "        split=\"train\",\n",
    "        trust_remote_code=True\n",
    "    ),\n",
    "    \n",
    "    # Wikipedia - use larger dataset\n",
    "    \"wiki\": load_dataset(\n",
    "        \"wikimedia/wikipedia\", \n",
    "        \"20231101.en\", \n",
    "        split=\"train[:15%]\",  # will have some overlap with fineweb\n",
    "        trust_remote_code=True\n",
    "    ),\n",
    "    \n",
    "    # For flavor / diversity\n",
    "    \"bookcorpus\": load_dataset(\n",
    "        \"bookcorpus\", \n",
    "        split=\"train[:10%]\",\n",
    "        trust_remote_code=True\n",
    "    ),\n",
    "}\n",
    "\n",
    "cleaned_datasets = {n: clean_columns(d) for n, d in cleaned_datasets.items()}\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(cleaned_datasets)} datasets\")\n",
    "for n, ds in cleaned_datasets.items():\n",
    "    print(f\"  Dataset {n}: {len(ds):,} examples\")\n",
    "\n",
    "\n",
    "print(\"\\nConcatenating datasets...\")\n",
    "train_ds = concatenate_datasets(list(cleaned_datasets.values()))\n",
    "print(f\"  Combined size: {len(train_ds):,} examples\")\n",
    "\n",
    "# TODO : Way too slow on my machine\n",
    "# # ============================================================================\n",
    "# # Interleave and Shuffle for Better Mixing\n",
    "# # ============================================================================\n",
    "# from datasets import interleave_datasets\n",
    "# print(\"\\nInterleaving datasets...\")\n",
    "# train_ds = interleave_datasets(\n",
    "#     list(cleaned_datasets.values()),\n",
    "#     probabilities=[0.70, 0.15, 0.10, 0.05],  # Adjust weights as needed\n",
    "#     seed=42,\n",
    "#     stopping_strategy=\"all_exhausted\"\n",
    "# )\n",
    "\n",
    "print(\"Shuffling...\")\n",
    "train_ds = train_ds.shuffle(seed=42)\n",
    "print(f\"✓ Final Train Size: {len(train_ds):,} rows\")\n",
    "\n",
    "\n",
    "def clean_text(example):\n",
    "    \"\"\"Remove special tokens and other artifacts\"\"\"\n",
    "    text = example[\"text\"]\n",
    "    \n",
    "    # Remove all common special tokens\n",
    "    special_tokens = [\n",
    "        \"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<s>\", \"</s>\", \"[INST]\", \"[/INST]\",\n",
    "        \"<<SYS>>\", \"<</SYS>>\"\n",
    "    ]\n",
    "    \n",
    "    for token in special_tokens:\n",
    "        text = text.replace(token, \"\")\n",
    "    \n",
    "    example[\"text\"] = text\n",
    "    return example\n",
    "\n",
    "# Tokenizer is going to complain if special tokens are found in training data\n",
    "print(\"\\nCleaning special tokens from datasets...\")\n",
    "train_ds = train_ds.map(\n",
    "    clean_text,\n",
    "    num_proc=os.cpu_count() // 2,\n",
    "    desc=\"Cleaning special tokens\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Document Packing Function\n",
    "# ============================================================================\n",
    "def pack_documents(examples):\n",
    "    \"\"\"\n",
    "    Concatenate all documents in the batch, then slice into fixed-size blocks.\n",
    "    Each document is terminated with exactly ONE EOT token.\n",
    "    \n",
    "    Output chunks are of length (block_size + 1), suitable for\n",
    "    x = chunk[:-1], y = chunk[1:].\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        if not text or not text.strip():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            doc_ids = tokenizer.encode(text)\n",
    "        except Exception as e:\n",
    "            continue  # skip bad docs safely\n",
    "\n",
    "        doc_ids.append(eot_id)  # exactly one end-of-text\n",
    "        all_tokens.extend(doc_ids)\n",
    "\n",
    "    # Now chop into blocks of (block_size + 1)\n",
    "    chunks = []\n",
    "    total = len(all_tokens)\n",
    "\n",
    "    for i in range(0, total, block_size):\n",
    "        chunk = all_tokens[i : i + block_size + 1]\n",
    "        if len(chunk) == block_size + 1:\n",
    "            chunks.append(chunk)\n",
    "        # else: drop the final tiny tail (standard practice)\n",
    "\n",
    "    return {\"chunk_ids\": chunks}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Apply Processing\n",
    "# ============================================================================\n",
    "print(\"\\nTokenizing and packing documents (this may take a few minutes)...\")\n",
    "train_tokenized = train_ds.map(\n",
    "    pack_documents,\n",
    "    batched=True,\n",
    "    batch_size=1250,\n",
    "    num_proc=multiprocessing.cpu_count() // 2,\n",
    "    remove_columns=train_ds.column_names,\n",
    "    desc=\"Packing documents\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Tokenization complete!\")\n",
    "print(f\"  Total chunks: {len(train_tokenized):,}\")\n",
    "print(f\"  Chunk size: {block_size + 1} tokens\")\n",
    "print(f\"  Approx total tokens: {len(train_tokenized) * block_size:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create DataLoader\n",
    "# ============================================================================\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx][\"chunk_ids\"]\n",
    "        # Return input (x) and target (y) shifted by 1\n",
    "        # [0, 1, 2, ..., block_size-1]\n",
    "        # [1, 2, 3, ..., block_size]\n",
    "        return chunk[:-1].long(), chunk[1:].long()\n",
    "\n",
    "# Can also use numpy format and cast to tensors in the loader, not sure which one is slower\n",
    "train_tokenized.set_format(type=\"torch\", columns=[\"chunk_ids\"])\n",
    "token_dataset = TokenDataset(train_tokenized)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    token_dataset,\n",
    "    batch_size=batch_size,\n",
    "    prefetch_factor=prefetch,\n",
    "    shuffle=True,\n",
    "    drop_last=True,  # CRITICAL for torch.compile ! If one batch has dif shape it will trigger re-compilation and slow down training a lot\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ DataLoader ready\")\n",
    "print(f\"  Batches per epoch: {len(train_loader):,}\")\n",
    "print(f\"  Tokens per batch: {batch_size * block_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c794d1b3-6b3f-4d29-bb98-22bbfe6f602e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Final Train Batches: 326566\n",
      "Input x shape: torch.Size([32, 1024])\n",
      "Target y shape: torch.Size([32, 1024])\n",
      "\n",
      "Sanity Check (Shifting 25):\n",
      "x[0, -25:]: [16815, 286, 362, 4, 284, 604, 4, 287, 19922, 4760, 3871, 13, 17, 198, 5886, 1802, 45752, 7652, 357, 1533, 11, 311, 10426, 16, 14]\n",
      "x[0, -25:]:  prevalence of 2% to 4% in POAG patients.2\n",
      "Over 100 genomic regions (eg, SIX1/\n",
      "\n",
      "y[0, -25:]: [286, 362, 4, 284, 604, 4, 287, 19922, 4760, 3871, 13, 17, 198, 5886, 1802, 45752, 7652, 357, 1533, 11, 311, 10426, 16, 14, 50]\n",
      "y[0, -25:]:  of 2% to 4% in POAG patients.2\n",
      "Over 100 genomic regions (eg, SIX1/S\n"
     ]
    }
   ],
   "source": [
    "# --- Verification ---\n",
    "print(\"-\" * 20)\n",
    "print(f\"Final Train Batches: {len(train_loader)}\")\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"Input x shape: {x.shape}\")  # Should be [Batch, Block_Size]\n",
    "print(f\"Target y shape: {y.shape}\") # Should be [Batch, Block_Size]\n",
    "\n",
    "s = 25\n",
    "print(f\"\\nSanity Check (Shifting {s}):\")\n",
    "print(f\"x[0, -{s}:]: {x[0, -s:].tolist()}\")\n",
    "print(f\"x[0, -{s}:]: {tokenizer.decode(x[0, -s:].tolist())}\")\n",
    "print()\n",
    "print(f\"y[0, -{s}:]: {y[0, -s:].tolist()}\")\n",
    "print(f\"y[0, -{s}:]: {tokenizer.decode(y[0, -s:].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f2698c4-2604-4801-a70b-496ef8e6df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with config : \n",
      "{'dropout_prob': 0.05,\n",
      " 'embed_dim': 768,\n",
      " 'is_causal': True,\n",
      " 'mlp_ratio': 4,\n",
      " 'num_heads': 12,\n",
      " 'num_layers': 12,\n",
      " 'vocab_size': 50304}\n"
     ]
    }
   ],
   "source": [
    "# --- Model ---\n",
    "model_config = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"mlp_ratio\": mlp_ratio,\n",
    "    \"dropout_prob\": dropout_prob,\n",
    "    \"is_causal\": True,\n",
    "}\n",
    "\n",
    "print(\"Initializing model with config : \")\n",
    "pprint(model_config)\n",
    "\n",
    "model = GPT(**model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36ceac27-4430-4319-96ae-d53bf4b15e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # # TODO : implement resume from checkpoint (i.e optimizer state, step count etc.)\n",
    "ckpt_path = \"gpt_full_run_250000.pt\"\n",
    "state_dict = strip_compile_prefix(torch.load(ckpt_path, map_location=device))\n",
    "model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b4254-6a67-4537-90f4-a47fa740da21",
   "metadata": {},
   "source": [
    "Using standard practice :\n",
    "- linear warmup + cosine schedule\n",
    "- big weight decay (empirically proven to be beneficial)\n",
    "- gradient accumulation to emulate large batch size\n",
    "\n",
    "Nb : hearing contradicting statements about label_smoothing so disabled for now, we also only decay linear weights unlike nanogpt that also decay embeddings (NanoGPT does not follow modern best practices here apparently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e581bf-67ec-4db3-9bcd-a1516f736f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decay params: 48, No decay params: 26\n",
      "Total Micro-batches: 700000\n",
      "Gradient Accumulation: 20\n",
      "Total Optimizer Updates: 35000\n",
      "Using fused AdamW :  True\n",
      "Using fused cross-entropy loss (Triton kernel)\n"
     ]
    }
   ],
   "source": [
    "# --- Optimizer ---\n",
    "# Separate parameters into decay and no-decay groups\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        continue\n",
    "    \n",
    "    # Don't apply weight decay to:\n",
    "    # - Norm parameters (scale/weight etc.)\n",
    "    # - Embedding table (which is tied to lm_head)\n",
    "    # - Any bias terms if present\n",
    "    if any(keyword in name.lower() for keyword in ['norm', 'bias', 'embed', 'embedding', 'lm_head']):\n",
    "        no_decay_params.append(param)\n",
    "    else:\n",
    "        decay_params.append(param)\n",
    "\n",
    "print(f\"\\nDecay params: {len(decay_params)}, No decay params: {len(no_decay_params)}\")\n",
    "\n",
    "total_optim_steps = MAX_STEPS // GRAD_ACCUM_STEPS\n",
    "print(f\"Total Micro-batches: {MAX_STEPS}\")\n",
    "print(f\"Gradient Accumulation: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Total Optimizer Updates: {total_optim_steps}\")\n",
    "\n",
    "# Perform ADAM update with a single kernel\n",
    "fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "use_fused = fused_available and device == 'cuda'\n",
    "extra_args = dict(fused=True) if use_fused else dict()\n",
    "print(\"Using fused AdamW : \", use_fused)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params, 'weight_decay': 0.1},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "], \n",
    "    lr=3e-4, \n",
    "    betas=(0.9, 0.95),\n",
    "    **extra_args\n",
    ")\n",
    "\n",
    "warmup_steps = int(total_optim_steps * 0.05)  # 5% of total optim steps\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer, \n",
    "    start_factor=0.1,  # Start at 3e-5\n",
    "    total_iters=warmup_steps\n",
    ")\n",
    "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=total_optim_steps - warmup_steps, \n",
    "    eta_min=1e-5\n",
    ")\n",
    "scheduler = SequentialLR(\n",
    "    optimizer, \n",
    "    schedulers=[warmup_scheduler, cosine_scheduler], \n",
    "    milestones=[warmup_steps]\n",
    ")\n",
    "\n",
    "# Use fused cross-entropy for efficiency (defined above with Triton kernels)\n",
    "# Saves ~3GB memory and is ~2x faster than standard F.cross_entropy for large vocab\n",
    "print(\"Using fused cross-entropy loss (Triton kernel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9411b2-d814-4acb-a36c-288ff1bc744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up compilation...\n",
      "  Warmup 1: 156.37s\n",
      "  Warmup 2: 68.05s\n",
      "  Warmup 3: 0.28s\n",
      "  Warmup 4: 0.28s\n",
      "  Warmup 5: 0.28s\n",
      "  Warmup 6: 0.28s\n",
      "  Warmup 7: 0.28s\n",
      "  Warmup 8: 0.28s\n",
      "  Warmup 9: 0.28s\n",
      "  Warmup 10: 0.28s\n",
      "✓ Model fully compiled in 226.65s\n"
     ]
    }
   ],
   "source": [
    "# reduce-overhead mode: faster compilation, better for iterative development\n",
    "# max-autotune mode : overkill and causes very long compile times with marginal gains\n",
    "# default mode should be plenty enough\n",
    "# torch._inductor.config.coordinate_descent_tuning = True can be useful but makes compile MUCH slower\n",
    "# Must use dynamic=False or else it's much slower\n",
    "\n",
    "torch._inductor.config.coordinate_descent_tuning = True\n",
    "model = torch.compile(model, dynamic=False, fullgraph=True)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Multiple warmup passes to force compilation\n",
    "print(\"Warming up compilation...\")\n",
    "x = torch.randint(0, vocab_size, (batch_size, block_size), device=device)\n",
    "y = x.clone()\n",
    "\n",
    "s = time.time()\n",
    "for i in range(10):\n",
    "    start = time.time()\n",
    "    with torch.autocast(\"cuda\", torch.bfloat16):\n",
    "        logits = model(x)\n",
    "        loss = fused_cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            y.view(-1)\n",
    "        )\n",
    "    loss.backward()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"  Warmup {i+1}: {time.time() - start:.2f}s\")\n",
    "\n",
    "print(f\"✓ Model fully compiled in {time.time() - s:.2f}s\")\n",
    "\n",
    "x = y = None\n",
    "del x\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6814e7b9-d6c8-468a-bcb9-8f36c58958b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Second pass : 0.03s\n"
     ]
    }
   ],
   "source": [
    "# second pass (should be fast)\n",
    "x = torch.randint(0, vocab_size, (batch_size, block_size), device=device)\n",
    "y = x.clone()\n",
    "start = time.time()\n",
    "with torch.autocast(\"cuda\", torch.bfloat16):\n",
    "    loss = fused_cross_entropy(model(x).view(-1, vocab_size), y.view(-1))\n",
    "loss.backward()\n",
    "model.zero_grad()  # cleanup gradients\n",
    "print(f\"✓ Second pass : {time.time() - start:.2f}s\")\n",
    "x = y = None\n",
    "del x\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58187ecc-cf39-4679-be51-640204f73dbe",
   "metadata": {},
   "source": [
    "Ideally we should turn this into a python script and run the training from the terminal, would eliminate some python overhang and squeeze out some speed zzzzzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs in :  gpt_full_run__2026-01-22__pretraining_logs.csv\n"
     ]
    }
   ],
   "source": [
    "# --- CSV Logger ---\n",
    "log_file = f'{model_path.split(\".\")[0]}__{datetime.now().strftime(\"%Y-%m-%d\")}__pretraining_logs.csv'\n",
    "print(\"Saving logs in : \", log_file)\n",
    "file_exists = os.path.isfile(log_file)\n",
    "with open(log_file, \"a\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    if not file_exists:\n",
    "        writer.writerow([\"micro_step\", \"optim_step\", \"loss\", \"lr\", \"tokens_seen\", \"tokens_per_sec\", \"timestamp\"])\n",
    "\n",
    "# --- Training Loop ---\n",
    "micro_step = 0\n",
    "optim_step = 0\n",
    "tokens_seen = 0\n",
    "# Accumulate loss on GPU to avoid CPU-GPU sync every step\n",
    "# Only call .item() at log intervals (every 500 steps instead of every step)\n",
    "running_loss = torch.zeros(1, device=device)\n",
    "\n",
    "start_time = time.time()\n",
    "start_training = time.time()\n",
    "last_tokens_seen = 0\n",
    "\n",
    "model_params = decay_params + no_decay_params\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "while micro_step < MAX_STEPS:\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        B, T = x.shape\n",
    "        tokens_seen += B * T\n",
    "\n",
    "        # --- Forward ---\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "            logits = model(x)\n",
    "            loss = fused_cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                y.view(-1)\n",
    "            )\n",
    "\n",
    "        # --- Backward (gradient accumulation) ---\n",
    "        (loss / GRAD_ACCUM_STEPS).backward()\n",
    "\n",
    "        # --- Optimizer step ---\n",
    "        if (micro_step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model_params, 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            optim_step += 1\n",
    "\n",
    "        # --- Bookkeeping (.item() triggers cpu-gpu synchro so we avoid it) ---\n",
    "        running_loss += loss.detach()\n",
    "        micro_step += 1\n",
    "\n",
    "        # --- Logging ---\n",
    "        if micro_step % LOG_INTERVAL == 0:\n",
    "            # Only sync with CPU here (once per 500 steps, not every step)\n",
    "            avg_loss = (running_loss / LOG_INTERVAL).item()\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            tokens_delta = tokens_seen - last_tokens_seen\n",
    "            tokens_per_sec = tokens_delta / elapsed\n",
    "\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            print(\n",
    "                f\"step {micro_step:06d} | \"\n",
    "                f\"opt_step {optim_step:04d} | \"\n",
    "                f\"loss {avg_loss:.3f} | \"\n",
    "                f\"lr {current_lr:.2e} | \"\n",
    "                f\"{tokens_per_sec:,.0f} tok/s\"\n",
    "            )\n",
    "\n",
    "            with open(log_file, \"a\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    micro_step,\n",
    "                    optim_step,\n",
    "                    f\"{avg_loss:.4f}\",\n",
    "                    f\"{current_lr:.2e}\",\n",
    "                    tokens_seen,\n",
    "                    int(tokens_per_sec),\n",
    "                    timestamp,\n",
    "                ])\n",
    "\n",
    "            running_loss.zero_()  # Reset on GPU\n",
    "            start_time = time.time()\n",
    "            last_tokens_seen = tokens_seen\n",
    "\n",
    "        # --- Checkpointing ---\n",
    "        if micro_step % 50_000 == 0 and micro_step > 0:\n",
    "            mid_model_path = model_path.replace(\".pt\", f\"_{micro_step}.pt\")\n",
    "            print(f\"Saving intermediate model to {mid_model_path}\")\n",
    "            torch.save(model.state_dict(), mid_model_path)\n",
    "\n",
    "        # --- Exit ---\n",
    "        if micro_step >= MAX_STEPS:\n",
    "            elapsed = int(time.time() - start_training)\n",
    "            h, m, s = elapsed // 3600, (elapsed % 3600) // 60, elapsed % 60\n",
    "            print(f\"\\nProcessed {tokens_seen:,} tokens in {h:02d}:{m:02d}:{s:02d}\")\n",
    "            print(f\"Saving final model to {model_path}\")\n",
    "            # avoid saving 2 times same model\n",
    "            if model_path == mid_model_path:\n",
    "                os.remove(mid_model_path)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d4a028a-7866-40ad-ba90-570cb8bb614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output :  Redtoothed triggerfish are normally deep blue or purple with a light blue head. They are often found in calmer waters.\n",
      "At the Zoo\n",
      "Diptops (also known as North Sea lampreys)\n",
      "These are small, fairly small freshwater fish and are only found in the Kerguelen, Ardoch, Vitek and Wankeln rivers. They have a wide flattened body that is most often used as a display tank. They are normally white, but can also be bright green and red. They can grow to over 4 metres in length and may live up to 12 years. They are very beautiful, very colourful and very hardy. They are well suited to tanks that are kept in slow running waters.\n",
      "Great white mako (Paliwa mako)\n",
      "The Great white mako (Paliwa mako) is a large fish, about the size of a carp. They are among the most common fish in the Nieuwe Plight. They grow up to 14-17 cm in length and can live up to 11 years in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Redtoothed triggerfish are normally deep blue or purple with a light blue head. They are\" \n",
    "x = torch.tensor(tokenizer.encode(prompt))\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "out = model.generate(\n",
    "    x.unsqueeze(0).to(\"cuda\"),\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95,\n",
    "    top_k=0,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "print(\"\\nOutput : \", tokenizer.decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be60b0-bbd9-419a-9090-c725148142ec",
   "metadata": {},
   "source": [
    "Conclusion : \n",
    "\n",
    "2 days of training, 20B tokens seen, final loss ~ 2.99, managed to produce english looking text, factually incorrect and with some artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d1452-0749-4415-a850-6138ae435283",
   "metadata": {},
   "source": [
    "## TO DO : \n",
    "\n",
    "- [x] ROPE for K, V, Q\n",
    "- [x] ROPE for K, V cache, need double check\n",
    "- [x] AdaptiveLogSoftmaxWithLoss ? Since with modern tokenizer output space is huge (Fused CE is it)\n",
    "- [x] Figure out the compile stuff + hyperparameters to maximize throughput\n",
    "- [x] Maybe check if compiled optimizer step does anything\n",
    "- [x] Double check training loop + if we can use `xformers.swiglu` with compile, maybe with explicit `torch.compiler.cudagraph_mark_step_begin()` ?\n",
    "- [x] Chunk documents properly to avoid topic jumps\n",
    "- [ ] Loss for `<eos>` prediction (avoid endless rambling ?)\n",
    "- [x] Top k sampling\n",
    "- [x] Top p nucleus\n",
    "- [x] Temperature\n",
    "- [x] Add stop token / EOS handling\n",
    "- [x] Training on a real problem to see how far we can push current model\n",
    "- [x] Clean up / Revisit markdown / maths\n",
    "- [ ] Explore hyper connections and manifold constrained HC\n",
    "- [x] Check newer architectures / design choices (https://github.com/lucidrains git is a gold mine)\n",
    "- [x] MUON optimizer ?  -> not worth for a small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7b86e-2803-4838-840f-e22e311cb940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
