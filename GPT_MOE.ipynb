{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": "# GPT with Mixture of Experts (MoE)\n\nArchitectural improvements over base GPT:\n- **Mixture of Experts (MoE)**: Multiple expert MLPs with learned routing. Only top-k experts activated per token.\n  - More total parameters but same compute per forward pass\n  - Each expert can specialize in different types of tokens/patterns\n- **QK-Norm**: RMSNorm on queries and keys before RoPE for training stability\n- **Load balancing loss**: Prevents expert collapse (all tokens going to same expert)\n\nExpected behavior:\n- ~4x total parameters vs dense model at same compute budget\n- Better quality at fixed FLOP budget\n- Requires load balancing to prevent expert collapse"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import csv\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment config\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "from datasets import Dataset as ds, concatenate_datasets, load_dataset\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Torch runtime config\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Custom\n",
    "from utils import count_parameters, load_synthetic_data, strip_compile_prefix, round_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7494176-e827-4d1a-b659-a2c313af239c",
   "metadata": {},
   "source": [
    "## Config & Model Definition\n",
    "\n",
    "Mostly the same code as the pretraining notebook, including Flash Attention, RMSNorm etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa17962-52b5-4773-868c-5c87bae6d59b",
   "metadata": {},
   "outputs": [],
   "source": "#### CONFIG #####\n\n# Model architecture - Moderate scale MoE for single 32GB GPU\n# Strategy: Go deeper (better for small models) + MoE for capacity\nblock_size = 1024\nbatch_size = 12       # conservative for MoE memory overhead\nembed_dim = 1024      # GPT-2 Medium width\nnum_layers = 16       # deeper than GPT-2 Small (12), helps reasoning\nnum_heads = 16        # head_dim = 64, good for efficiency\ndropout_prob = 0.1\nmlp_ratio = 4\n\n# MoE config\nnum_experts = 8       # 8 experts total\ntop_k_experts = 2     # activate 2 per token → 4x params, same compute\naux_loss_coef = 0.01  # load balancing (0.01-0.1 typical range)\n\n# Expected params:\n#   Total:  ~800M (all experts)\n#   Active: ~250M (similar compute to GPT-2 Medium)\n\n# Tokenizer\nROLE_TOKENS = [\"<|user|>\", \"<|assistant|>\"]\nIGNORE_INDEX = -100\n\n# Training\nNUM_EPOCHS = 3\nnum_workers = 4\nprefetch = 8\ndtype = torch.bfloat16\ndevice = \"cuda\"\n\n# Estimated VRAM usage (bf16):\n#   Model params:     ~800M × 2B = 1.6 GB\n#   Optimizer states: ~800M × 8B = 6.4 GB (AdamW)\n#   Gradients:        ~800M × 2B = 1.6 GB\n#   Activations:      ~8-12 GB (batch=12, seq=1024)\n#   Total:            ~18-22 GB → fits in 32GB comfortably\n\nprint(\"=\" * 60)\nprint(\"MoE Model Configuration\")\nprint(\"=\" * 60)\nprint(f\"  Architecture:    {embed_dim}d × {num_layers}L × {num_heads}H\")\nprint(f\"  MoE:             {num_experts} experts, top-{top_k_experts}\")\nprint(f\"  Batch:           {batch_size} × {block_size} tokens\")\nprint(f\"  Params:          ~800M total, ~250M active\")\nprint(f\"  bf16 supported:  {torch.cuda.is_bf16_supported()}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": "class MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head attention with QK-Norm for improved training stability.\"\"\"\n    \n    def __init__(self,\n                 embed_dim: int,\n                 num_heads: int,\n                 rotary_emb: RotaryEmbedding,\n                 causal: bool = True,\n                 dropout: float = 0.1\n                ):\n        super().__init__()\n        if embed_dim % num_heads != 0:\n            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\")\n        \n        self.causal = causal\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.dropout_p = dropout\n        \n        # Fused QKV projection\n        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n        \n        # QK-Norm: normalize queries and keys before RoPE\n        self.q_norm = nn.RMSNorm(self.head_dim)\n        self.k_norm = nn.RMSNorm(self.head_dim)\n        \n        self.rotary_emb = rotary_emb\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n    \n    def forward(self, x, k_v_cache=None):\n        B, T, _ = x.shape\n        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n    \n        if using_cache:\n            x_q = x[:, -1:, :]\n            qkv = self.qkv_proj(x_q)\n        else:\n            qkv = self.qkv_proj(x)\n        \n        Q, K, V = qkv.chunk(3, dim=-1)\n        \n        def split_heads(t):\n            return t.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        Q = split_heads(Q)\n        K = split_heads(K)\n        V = split_heads(V)\n        \n        # Apply QK-Norm before RoPE\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n    \n        # Apply RoPE\n        if using_cache:\n            past_len = k_v_cache[\"K\"].shape[-2]\n            Q = self.rotary_emb.rotate_queries_or_keys(Q, offset=past_len)\n            K = self.rotary_emb.rotate_queries_or_keys(K, offset=past_len)\n            \n            K = torch.cat([k_v_cache[\"K\"], K], dim=-2)\n            V = torch.cat([k_v_cache[\"V\"], V], dim=-2)\n            is_causal_step = False\n        else:\n            Q = self.rotary_emb.rotate_queries_or_keys(Q)\n            K = self.rotary_emb.rotate_queries_or_keys(K)\n            is_causal_step = self.causal\n    \n        if k_v_cache is not None:\n            k_v_cache[\"K\"] = K.detach()\n            k_v_cache[\"V\"] = V.detach()\n    \n        out = F.scaled_dot_product_attention(\n            query=Q, key=K, value=V,\n            attn_mask=None, \n            dropout_p=self.dropout_p if self.training else 0.0,\n            is_causal=is_causal_step\n        )\n        \n        out = out.transpose(1, 2).contiguous().view(B, -1, self.embed_dim)\n        return self.out_proj(out), k_v_cache\n\n\nclass Expert(nn.Module):\n    \"\"\"Single expert MLP (SwiGLU).\"\"\"\n    \n    def __init__(self, embed_dim, hidden_dim, dropout_prob=0.1):\n        super().__init__()\n        hidden_dim = round_up(2 * hidden_dim // 3, 8)\n        \n        self.gate_up_proj = nn.Linear(embed_dim, 2 * hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)\n        self.dropout = nn.Dropout(dropout_prob)\n       \n    def forward(self, x):\n        gate_up = self.gate_up_proj(x)\n        gate, up = gate_up.chunk(2, dim=-1)\n        return self.dropout(self.down_proj(F.silu(gate) * up))\n\n\nclass MoEMLP(nn.Module):\n    \"\"\"\n    Mixture of Experts MLP layer.\n    \n    Routes each token to top-k experts and combines their outputs.\n    Includes auxiliary load balancing loss to prevent expert collapse.\n    \"\"\"\n    \n    def __init__(self, embed_dim, hidden_dim, num_experts, top_k, dropout_prob=0.1):\n        super().__init__()\n        self.num_experts = num_experts\n        self.top_k = top_k\n        self.embed_dim = embed_dim\n        \n        # Router: learned linear layer to score experts\n        self.router = nn.Linear(embed_dim, num_experts, bias=False)\n        \n        # Expert MLPs\n        self.experts = nn.ModuleList([\n            Expert(embed_dim, hidden_dim, dropout_prob) \n            for _ in range(num_experts)\n        ])\n        \n        # For tracking load balancing\n        self.aux_loss = 0.0\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (B, T, D) input tensor\n        Returns:\n            output: (B, T, D) combined expert outputs\n        \"\"\"\n        B, T, D = x.shape\n        x_flat = x.view(-1, D)  # (B*T, D)\n        num_tokens = x_flat.shape[0]\n        \n        # Compute router logits and probabilities\n        router_logits = self.router(x_flat)  # (B*T, num_experts)\n        router_probs = F.softmax(router_logits, dim=-1)\n        \n        # Select top-k experts per token\n        top_k_probs, top_k_indices = torch.topk(router_probs, self.top_k, dim=-1)\n        \n        # Normalize top-k probabilities to sum to 1\n        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n        \n        # Compute auxiliary load balancing loss\n        # Goal: encourage uniform expert utilization\n        if self.training:\n            # Fraction of tokens routed to each expert\n            expert_mask = F.one_hot(top_k_indices, num_classes=self.num_experts).sum(dim=1)  # (B*T, E)\n            tokens_per_expert = expert_mask.float().mean(dim=0)  # (E,)\n            \n            # Average router probability per expert\n            router_prob_per_expert = router_probs.mean(dim=0)  # (E,)\n            \n            # Load balancing loss: minimize the product (encourages uniformity)\n            self.aux_loss = self.num_experts * (tokens_per_expert * router_prob_per_expert).sum()\n        \n        # Compute expert outputs (batched for efficiency)\n        # This is the \"loop over experts\" approach - simpler than sparse dispatch\n        output = torch.zeros_like(x_flat)\n        \n        for expert_idx in range(self.num_experts):\n            # Find which tokens selected this expert in their top-k\n            # expert_mask[i, j] = 1 if token i selected expert expert_idx in position j of top-k\n            expert_mask = (top_k_indices == expert_idx)  # (B*T, top_k)\n            \n            if not expert_mask.any():\n                continue\n            \n            # Get tokens that use this expert\n            token_indices = expert_mask.any(dim=-1).nonzero(as_tuple=True)[0]\n            \n            if len(token_indices) == 0:\n                continue\n                \n            # Get the weight for this expert for these tokens\n            # Shape: (num_selected_tokens,)\n            weights = (top_k_probs * expert_mask.float()).sum(dim=-1)[token_indices]\n            \n            # Compute expert output\n            expert_input = x_flat[token_indices]\n            expert_output = self.experts[expert_idx](expert_input)\n            \n            # Weighted addition to output\n            output[token_indices] += weights.unsqueeze(-1) * expert_output\n        \n        return output.view(B, T, D)\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer block with MoE MLP.\"\"\"\n    \n    def __init__(self,\n                 embed_dim,\n                 num_heads,\n                 rotary_emb,\n                 mlp_ratio=4,\n                 num_experts=8,\n                 top_k_experts=2,\n                 dropout_prob=0.1,\n                 causal=True,\n                ): \n        super().__init__()\n        self.norm1 = nn.RMSNorm(embed_dim)\n        self.mha = MultiHeadAttention(embed_dim, num_heads, rotary_emb, causal, dropout_prob)\n        self.norm2 = nn.RMSNorm(embed_dim)\n        \n        # MoE instead of standard MLP\n        hidden_dim = mlp_ratio * embed_dim\n        self.moe = MoEMLP(embed_dim, hidden_dim, num_experts, top_k_experts, dropout_prob)\n    \n    def forward(self, x, cache=None):\n        x1 = self.norm1(x)\n        x2, cache = self.mha(x1, cache)\n        x2 = x2 + x  # residual\n    \n        x3 = self.norm2(x2)\n        x3 = self.moe(x3) + x2  # residual\n        return x3, cache\n    \n    def get_aux_loss(self):\n        \"\"\"Return the MoE auxiliary loss for this block.\"\"\"\n        return self.moe.aux_loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": "class GPT_MoE(nn.Module):\n    \"\"\"\n    GPT with Mixture of Experts.\n    \n    Same architecture as base GPT but with MoE layers replacing standard MLPs.\n    Includes auxiliary loss collection for load balancing.\n    \"\"\"\n\n    def __init__(self,\n                 vocab_size,\n                 embed_dim,\n                 num_layers,\n                 num_heads,\n                 mlp_ratio=4,\n                 num_experts=8,\n                 top_k_experts=2,\n                 dropout_prob=0.1,\n                 is_causal=True,\n                ):\n        super().__init__()\n\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.mlp_ratio = mlp_ratio\n        self.num_experts = num_experts\n        self.top_k_experts = top_k_experts\n\n        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        \n        head_dim = embed_dim // num_heads\n        self.rotary_emb = RotaryEmbedding(dim=head_dim)\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                embed_dim, num_heads, self.rotary_emb, \n                mlp_ratio, num_experts, top_k_experts,\n                dropout_prob, is_causal\n            ) \n            for _ in range(num_layers)\n        ])\n        self.norm = nn.RMSNorm(embed_dim)\n        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n        self.lm_head.weight = self.embedding.weight  # weight tying\n\n        # Initialize weights\n        self.apply(self._init_weights)\n        # Scale residual projections\n        for pn, p in self.named_parameters():\n            if pn.endswith((\"out_proj.weight\", \"down_proj.weight\")):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.num_layers))\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n       \n    def forward(self, tokens):\n        embeddings = self.embedding(tokens)\n        x = self.dropout(embeddings)\n        for b in self.blocks:\n            x, _ = b(x)\n        features = self.norm(x)\n        return self.lm_head(features)\n    \n    def get_aux_loss(self):\n        \"\"\"Collect and sum auxiliary losses from all MoE layers.\"\"\"\n        total_aux_loss = 0.0\n        for block in self.blocks:\n            total_aux_loss += block.get_aux_loss()\n        return total_aux_loss / self.num_layers  # average over layers\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n    \n    def count_parameters(self):\n        \"\"\"Count total and active parameters.\"\"\"\n        total_params = sum(p.numel() for p in self.parameters())\n        \n        # Estimate active params (non-MoE + top-k fraction of MoE)\n        non_moe_params = 0\n        moe_params = 0\n        \n        for name, p in self.named_parameters():\n            if 'experts' in name:\n                moe_params += p.numel()\n            else:\n                non_moe_params += p.numel()\n        \n        # Active MoE params = (top_k / num_experts) * total_moe_params\n        active_moe = moe_params * (self.top_k_experts / self.num_experts)\n        active_params = non_moe_params + active_moe\n        \n        return total_params, int(active_params)\n\n    @torch.no_grad()\n    def generate(self,\n                 prompt_tokens,\n                 max_new_tokens=50,\n                 temperature=1.0,\n                 top_k=0,\n                 top_p=0.0,\n                 use_cache=True,\n                ):\n        self.eval()\n\n        tokens_out = prompt_tokens.clone()\n        current_tokens = prompt_tokens.clone()\n        tokens_out = tokens_out.to(self.device)\n        current_tokens = current_tokens.to(self.device)\n        cache = [{} if use_cache else None for _ in range(len(self.blocks))]\n        \n        for _ in range(max_new_tokens):\n            x = self.embedding(current_tokens)\n            for i, b in enumerate(self.blocks):\n                x, c_i = b(x, cache[i])\n                cache[i] = c_i\n            \n            features = self.norm(x)\n            logits = self.lm_head(features)    \n            last_logits = logits[:, -1, :]\n    \n            if temperature == 0:\n                next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n            else:\n                scaled_logits = last_logits / temperature\n                \n                if int(top_k) > 0:\n                    values, indices = torch.topk(scaled_logits, top_k)\n                    scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n                    scaled_logits.scatter_(1, indices, values)\n\n                if top_p > 0.0 and top_p < 1.0:\n                    sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True, dim=-1)\n                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n                    sorted_indices_to_remove = cumulative_probs > top_p\n                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                    sorted_indices_to_remove[..., 0] = 0\n                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n                    scaled_logits[indices_to_remove] = float('-inf')\n                \n                probs = torch.softmax(scaled_logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)\n\n            if next_token.item() == eot_id:\n                break\n            \n            tokens_out = torch.cat([tokens_out, next_token], dim=1)\n            current_tokens = next_token if use_cache else tokens_out\n       \n        return tokens_out"
  },
  {
   "cell_type": "markdown",
   "id": "d9ed8616-4de7-4595-afcb-8b36c3dea3f9",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233fbc6d-4607-4bdc-bc51-2bd536a68ba2",
   "metadata": {},
   "source": [
    "Must be the exact same used for pretraining, on top just add 2 extra tokens for assistant / user roles and assign these the same embedding we learnt during pretraining as the end of text token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cbeb30-9756-4a6f-a1a4-a812ba79d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tiktoken.core import Encoding\n",
    "\n",
    "base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "special_tokens = {\n",
    "    \"<|endoftext|>\": base.eot_token,        # must be preserved\n",
    "    \"<|user|>\": base.n_vocab,\n",
    "    \"<|assistant|>\": base.n_vocab + 1,\n",
    "}\n",
    "\n",
    "tokenizer = Encoding(\n",
    "    name=\"gpt2-with-roles\",\n",
    "    pat_str=base._pat_str,\n",
    "    mergeable_ranks=base._mergeable_ranks,\n",
    "    special_tokens=special_tokens,\n",
    ")\n",
    "\n",
    "eot_id = tokenizer.eot_token\n",
    "user_id = tokenizer.encode_single_token(\"<|user|>\")\n",
    "assistant_id = tokenizer.encode_single_token(\"<|assistant|>\")\n",
    "\n",
    "print(user_id, assistant_id, eot_id)\n",
    "print(tokenizer.decode([user_id, assistant_id, eot_id]))\n",
    "\n",
    "vocab_size = round_up(tokenizer.n_vocab, 128)\n",
    "print(\"Vocab size:\", tokenizer.n_vocab, \"→ padded:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925259a-61f9-4bde-aa95-f59755dc5478",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning Dataset\n",
    "\n",
    "### Goal\n",
    "Train the model to generate assistant responses, NOT to predict instructions -> Autoregressive LMs predict the NEXT token at each position. We use this by masking instruction tokens in the loss calculation.\n",
    "\n",
    "---\n",
    "\n",
    "### The Process\n",
    "\n",
    "#### 1. Format the Data\n",
    "```\n",
    "Input text: \"<|user|>\\n{instruction}\\n<|assistant|>\\n{response}\"\n",
    "\n",
    "```\n",
    "\n",
    "#### 2. Tokenize\n",
    "```\n",
    "ids = [user_tok, What, is, 2, +, 2, ?, asst_tok, The, answer, is, 4, eot]\n",
    "idx:   0        1     2   3  4  5  6  7         8    9      10  11 12\n",
    "```\n",
    "\n",
    "#### 3. Create Shifted Labels\n",
    "```python\n",
    "labels[:-1] = ids[1:]  # Each label is the NEXT token to predict\n",
    "\n",
    "labels = [What, is, 2, +, 2, ?, asst_tok, The, answer, is, 4, eot, IGNORE]\n",
    "```\n",
    "**Meaning:** `labels[i]` = what should be predicted after seeing `ids[i]`\n",
    "\n",
    "#### 4. Mask the Instruction\n",
    "```python\n",
    "# Find position of <|assistant|> token (position 7 in example)\n",
    "labels[:assistant_pos+1] = IGNORE_INDEX (-100)\n",
    "\n",
    "labels = [IGN, IGN, IGN, IGN, IGN, IGN, IGN, The, answer, is, 4, eot, IGN]\n",
    "          └─────────instruction masked───────────┘  └────train here────┘\n",
    "```\n",
    "\n",
    "#### 5. Compute Loss (during training)\n",
    "```python\n",
    "logits = model(ids)  # Model predicts next token at each position\n",
    "loss = CrossEntropyLoss(logits, labels, ignore_index=-100)\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- Position 0-6: `labels[i] = -100` → loss ignored (don't train on instruction)\n",
    "- Position 7: predict \"The\" after `<|assistant|>` → **COMPUTE LOSS** ✓\n",
    "- Position 8: predict \"answer\" after \"The\" → **COMPUTE LOSS** ✓\n",
    "- Position 9: predict \"is\" after \"answer\" → **COMPUTE LOSS** ✓\n",
    "- Position 10: predict \"4\" after \"is\" → **COMPUTE LOSS** ✓\n",
    "- Position 11: predict `<eot>` after \"4\" → **COMPUTE LOSS** ✓\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "**Causal Attention Mask**\n",
    "- Prevents the model from \"seeing\" future tokens\n",
    "- At position i, model only attends to tokens 0 to i\n",
    "\n",
    "**Teacher Forcing**\n",
    "- Model sees correct previous tokens during training\n",
    "- Learns to predict the next one\n",
    "\n",
    "**Masking with -100**\n",
    "- `CrossEntropyLoss` ignores these positions\n",
    "- Gradients only flow through response tokens\n",
    "\n",
    "**Result:** Model learns \"given instruction X, generate response Y\" without wasting compute trying to predict the instruction itself.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Facts\n",
    "\n",
    "- `IGNORE_INDEX = -100` (standard PyTorch convention)\n",
    "- Only ~5-20% of tokens typically contribute to loss (just the responses)\n",
    "- The shift (`labels[i] = ids[i+1]`) aligns predictions with targets\n",
    "- The masking + smaller size dataset is going to finetune behavior but not (or barely) knowledge !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841eb3d5-48d3-4403-aa9f-20331683a61d",
   "metadata": {},
   "outputs": [],
   "source": "# Load multiple datasets\nprint(\"Loading datasets...\")\nalpaca_cleaned = load_dataset(\"yahma/alpaca-cleaned\")\nplatypus = load_dataset(\"garage-bAInd/Open-Platypus\")\nno_robots = load_dataset(\"HuggingFaceH4/no_robots\")\n\n# Format functions for each dataset\ndef format_alpaca(example):\n    if example[\"input\"].strip():\n        user_text = (\n            f\"{example['instruction']}\\n\\n\"\n            f\"{example['input']}\"\n        )\n    else:\n        user_text = example[\"instruction\"]\n    return {\n        \"user\": user_text,\n        \"assistant\": example[\"output\"]\n    }\n\ndef format_platypus(example):\n    return {\n        \"user\": example[\"instruction\"],\n        \"assistant\": example[\"output\"]\n    }\n\ndef format_no_robots(example):\n    return {\n        \"user\": example[\"prompt\"],\n        \"assistant\": example[\"messages\"][1][\"content\"]\n    }\n\n# Map each dataset to common format\nprint(\"Formatting datasets...\")\nalpaca_formatted = alpaca_cleaned.map(\n    format_alpaca, \n    remove_columns=alpaca_cleaned[\"train\"].column_names\n)\nplatypus_formatted = platypus.map(\n    format_platypus, \n    remove_columns=platypus[\"train\"].column_names\n)\nno_robots_formatted = no_robots.map(\n    format_no_robots, \n    remove_columns=no_robots[\"train\"].column_names\n)\n\ncombined_datasets = [\n    alpaca_formatted[\"train\"],\n    platypus_formatted[\"train\"],\n    no_robots_formatted[\"train\"]\n]\n\n# OPTIONAL : can use any modern LLM to generate custom SFT data\nif os.path.isfile(\"synthetic_sft_data.jsonl\"):\n    print(\"Loading synthetic data...\")\n    synthetic_data = load_synthetic_data(\"synthetic_sft_data.jsonl\")\n    print(f\"Loaded {len(synthetic_data)} synthetic examples\")\n    \n    # Transform list of dicts to dict of lists\n    data_dict = {}\n    for key in synthetic_data[0].keys():\n        data_dict[key] = [item[key] for item in synthetic_data]\n    \n    synthetic_dataset = ds.from_dict(data_dict)\n    \n    # Print example to verify format\n    print(\"\\nExample from synthetic dataset:\")\n    pprint(synthetic_dataset[0])\n\n    # Add to all datasets\n    combined_datasets.append(synthetic_dataset)\n\n# Combine all datasets\nprint(\"\\n\\nCombining datasets...\")\ncombined_train = concatenate_datasets(combined_datasets)\n\nprint(f\"Total training examples: {len(combined_train)}\")\nprint(\"\\nExample from combined dataset:\")\npprint(next(iter(combined_train)))\n\ndef tokenize_sft(example):\n    text = (\n        \"<|user|>\\n\"\n        f\"{example['user']}\\n\"\n        \"<|assistant|>\\n\"\n        f\"{example['assistant']}\"\n    )\n\n    ids = tokenizer.encode(text, allowed_special=set(special_tokens.keys()))\n    ids.append(eot_id)\n\n    # Find assistant token\n    try:\n        assistant_pos = ids.index(assistant_id)\n    except ValueError:\n        # Return empty tensors that will be filtered out\n        return {\"input_ids\": [], \"labels\": []}\n\n    # Create labels as shifted version of ids\n    labels = [IGNORE_INDEX] * len(ids)\n    labels[:-1] = ids[1:]\n\n    # Mask out everything before assistant response\n    labels[:assistant_pos + 1] = [IGNORE_INDEX] * (assistant_pos + 1)\n\n    # Truncate\n    ids = ids[:block_size]\n    labels = labels[:block_size]\n\n    return {\n        \"input_ids\": ids,\n        \"labels\": labels,\n    }\n\n# Tokenize combined dataset\nprint(\"Tokenizing combined dataset...\")\ncombined_tokenized = combined_train.map(\n    tokenize_sft,\n    remove_columns=combined_train.column_names,\n    num_proc=4,\n)\n\n# Filter out empty examples (failed tokenization)\npre_filter_len = len(combined_tokenized)\ncombined_tokenized = combined_tokenized.filter(lambda x: len(x[\"input_ids\"]) > 0)\nprint(f\"Filtered {pre_filter_len - len(combined_tokenized)} invalid examples\")\nprint(f\"Final dataset size: {len(combined_tokenized)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2dc3f0-4902-4404-b046-416806c20f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Inputs from user / assistant conversations are of variable length -> pad for training\n",
    "    # To squeeze out performance, can assign inputs to buckets or pad with fixed len -> compile model\n",
    "    # Here the data is reasonably sized so we can skip\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    \n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    \n",
    "    for x in batch:\n",
    "        pad_len = max_len - len(x[\"input_ids\"])\n",
    "        input_ids.append(\n",
    "            x[\"input_ids\"] + [eot_id] * pad_len\n",
    "        )\n",
    "        labels.append(\n",
    "            x[\"labels\"] + [IGNORE_INDEX] * pad_len\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    combined_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=prefetch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with {len(train_loader)} batches of {batch_size} seqs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237d148-f21d-45e7-81dc-7b7d5543a14d",
   "metadata": {},
   "source": "## Model Initialization\n\n**Note**: MoE architecture is different from dense GPT, so we train from scratch.\nCannot load pretrained weights from the dense model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae4514-68d7-466c-b69e-3480dc0b14c6",
   "metadata": {},
   "outputs": [],
   "source": "model_config = {\n    \"vocab_size\": vocab_size,\n    \"embed_dim\": embed_dim,\n    \"num_layers\": num_layers,\n    \"num_heads\": num_heads,\n    \"mlp_ratio\": mlp_ratio,\n    \"num_experts\": num_experts,\n    \"top_k_experts\": top_k_experts,\n    \"dropout_prob\": dropout_prob,\n}\n\nprint(\"Initializing MoE model with config:\")\npprint(model_config)\n\nmodel = GPT_MoE(**model_config).to(device)\n\n# Count parameters\ntotal_params, active_params = model.count_parameters()\nprint(f\"\\nParameter counts:\")\nprint(f\"  Total parameters:  {total_params:,} ({total_params/1e6:.1f}M)\")\nprint(f\"  Active parameters: {active_params:,} ({active_params/1e6:.1f}M)\")\nprint(f\"  Ratio: {total_params/active_params:.2f}x total vs active\")\n\n# Initialize special token embeddings\nwith torch.no_grad():\n    model.embedding.weight[user_id] = model.embedding.weight[eot_id].clone()\n    model.embedding.weight[assistant_id] = model.embedding.weight[eot_id].clone()\n\nprint(f\"\\nInitialized special tokens: <|user|>={user_id}, <|assistant|>={assistant_id}\")\nmodel.train()"
  },
  {
   "cell_type": "markdown",
   "id": "117ff922-468a-49b1-a8c7-26230af36f68",
   "metadata": {},
   "source": "## Optimizer\n\nFor MoE models:\n- Same weight decay rules (don't decay norms, embeddings)\n- Router parameters should NOT be decayed (they're like attention-ish)\n- Add auxiliary load balancing loss to prevent expert collapse"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e581bf-67ec-4db3-9bcd-a1516f736f0f",
   "metadata": {},
   "outputs": [],
   "source": "# Separate parameters into decay and no-decay groups\ndecay_params = []\nno_decay_params = []\n\nfor name, param in model.named_parameters():\n    if not param.requires_grad:\n        continue\n    # Don't apply weight decay to:\n    # - Norm parameters (RMSNorm weights, QK-norm)\n    # - Embedding table (tied with lm_head)\n    # - Router (small, should be flexible)\n    # - Any bias terms\n    if any(kw in name.lower() for kw in ['norm', 'bias', 'embed', 'embedding', 'lm_head', 'router']):\n        no_decay_params.append(param)\n    else:\n        decay_params.append(param)\n\nprint(f\"Decay params: {len(decay_params)}, No decay params: {len(no_decay_params)}\")\n\n# Hyperparameters tuned for ~800M param MoE model\n# Rule of thumb: larger models need lower LR\nbase_lr = 2e-4  # slightly lower than 124M model\n\noptimizer = torch.optim.AdamW([\n    {'params': decay_params, 'weight_decay': 0.1},\n    {'params': no_decay_params, 'weight_decay': 0.0}\n],\n    lr=base_lr,\n    betas=(0.9, 0.95),  # standard for LLMs\n    eps=1e-8,\n)\n\n# Learning rate scheduler: 10% warmup + cosine decay\ntotal_steps = NUM_EPOCHS * len(train_loader)\nwarmup_steps = int(total_steps * 0.10)  # 10% warmup for training from scratch\n\nfrom torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n\nwarmup_scheduler = LinearLR(\n    optimizer,\n    start_factor=0.01,  # start at 1% of base LR\n    total_iters=warmup_steps\n)\ncosine_scheduler = CosineAnnealingLR(\n    optimizer,\n    T_max=total_steps - warmup_steps,\n    eta_min=base_lr * 0.1  # decay to 10% of base\n)\nscheduler = SequentialLR(\n    optimizer,\n    schedulers=[warmup_scheduler, cosine_scheduler],\n    milestones=[warmup_steps]\n)\n\nprint(f\"\\nTraining schedule:\")\nprint(f\"  Total steps:     {total_steps:,}\")\nprint(f\"  Warmup steps:    {warmup_steps:,} ({100*warmup_steps/total_steps:.0f}%)\")\nprint(f\"  Base LR:         {base_lr}\")\nprint(f\"  Min LR:          {base_lr * 0.1}\")\nprint(f\"  Weight decay:    0.1 (linear), 0.0 (norm/embed/router)\")\nprint(f\"  Aux loss coef:   {aux_loss_coef}\")"
  },
  {
   "cell_type": "markdown",
   "id": "ade3a017-b03e-4e16-9e46-169a2e6fc1d4",
   "metadata": {},
   "source": "## Training Loop\n\nMoE training includes:\n- Main cross-entropy loss (language modeling)\n- Auxiliary load balancing loss (prevents expert collapse)\n\nTotal loss = CE loss + aux_loss_coef × aux_loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [],
   "source": "import time\n\nglobal_step = 0\ntotal_batches = len(train_loader)\nlog_interval = 100\n\nprint(f\"Starting MoE training: {NUM_EPOCHS} epochs, {total_batches} batches/epoch\")\nprint(f\"Aux loss coefficient: {aux_loss_coef}\")\nprint(\"-\" * 80)\n\ntraining_start = time.time()\n\nfor epoch in range(NUM_EPOCHS):\n    epoch_start = time.time()\n    epoch_ce_loss_sum = 0.0\n    epoch_aux_loss_sum = 0.0\n    epoch_loss_count = 0\n\n    for step, batch in enumerate(train_loader):\n        input_ids = batch[\"input_ids\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        with torch.amp.autocast('cuda', dtype=dtype):\n            logits = model(input_ids)\n            \n            # Main language modeling loss\n            ce_loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                labels.view(-1),\n                ignore_index=IGNORE_INDEX,\n            )\n            \n            # Auxiliary load balancing loss\n            aux_loss = model.get_aux_loss()\n            \n            # Combined loss\n            loss = ce_loss + aux_loss_coef * aux_loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        global_step += 1\n\n        # Track losses\n        epoch_ce_loss_sum += ce_loss.item()\n        epoch_aux_loss_sum += aux_loss.item() if isinstance(aux_loss, torch.Tensor) else aux_loss\n        epoch_loss_count += 1\n\n        if step % log_interval == 0:\n            current_lr = optimizer.param_groups[0][\"lr\"]\n            pct_complete = 100 * (step + 1) / total_batches\n            elapsed = time.time() - epoch_start\n            \n            if step > 0:\n                steps_per_sec = step / elapsed\n                remaining_steps = total_batches - step\n                eta_sec = remaining_steps / steps_per_sec\n                eta_str = f\"{int(eta_sec // 60):02d}:{int(eta_sec % 60):02d}\"\n            else:\n                eta_str = \"--:--\"\n            \n            avg_ce = epoch_ce_loss_sum / epoch_loss_count\n            avg_aux = epoch_aux_loss_sum / epoch_loss_count\n            \n            print(\n                f\"epoch {epoch+1}/{NUM_EPOCHS} | \"\n                f\"step {step:>5}/{total_batches} ({pct_complete:5.1f}%) | \"\n                f\"ce {ce_loss.item():.3f} (avg {avg_ce:.3f}) | \"\n                f\"aux {aux_loss:.3f} | \"\n                f\"lr {current_lr:.2e} | \"\n                f\"ETA {eta_str}\"\n            )\n\n    # End of epoch summary\n    epoch_elapsed = time.time() - epoch_start\n    epoch_avg_ce = epoch_ce_loss_sum / epoch_loss_count\n    epoch_avg_aux = epoch_aux_loss_sum / epoch_loss_count\n    print(\"-\" * 80)\n    print(\n        f\"Epoch {epoch+1} complete | \"\n        f\"avg CE loss: {epoch_avg_ce:.4f} | \"\n        f\"avg aux loss: {epoch_avg_aux:.4f} | \"\n        f\"time: {int(epoch_elapsed // 60):02d}:{int(epoch_elapsed % 60):02d}\"\n    )\n    print(\"-\" * 80)\n\n    torch.save(model.state_dict(), f\"GPT_MoE_epoch_{epoch}.pt\")\n    print(f\"Saved checkpoint: GPT_MoE_epoch_{epoch}.pt\")\n\n# Final summary\ntotal_time = time.time() - training_start\nprint(\"=\" * 80)\nprint(f\"Training complete! Total time: {int(total_time // 3600):02d}:{int((total_time % 3600) // 60):02d}:{int(total_time % 60):02d}\")\nprint(f\"Final checkpoint: GPT_MoE_epoch_{NUM_EPOCHS - 1}.pt\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67998ad-5c05-424b-9358-fc72fe99d35b",
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef generate(\n    model,\n    prompt_text,\n    max_new_tokens=150,\n    temperature=0.7,\n    top_p=0.9,\n    stop_token_id=eot_id,\n):\n    \"\"\"Generate response from a prompt string.\"\"\"\n    model.eval()\n    \n    enc = tokenizer.encode(prompt_text, allowed_special=set(special_tokens.keys()))\n    input_ids = torch.tensor(enc, dtype=torch.long).unsqueeze(0).to(device)\n    \n    for _ in range(max_new_tokens):\n        # Crop to block_size if needed\n        input_ids_cond = input_ids if input_ids.shape[1] <= block_size else input_ids[:, -block_size:]\n        \n        logits = model(input_ids_cond)\n        next_token_logits = logits[:, -1, :]\n        \n        # Apply temperature\n        next_token_logits = next_token_logits / temperature\n        \n        # Top-p sampling\n        if top_p < 1.0:\n            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n            sorted_indices_to_remove = cumulative_probs > top_p\n            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n            sorted_indices_to_remove[..., 0] = 0\n            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n            next_token_logits[indices_to_remove] = float('-inf')\n        \n        probs = F.softmax(next_token_logits, dim=-1)\n        next_token = torch.multinomial(probs, num_samples=1)\n        \n        input_ids = torch.cat([input_ids, next_token], dim=1)\n        \n        if stop_token_id is not None and next_token.item() == stop_token_id:\n            break\n    \n    return tokenizer.decode(input_ids[0].tolist())\n\n\ndef format_prompt(user_message):\n    \"\"\"Format a user message into the chat template.\"\"\"\n    return f\"<|user|>\\n{user_message}\\n<|assistant|>\\n\"\n\n\n# =============================================================================\n# Quick Evaluation Suite\n# =============================================================================\nprint(\"=\" * 80)\nprint(\"POST-TRAINING EVALUATION\")\nprint(\"=\" * 80)\n\neval_prompts = [\n    # Basic instruction following\n    (\"Simple instruction\", \"Write a short greeting message.\"),\n    \n    # Knowledge recall (don't expect accuracy, just coherence)\n    (\"Knowledge\", \"What is the capital of France?\"),\n    \n    # Reasoning (basic)\n    (\"Basic reasoning\", \"If I have 3 apples and buy 2 more, how many do I have?\"),\n    \n    # Creative\n    (\"Creative\", \"Write a haiku about programming.\"),\n    \n    # Explanation\n    (\"Explanation\", \"Explain what a neural network is in simple terms.\"),\n    \n    # Code (if trained on code data)\n    (\"Code\", \"Write a Python function that adds two numbers.\"),\n    \n    # Multi-step\n    (\"Multi-step\", \"List 3 benefits of exercise.\"),\n    \n    # Refusal/boundary (interesting to see behavior)\n    (\"Edge case\", \"Summarize the following text: \"),\n]\n\nmodel.eval()\nfor category, user_msg in eval_prompts:\n    prompt = format_prompt(user_msg)\n    \n    print(f\"\\n[{category}]\")\n    print(f\"User: {user_msg}\")\n    print(\"-\" * 40)\n    \n    try:\n        response = generate(model, prompt, max_new_tokens=150, temperature=0.7)\n        # Extract just the assistant response\n        if \"<|assistant|>\" in response:\n            assistant_part = response.split(\"<|assistant|>\\n\")[-1]\n            # Clean up any trailing special tokens\n            assistant_part = assistant_part.replace(\"<|endoftext|>\", \"\").strip()\n        else:\n            assistant_part = response\n        print(f\"Assistant: {assistant_part}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n    \n    print(\"-\" * 40)\n\n# =============================================================================\n# Quantitative checks\n# =============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FORMAT COMPLIANCE CHECK\")\nprint(\"=\" * 80)\n\n# Check if model properly terminates with EOT\ntest_prompts = [format_prompt(p) for _, p in eval_prompts[:3]]\neot_count = 0\ntotal_length = 0\n\nfor prompt in test_prompts:\n    enc = tokenizer.encode(prompt, allowed_special=set(special_tokens.keys()))\n    input_ids = torch.tensor(enc, dtype=torch.long).unsqueeze(0).to(device)\n    \n    # Generate with greedy decoding for consistency\n    model.eval()\n    for _ in range(200):\n        logits = model(input_ids[:, -block_size:] if input_ids.shape[1] > block_size else input_ids)\n        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n        input_ids = torch.cat([input_ids, next_token], dim=1)\n        if next_token.item() == eot_id:\n            eot_count += 1\n            break\n    \n    total_length += input_ids.shape[1] - len(enc)\n\navg_response_len = total_length / len(test_prompts)\neot_rate = 100 * eot_count / len(test_prompts)\n\nprint(f\"EOT termination rate: {eot_count}/{len(test_prompts)} ({eot_rate:.0f}%)\")\nprint(f\"Avg response length: {avg_response_len:.0f} tokens\")\nprint(f\"  → {'Good: Model learns to stop' if eot_rate > 50 else 'Warning: Model may ramble'}\")\nprint(f\"  → {'Good: Reasonable length' if 20 < avg_response_len < 150 else 'Check: Unusual response length'}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Evaluation complete!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628d8da-e6fc-4e40-bf7c-5e3a20e22bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}