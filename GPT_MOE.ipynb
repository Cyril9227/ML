{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT with Mixture of Experts (MoE)\n",
    "\n",
    "In `GPT_pretraining.ipynb` we added a bunch of architectural improvements and optimization tricks, but we still worked with a tiny model (`GPT2-small` scale so ~ $120M$ parameters) which restricted our final output (which wasn't too bad !). In this quick notebook, we try to scale the model a bit and experiment with the idea of mixture of experts.\n",
    "\n",
    "\n",
    "- **Mixture of Experts (MoE)**: Multiple expert MLPs with learned routing. Only top-k experts activated per token.\n",
    "  - More total parameters but same compute per forward pass\n",
    "  - Each expert can specialize in different types of tokens/patterns\n",
    "- **QK-Norm**: RMSNorm on queries and keys before RoPE for training stability\n",
    "- **Load balancing loss**: Prevents expert collapse (all tokens going to same expert)\n",
    "\n",
    "Expected behavior:\n",
    "- ~4x total parameters vs dense model at same compute budget\n",
    "- Better quality at fixed FLOP budget\n",
    "- Requires load balancing to prevent expert collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import csv\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment config\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "# Environment config\n",
    "from huggingface_hub import login\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "if os.path.isfile(\".env\"):\n",
    "    with open(\".env\") as f:\n",
    "        for line in f:\n",
    "            key, value = line.strip().split(\"=\")\n",
    "            os.environ[key] = value\n",
    "\n",
    "    HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "    login(HF_TOKEN)\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import Dataset as ds, concatenate_datasets, load_dataset\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Torch runtime config\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Custom\n",
    "from utils import count_parameters, load_synthetic_data, strip_compile_prefix, round_up, clean_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7494176-e827-4d1a-b659-a2c313af239c",
   "metadata": {},
   "source": [
    "## Config & Model Definition\n",
    "\n",
    "Mostly the same code as the pretraining notebook, including Flash Attention, RMSNorm etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa17962-52b5-4773-868c-5c87bae6d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CONFIG #####\n",
    "\n",
    "# Model architecture - Moderate scale MoE\n",
    "# Strategy: Go deeper (better for small models) + MoE for capacity\n",
    "block_size = 1024\n",
    "batch_size = 10       # conservative for MoE memory overhead\n",
    "embed_dim = 1024      # GPT-2 Medium width\n",
    "num_layers = 16       # deeper than GPT-2 Small (12), helps reasoning\n",
    "num_heads = 16        # head_dim = 64, good for efficiency\n",
    "dropout_prob = 0.1    # should help at this scale\n",
    "mlp_ratio = 4\n",
    "\n",
    "# MoE config\n",
    "num_experts = 8       # 8 experts total\n",
    "top_k_experts = 2     # activate 2 per token → 4x params, same compute\n",
    "aux_loss_coef = 0.01  # load balancing (0.01-0.1 typical range)\n",
    "\n",
    "# Expected params:\n",
    "#   Total:  ~800M (all experts)\n",
    "#   Active: ~250M (similar compute to GPT-2 Medium)\n",
    "\n",
    "# Training\n",
    "MAX_STEPS = 700_000                        # Total number of micro-batches to process\n",
    "GRAD_ACCUM_STEPS = 20                      # Gradient accumulation steps\n",
    "LOG_INTERVAL = MAX_STEPS // 1000           # Log every xxx micro-batches\n",
    "num_workers = 4\n",
    "prefetch = 8\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "model_path = \"gpt_moe_pretrain.pt\"\n",
    "\n",
    "# Estimated VRAM usage (bf16):\n",
    "#   Model params:     ~800M × 2B = 1.6 GB\n",
    "#   Optimizer states: ~800M × 8B = 6.4 GB (AdamW)\n",
    "#   Gradients:        ~800M × 2B = 1.6 GB\n",
    "#   Activations:      ~8-12 GB (batch=12, seq=1024)\n",
    "#   Total:            ~18-22 GB → fits in 32GB comfortably\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MoE Model Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Architecture:    {embed_dim}d × {num_layers}L × {num_heads}H\")\n",
    "print(f\"  MoE:             {num_experts} experts, top-{top_k_experts}\")\n",
    "print(f\"  Batch:           {batch_size} × {block_size} tokens\")\n",
    "print(f\"  Grad Accum:      {GRAD_ACCUM_STEPS} steps\")\n",
    "print(f\"  Effective Batch: {batch_size * GRAD_ACCUM_STEPS * block_size:,} tokens\")\n",
    "print(f\"  Params:          ~800M total, ~250M active\")\n",
    "print(f\"  bf16 supported:  {torch.cuda.is_bf16_supported()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with QK-Norm for improved training stability.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 rotary_emb: RotaryEmbedding,\n",
    "                 causal: bool = True,\n",
    "                 dropout: float = 0.1\n",
    "                ):\n",
    "        super().__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "        \n",
    "        self.causal = causal\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout_p = dropout\n",
    "        \n",
    "        # Fused QKV projection\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        \n",
    "        # QK-Norm: normalize queries and keys before RoPE\n",
    "        self.q_norm = nn.RMSNorm(self.head_dim)\n",
    "        self.k_norm = nn.RMSNorm(self.head_dim)\n",
    "        \n",
    "        self.rotary_emb = rotary_emb\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, k_v_cache=None):\n",
    "        B, T, _ = x.shape\n",
    "        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n",
    "    \n",
    "        if using_cache:\n",
    "            x_q = x[:, -1:, :]\n",
    "            qkv = self.qkv_proj(x_q)\n",
    "        else:\n",
    "            qkv = self.qkv_proj(x)\n",
    "        \n",
    "        Q, K, V = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        def split_heads(t):\n",
    "            return t.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "        \n",
    "        # Apply QK-Norm before RoPE\n",
    "        Q = self.q_norm(Q)\n",
    "        K = self.k_norm(K)\n",
    "    \n",
    "        # Apply RoPE\n",
    "        if using_cache:\n",
    "            past_len = k_v_cache[\"K\"].shape[-2]\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q, offset=past_len)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K, offset=past_len)\n",
    "            \n",
    "            K = torch.cat([k_v_cache[\"K\"], K], dim=-2)\n",
    "            V = torch.cat([k_v_cache[\"V\"], V], dim=-2)\n",
    "            is_causal_step = False\n",
    "        else:\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K)\n",
    "            is_causal_step = self.causal\n",
    "    \n",
    "        if k_v_cache is not None:\n",
    "            k_v_cache[\"K\"] = K.detach()\n",
    "            k_v_cache[\"V\"] = V.detach()\n",
    "    \n",
    "        out = F.scaled_dot_product_attention(\n",
    "            query=Q, key=K, value=V,\n",
    "            attn_mask=None, \n",
    "            dropout_p=self.dropout_p if self.training else 0.0,\n",
    "            is_causal=is_causal_step\n",
    "        )\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(B, -1, self.embed_dim)\n",
    "        out = self.out_proj(out)\n",
    "        return out, k_v_cache\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"Single expert MLP (SwiGLU).\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = round_up(2 * hidden_dim // 3, 8)\n",
    "\n",
    "        # Still using fused implementation for efficiency\n",
    "        self.gate_up_proj = nn.Linear(embed_dim, 2 * hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        gate_up = self.gate_up_proj(x)\n",
    "        gate, up = gate_up.chunk(2, dim=-1)\n",
    "        return self.dropout(self.down_proj(F.silu(gate) * up))\n",
    "\n",
    "\n",
    "class MoEMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts MLP layer.\n",
    "    \n",
    "    Routes each token to top-k experts and combines their outputs.\n",
    "    Includes auxiliary load balancing loss to prevent expert collapse.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, num_experts, top_k, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Router: learned linear layer to score experts\n",
    "        self.router = nn.Linear(embed_dim, num_experts, bias=False)\n",
    "        \n",
    "        # Expert MLPs\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(embed_dim, hidden_dim, dropout_prob) \n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # For tracking load balancing\n",
    "        self.aux_loss = 0.0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, T, D) input tensor\n",
    "        Returns:\n",
    "            output: (B, T, D) combined expert outputs\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        x_flat = x.view(-1, D)  # (B*T, D)\n",
    "        num_tokens = x_flat.shape[0]\n",
    "        \n",
    "        # Compute router logits and probabilities\n",
    "        router_logits = self.router(x_flat)  # (B*T, num_experts)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts per token\n",
    "        top_k_probs, top_k_indices = torch.topk(router_probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Normalize top-k probabilities to sum to 1\n",
    "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute auxiliary load balancing loss\n",
    "        # Goal: encourage uniform expert utilization\n",
    "        if self.training:\n",
    "            # Fraction of tokens routed to each expert\n",
    "            expert_mask = F.one_hot(top_k_indices, num_classes=self.num_experts).sum(dim=1)  # (B*T, E)\n",
    "            tokens_per_expert = expert_mask.float().mean(dim=0)  # (E,)\n",
    "            \n",
    "            # Average router probability per expert\n",
    "            router_prob_per_expert = router_probs.mean(dim=0)  # (E,)\n",
    "            \n",
    "            # Load balancing loss: minimize the product (encourages uniformity)\n",
    "            self.aux_loss = self.num_experts * (tokens_per_expert * router_prob_per_expert).sum()\n",
    "        \n",
    "        # Compute expert outputs (batched for efficiency)\n",
    "        # This is the \"loop over experts\" approach - simpler than sparse dispatch\n",
    "        output = torch.zeros_like(x_flat)\n",
    "        \n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # Find which tokens selected this expert in their top-k\n",
    "            # expert_mask[i, j] = 1 if token i selected expert expert_idx in position j of top-k\n",
    "            expert_mask = (top_k_indices == expert_idx)  # (B*T, top_k)\n",
    "            \n",
    "            if not expert_mask.any():\n",
    "                continue\n",
    "            \n",
    "            # Get tokens that use this expert\n",
    "            token_indices = expert_mask.any(dim=-1).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "            if len(token_indices) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get the weight for this expert for these tokens\n",
    "            # Shape: (num_selected_tokens,)\n",
    "            weights = (top_k_probs * expert_mask.float()).sum(dim=-1)[token_indices]\n",
    "            \n",
    "            # Compute expert output\n",
    "            expert_input = x_flat[token_indices]\n",
    "            expert_output = self.experts[expert_idx](expert_input)\n",
    "            \n",
    "            # Weighted addition to output\n",
    "            output[token_indices] += weights.unsqueeze(-1) * expert_output\n",
    "        \n",
    "        return output.view(B, T, D)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with MoE MLP.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 rotary_emb,\n",
    "                 mlp_ratio=4,\n",
    "                 num_experts=8,\n",
    "                 top_k_experts=2,\n",
    "                 dropout_prob=0.1,\n",
    "                 causal=True,\n",
    "                ): \n",
    "        super().__init__()\n",
    "        self.norm1 = nn.RMSNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, rotary_emb, causal, dropout_prob)\n",
    "        self.norm2 = nn.RMSNorm(embed_dim)\n",
    "        \n",
    "        # MoE instead of standard MLP\n",
    "        hidden_dim = mlp_ratio * embed_dim\n",
    "        self.moe = MoEMLP(embed_dim, hidden_dim, num_experts, top_k_experts, dropout_prob)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        x1 = self.norm1(x)\n",
    "        x2, cache = self.mha(x1, cache)\n",
    "        x2 = x2 + x  # residual\n",
    "    \n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.moe(x3) + x2  # residual\n",
    "        return x3, cache\n",
    "    \n",
    "    def get_aux_loss(self):\n",
    "        \"\"\"Return the MoE auxiliary loss for this block.\"\"\"\n",
    "        return self.moe.aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT with Mixture of Experts.\n",
    "    \n",
    "    Same architecture as base GPT but with MoE layers replacing standard MLPs.\n",
    "    Includes auxiliary loss collection for load balancing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 num_experts=8,\n",
    "                 top_k_experts=2,\n",
    "                 dropout_prob=0.1,\n",
    "                 is_causal=True,\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k_experts = top_k_experts\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.rotary_emb = RotaryEmbedding(dim=head_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_dim, num_heads, self.rotary_emb, \n",
    "                mlp_ratio, num_experts, top_k_experts,\n",
    "                dropout_prob, is_causal\n",
    "            ) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.RMSNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # weight tying\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Scale residual projections\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith((\"out_proj.weight\", \"down_proj.weight\")):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.num_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "       \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.embedding(tokens)\n",
    "        x = self.dropout(embeddings)\n",
    "        for b in self.blocks:\n",
    "            x, _ = b(x)\n",
    "        features = self.norm(x)\n",
    "        return self.lm_head(features)\n",
    "    \n",
    "    def get_aux_loss(self):\n",
    "        \"\"\"Collect and sum auxiliary losses from all MoE layers.\"\"\"\n",
    "        total_aux_loss = 0.0\n",
    "        for block in self.blocks:\n",
    "            total_aux_loss += block.get_aux_loss()\n",
    "        return total_aux_loss / self.num_layers  # average over layers\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total and active parameters.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        \n",
    "        # Estimate active params (non-MoE + top-k fraction of MoE)\n",
    "        non_moe_params = 0\n",
    "        moe_params = 0\n",
    "        \n",
    "        for name, p in self.named_parameters():\n",
    "            if 'experts' in name:\n",
    "                moe_params += p.numel()\n",
    "            else:\n",
    "                non_moe_params += p.numel()\n",
    "        \n",
    "        # Active MoE params = (top_k / num_experts) * total_moe_params\n",
    "        active_moe = moe_params * (self.top_k_experts / self.num_experts)\n",
    "        active_params = non_moe_params + active_moe\n",
    "        \n",
    "        return total_params, int(active_params)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 prompt_tokens,\n",
    "                 max_new_tokens=50,\n",
    "                 temperature=1.0,\n",
    "                 top_k=0,\n",
    "                 top_p=0.0,\n",
    "                 use_cache=True,\n",
    "                ):\n",
    "        self.eval()\n",
    "\n",
    "        tokens_out = prompt_tokens.clone()\n",
    "        current_tokens = prompt_tokens.clone()\n",
    "        tokens_out = tokens_out.to(self.device)\n",
    "        current_tokens = current_tokens.to(self.device)\n",
    "        cache = [{} if use_cache else None for _ in range(len(self.blocks))]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            x = self.embedding(current_tokens)\n",
    "            for i, b in enumerate(self.blocks):\n",
    "                x, c_i = b(x, cache[i])\n",
    "                cache[i] = c_i\n",
    "            \n",
    "            features = self.norm(x)\n",
    "            logits = self.lm_head(features)    \n",
    "            last_logits = logits[:, -1, :]\n",
    "    \n",
    "            if temperature == 0:\n",
    "                next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                scaled_logits = last_logits / temperature\n",
    "                \n",
    "                if int(top_k) > 0:\n",
    "                    values, indices = torch.topk(scaled_logits, top_k)\n",
    "                    scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n",
    "                    scaled_logits.scatter_(1, indices, values)\n",
    "\n",
    "                if top_p > 0.0 and top_p < 1.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True, dim=-1)\n",
    "                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                    scaled_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            if next_token.item() == eot_id:\n",
    "                break\n",
    "            \n",
    "            tokens_out = torch.cat([tokens_out, next_token], dim=1)\n",
    "            current_tokens = next_token if use_cache else tokens_out\n",
    "       \n",
    "        return tokens_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed8616-4de7-4595-afcb-8b36c3dea3f9",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233fbc6d-4607-4bdc-bc51-2bd536a68ba2",
   "metadata": {},
   "source": [
    "For pretraining we use the base GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cbeb30-9756-4a6f-a1a4-a812ba79d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Setup Tokenizer (tiktoken gpt2 - 50,257 vocab)\n",
    "# ============================================================================\n",
    "\n",
    "# print(\"Loading tokenizer...\")\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# eot_id = tokenizer.eot_token\n",
    "\n",
    "# # Pad vocab to nearest 128 multiple for GPU efficiency\n",
    "# vocab_size = round_up(tokenizer.n_vocab, 128)\n",
    "\n",
    "# print(f\"✓ Loaded tiktoken gpt2\")\n",
    "# print(f\"  Vocab size: {tokenizer.n_vocab} → padded: {vocab_size}\")\n",
    "# print(f\"  EOT token ID: {eot_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94563882-7e25-482e-a16a-c95122945aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Mistral tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "eot_id = tokenizer.eos_token_id\n",
    "vocab_size = round_up(len(tokenizer), 128)\n",
    "print(f\"✓ Loaded Mistral tokenizer\")\n",
    "print(f\"  Vocab size: {len(tokenizer)} → padded: {vocab_size}\")\n",
    "print(f\"  EOT token ID: {eot_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0dfe38-ef9a-410b-9fad-1bcec3c08e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load and Clean Datasets\n",
    "# ============================================================================\n",
    "print(\"\\nLoading & cleaning up datasets...\")\n",
    "cleaned_datasets = {\n",
    "    # Main dataset, high quality web crawl\n",
    "    \"fineweb-edu\": load_dataset(\n",
    "        \"HuggingFaceFW/fineweb-edu\", \n",
    "        name=\"sample-10BT\", \n",
    "        split=\"train\",\n",
    "        trust_remote_code=True\n",
    "    ),\n",
    "}\n",
    "\n",
    "cleaned_datasets = {n: clean_columns(d) for n, d in cleaned_datasets.items()}\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(cleaned_datasets)} datasets\")\n",
    "for n, ds in cleaned_datasets.items():\n",
    "    print(f\"  Dataset {n}: {len(ds):,} examples\")\n",
    "\n",
    "\n",
    "print(\"\\nConcatenating datasets...\")\n",
    "train_ds = concatenate_datasets(list(cleaned_datasets.values()))\n",
    "print(f\"  Combined size: {len(train_ds):,} examples\")\n",
    "\n",
    "\n",
    "print(\"Shuffling...\")\n",
    "train_ds = train_ds.shuffle(seed=42)\n",
    "print(f\"✓ Final Train Size: {len(train_ds):,} rows\")\n",
    "\n",
    "\n",
    "def clean_text(example):\n",
    "    \"\"\"Remove special tokens and other artifacts\"\"\"\n",
    "    text = example[\"text\"]\n",
    "    \n",
    "    # Remove all common special tokens\n",
    "    special_tokens = [\n",
    "        \"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|system|>\", \"<|user|>\", \"<|assistant|>\",\n",
    "        \"<s>\", \"</s>\", \"[INST]\", \"[/INST]\",\n",
    "        \"<<SYS>>\", \"<</SYS>>\"\n",
    "    ]\n",
    "    \n",
    "    for token in special_tokens:\n",
    "        text = text.replace(token, \"\")\n",
    "    \n",
    "    example[\"text\"] = text\n",
    "    return example\n",
    "\n",
    "# Tokenizer is going to complain if special tokens are found in training data\n",
    "print(\"\\nCleaning special tokens from datasets...\")\n",
    "train_ds = train_ds.map(\n",
    "    clean_text,\n",
    "    num_proc=os.cpu_count() // 2,\n",
    "    desc=\"Cleaning special tokens\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Document Packing Function\n",
    "# ============================================================================\n",
    "def pack_documents(examples):\n",
    "    \"\"\"\n",
    "    Concatenate all documents in the batch, then slice into fixed-size blocks.\n",
    "    Each document is terminated with exactly ONE EOT token.\n",
    "    \n",
    "    Output chunks are of length (block_size + 1), suitable for\n",
    "    x = chunk[:-1], y = chunk[1:].\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        if not text or not text.strip():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            doc_ids = tokenizer.encode(text)\n",
    "        except Exception as e:\n",
    "            continue  # skip bad docs safely\n",
    "\n",
    "        doc_ids.append(eot_id)  # exactly one end-of-text\n",
    "        all_tokens.extend(doc_ids)\n",
    "\n",
    "    # Now chop into blocks of (block_size + 1)\n",
    "    chunks = []\n",
    "    total = len(all_tokens)\n",
    "\n",
    "    for i in range(0, total, block_size):\n",
    "        chunk = all_tokens[i : i + block_size + 1]\n",
    "        if len(chunk) == block_size + 1:\n",
    "            chunks.append(chunk)\n",
    "        # else: drop the final tiny tail (standard practice)\n",
    "\n",
    "    return {\"chunk_ids\": chunks}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Apply Processing\n",
    "# ============================================================================\n",
    "print(\"\\nTokenizing and packing documents (this may take a few minutes)...\")\n",
    "train_tokenized = train_ds.map(\n",
    "    pack_documents,\n",
    "    batched=True,\n",
    "    batch_size=1250,\n",
    "    num_proc=multiprocessing.cpu_count() // 2,\n",
    "    remove_columns=train_ds.column_names,\n",
    "    desc=\"Packing documents\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Tokenization complete!\")\n",
    "print(f\"  Total chunks: {len(train_tokenized):,}\")\n",
    "print(f\"  Chunk size: {block_size + 1} tokens\")\n",
    "print(f\"  Approx total tokens: {len(train_tokenized) * block_size:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create DataLoader\n",
    "# ============================================================================\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx][\"chunk_ids\"]\n",
    "        # Return input (x) and target (y) shifted by 1\n",
    "        # [0, 1, 2, ..., block_size-1]\n",
    "        # [1, 2, 3, ..., block_size]\n",
    "        return chunk[:-1].long(), chunk[1:].long()\n",
    "\n",
    "# Can also use numpy format and cast to tensors in the loader, not sure which one is slower\n",
    "train_tokenized.set_format(type=\"torch\", columns=[\"chunk_ids\"])\n",
    "token_dataset = TokenDataset(train_tokenized)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    token_dataset,\n",
    "    batch_size=batch_size,\n",
    "    prefetch_factor=prefetch,\n",
    "    shuffle=True,\n",
    "    drop_last=True,  # CRITICAL for torch.compile ! If one batch has dif shape it will trigger re-compilation and slow down training a lot\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ DataLoader ready\")\n",
    "print(f\"  Batches per epoch: {len(train_loader):,}\")\n",
    "print(f\"  Tokens per batch: {batch_size * block_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237d148-f21d-45e7-81dc-7b7d5543a14d",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "**Note**: MoE architecture is different from dense GPT, so we train from scratch.\n",
    "Cannot load pretrained weights from the dense model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae4514-68d7-466c-b69e-3480dc0b14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"mlp_ratio\": mlp_ratio,\n",
    "    \"num_experts\": num_experts,\n",
    "    \"top_k_experts\": top_k_experts,\n",
    "    \"dropout_prob\": dropout_prob,\n",
    "}\n",
    "\n",
    "print(\"Initializing MoE model with config:\")\n",
    "pprint(model_config)\n",
    "\n",
    "model = GPT_MoE(**model_config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params, active_params = model.count_parameters()\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  Total parameters:  {total_params:,} ({total_params/1e6:.1f}M)\")\n",
    "print(f\"  Active parameters: {active_params:,} ({active_params/1e6:.1f}M)\")\n",
    "print(f\"  Ratio: {total_params/active_params:.2f}x total vs active\")\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ff922-468a-49b1-a8c7-26230af36f68",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "For MoE models:\n",
    "- Same weight decay rules (don't decay norms, embeddings)\n",
    "- Router parameters should NOT be decayed (they're like attention-ish)\n",
    "- Add auxiliary load balancing loss to prevent expert collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e581bf-67ec-4db3-9bcd-a1516f736f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate parameters into decay and no-decay groups\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        continue\n",
    "    # Don't apply weight decay to:\n",
    "    # - Norm parameters (RMSNorm weights, QK-norm)\n",
    "    # - Embedding table (tied with lm_head)\n",
    "    # - Router (small, should be flexible)\n",
    "    # - Any bias terms\n",
    "    if any(kw in name.lower() for kw in ['norm', 'bias', 'embed', 'embedding', 'lm_head', 'router']):\n",
    "        no_decay_params.append(param)\n",
    "    else:\n",
    "        decay_params.append(param)\n",
    "\n",
    "print(f\"Decay params: {len(decay_params)}, No decay params: {len(no_decay_params)}\")\n",
    "\n",
    "# Hyperparameters tuned for ~800M param MoE model\n",
    "# Rule of thumb: larger models need lower LR\n",
    "base_lr = 2e-4  # slightly lower than 124M model\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params, 'weight_decay': 0.1},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "],\n",
    "    lr=base_lr,\n",
    "    betas=(0.9, 0.95),  # standard for LLMs\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler based on OPTIMIZER steps (not micro steps!)\n",
    "total_optim_steps = MAX_STEPS // GRAD_ACCUM_STEPS\n",
    "warmup_steps = int(total_optim_steps * 0.05)  # 5% warmup\n",
    "\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.01,  # start at 1% of base LR\n",
    "    total_iters=warmup_steps\n",
    ")\n",
    "cosine_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_optim_steps - warmup_steps,\n",
    "    eta_min=base_lr * 0.1  # decay to 10% of base\n",
    ")\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "    milestones=[warmup_steps]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining schedule:\")\n",
    "print(f\"  Total micro steps: {MAX_STEPS:,}\")\n",
    "print(f\"  Grad accum steps:  {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"  Total optim steps: {total_optim_steps:,}\")\n",
    "print(f\"  Warmup steps:      {warmup_steps:,} ({100*warmup_steps/total_optim_steps:.0f}%)\")\n",
    "print(f\"  Base LR:           {base_lr}\")\n",
    "print(f\"  Min LR:            {base_lr * 0.1}\")\n",
    "print(f\"  Weight decay:      0.1 (linear), 0.0 (norm/embed/router)\")\n",
    "print(f\"  Aux loss coef:     {aux_loss_coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v29igusn6x",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Fused Cross-Entropy Loss (Triton)\n",
    "# ============================================================================\n",
    "# Saves ~3GB memory and is ~2x faster than standard F.cross_entropy for large vocab\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def fused_cross_entropy_fwd_kernel(\n",
    "    logits_ptr, losses_ptr, lse_ptr, targets_ptr,\n",
    "    stride_logits_row,\n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    row_idx = tl.program_id(0).to(tl.int64)\n",
    "    logits_row_ptr = logits_ptr + row_idx * stride_logits_row\n",
    "    \n",
    "    max_val = -float('inf')\n",
    "    sum_exp = 0.0\n",
    "    \n",
    "    for off in range(0, n_cols, BLOCK_SIZE):\n",
    "        cols = off + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = cols < n_cols\n",
    "        logits = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)\n",
    "        \n",
    "        curr_max = tl.max(logits, axis=0)\n",
    "        new_max = tl.maximum(max_val, curr_max)\n",
    "        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(logits - new_max), axis=0)\n",
    "        max_val = new_max\n",
    "    \n",
    "    lse = max_val + tl.log(sum_exp)\n",
    "    tl.store(lse_ptr + row_idx, lse)\n",
    "    \n",
    "    target = tl.load(targets_ptr + row_idx).to(tl.int64)\n",
    "    target_logit = tl.load(logits_row_ptr + target).to(tl.float32)\n",
    "    loss = lse - target_logit\n",
    "    \n",
    "    tl.store(losses_ptr + row_idx, loss)\n",
    "\n",
    "\n",
    "@triton.jit  \n",
    "def fused_cross_entropy_bwd_kernel(\n",
    "    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr,\n",
    "    stride_logits_row, stride_grad_row,\n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    row_idx = tl.program_id(0).to(tl.int64)\n",
    "    logits_row_ptr = logits_ptr + row_idx * stride_logits_row\n",
    "    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_row\n",
    "    \n",
    "    lse = tl.load(lse_ptr + row_idx)\n",
    "    grad_loss = tl.load(grad_output_ptr + row_idx)\n",
    "    target = tl.load(targets_ptr + row_idx).to(tl.int64)\n",
    "    \n",
    "    for off in range(0, n_cols, BLOCK_SIZE):\n",
    "        cols = off + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = cols < n_cols\n",
    "        \n",
    "        logits = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)\n",
    "        probs = tl.exp(logits - lse)\n",
    "        is_target = (cols == target).to(tl.float32)\n",
    "        grad = grad_loss * (probs - is_target)\n",
    "        \n",
    "        tl.store(grad_row_ptr + cols, grad.to(tl.bfloat16), mask=mask)\n",
    "\n",
    "\n",
    "class FusedCrossEntropyLoss(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits, targets):\n",
    "        n_rows, n_cols = logits.shape\n",
    "        \n",
    "        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)\n",
    "        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)\n",
    "        \n",
    "        logits = logits.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "        \n",
    "        grid = (n_rows,)\n",
    "        fused_cross_entropy_fwd_kernel[grid](\n",
    "            logits, losses, lse, targets,\n",
    "            logits.stride(0),\n",
    "            n_cols,\n",
    "            BLOCK_SIZE=1024,\n",
    "            num_warps=8,\n",
    "        )\n",
    "        \n",
    "        ctx.save_for_backward(logits, targets, lse)\n",
    "        return losses\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        logits, targets, lse = ctx.saved_tensors\n",
    "        n_rows, n_cols = logits.shape\n",
    "        \n",
    "        grad_input = torch.empty_like(logits, dtype=torch.bfloat16)\n",
    "        grad_output = grad_output.contiguous()\n",
    "        \n",
    "        grid = (n_rows,)\n",
    "        fused_cross_entropy_bwd_kernel[grid](\n",
    "            grad_input, grad_output, lse, logits, targets,\n",
    "            logits.stride(0), grad_input.stride(0),\n",
    "            n_cols,\n",
    "            BLOCK_SIZE=1024,\n",
    "            num_warps=8,\n",
    "        )\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "def fused_cross_entropy(logits, targets):\n",
    "    \"\"\"Drop-in replacement for F.cross_entropy with fused kernel.\"\"\"\n",
    "    return FusedCrossEntropyLoss.apply(logits, targets).mean()\n",
    "\n",
    "\n",
    "print(\"✓ Fused cross-entropy loss defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homio1e842u",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Compile Model\n",
    "# ============================================================================\n",
    "\n",
    "torch._inductor.config.coordinate_descent_tuning = True\n",
    "model = torch.compile(model, dynamic=False, fullgraph=False)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Warmup passes to force compilation\n",
    "print(\"Warming up compilation...\")\n",
    "x = torch.randint(0, vocab_size, (batch_size, block_size), device=device)\n",
    "y = x.clone()\n",
    "\n",
    "s = time.time()\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    with torch.autocast(\"cuda\", torch.bfloat16):\n",
    "        logits = model(x)\n",
    "        ce_loss = fused_cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            y.view(-1)\n",
    "        )\n",
    "        aux_loss = model.get_aux_loss()\n",
    "        loss = ce_loss + aux_loss_coef * aux_loss\n",
    "    loss.backward()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"  Warmup {i+1}: {time.time() - start:.2f}s\")\n",
    "\n",
    "print(f\"✓ Model fully compiled in {time.time() - s:.2f}s\")\n",
    "\n",
    "del x, y\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3a017-b03e-4e16-9e46-169a2e6fc1d4",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "MoE training includes:\n",
    "- Main cross-entropy loss (language modeling)\n",
    "- Auxiliary load balancing loss (prevents expert collapse)\n",
    "\n",
    "Total loss = CE loss + aux_loss_coef × aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CSV Logger ---\n",
    "log_file = f'{model_path.split(\".\")[0]}__{datetime.now().strftime(\"%Y-%m-%d\")}__pretraining_logs.csv'\n",
    "print(\"Saving logs in : \", log_file)\n",
    "file_exists = os.path.isfile(log_file)\n",
    "with open(log_file, \"a\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    if not file_exists:\n",
    "        writer.writerow([\"micro_step\", \"optim_step\", \"ce_loss\", \"aux_loss\", \"total_loss\", \"lr\", \"tokens_seen\", \"tokens_per_sec\", \"timestamp\"])\n",
    "\n",
    "# --- Training Loop ---\n",
    "micro_step = 0\n",
    "optim_step = 0\n",
    "tokens_seen = 0\n",
    "\n",
    "# Accumulate losses on GPU to avoid CPU-GPU sync every step\n",
    "running_ce_loss = torch.zeros(1, device=device)\n",
    "running_aux_loss = torch.zeros(1, device=device)\n",
    "\n",
    "start_time = time.time()\n",
    "start_training = time.time()\n",
    "last_tokens_seen = 0\n",
    "\n",
    "model_params = decay_params + no_decay_params\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "print(f\"\\nStarting training for {MAX_STEPS:,} micro steps...\")\n",
    "print(f\"Logging every {LOG_INTERVAL} steps, checkpointing every 50k steps\\n\")\n",
    "\n",
    "while micro_step < MAX_STEPS:\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        B, T = x.shape\n",
    "        tokens_seen += B * T\n",
    "\n",
    "        # --- Forward ---\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "            logits = model(x)\n",
    "            ce_loss = fused_cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                y.view(-1)\n",
    "            )\n",
    "            # MoE auxiliary loss for load balancing\n",
    "            aux_loss = model.get_aux_loss()\n",
    "            loss = ce_loss + aux_loss_coef * aux_loss\n",
    "\n",
    "        # --- Backward (gradient accumulation) ---\n",
    "        (loss / GRAD_ACCUM_STEPS).backward()\n",
    "\n",
    "        # --- Optimizer step ---\n",
    "        if (micro_step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model_params, 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            optim_step += 1\n",
    "\n",
    "        # --- Bookkeeping (.item() triggers cpu-gpu sync so we avoid it) ---\n",
    "        running_ce_loss += ce_loss.detach()\n",
    "        running_aux_loss += aux_loss.detach() if isinstance(aux_loss, torch.Tensor) else aux_loss\n",
    "        micro_step += 1\n",
    "\n",
    "        # --- Logging ---\n",
    "        if micro_step % LOG_INTERVAL == 0:\n",
    "            # Only sync with CPU here (once per LOG_INTERVAL steps)\n",
    "            avg_ce_loss = (running_ce_loss / LOG_INTERVAL).item()\n",
    "            avg_aux_loss = (running_aux_loss / LOG_INTERVAL).item()\n",
    "            avg_total_loss = avg_ce_loss + aux_loss_coef * avg_aux_loss\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            tokens_delta = tokens_seen - last_tokens_seen\n",
    "            tokens_per_sec = tokens_delta / elapsed\n",
    "\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            print(\n",
    "                f\"step {micro_step:06d} | \"\n",
    "                f\"opt {optim_step:05d} | \"\n",
    "                f\"ce {avg_ce_loss:.3f} | \"\n",
    "                f\"aux {avg_aux_loss:.3f} | \"\n",
    "                f\"lr {current_lr:.2e} | \"\n",
    "                f\"{tokens_per_sec:,.0f} tok/s\"\n",
    "            )\n",
    "\n",
    "            with open(log_file, \"a\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    micro_step,\n",
    "                    optim_step,\n",
    "                    f\"{avg_ce_loss:.4f}\",\n",
    "                    f\"{avg_aux_loss:.4f}\",\n",
    "                    f\"{avg_total_loss:.4f}\",\n",
    "                    f\"{current_lr:.2e}\",\n",
    "                    tokens_seen,\n",
    "                    int(tokens_per_sec),\n",
    "                    timestamp,\n",
    "                ])\n",
    "\n",
    "            running_ce_loss.zero_()\n",
    "            running_aux_loss.zero_()\n",
    "            start_time = time.time()\n",
    "            last_tokens_seen = tokens_seen\n",
    "\n",
    "        # --- Checkpointing ---\n",
    "        if micro_step % 50_000 == 0 and micro_step > 0:\n",
    "            mid_model_path = model_path.replace(\".pt\", f\"_{micro_step}.pt\")\n",
    "            print(f\"Saving intermediate model to {mid_model_path}\")\n",
    "            torch.save(model.state_dict(), mid_model_path)\n",
    "\n",
    "        # --- Exit ---\n",
    "        if micro_step >= MAX_STEPS:\n",
    "            elapsed = int(time.time() - start_training)\n",
    "            h, m, s = elapsed // 3600, (elapsed % 3600) // 60, elapsed % 60\n",
    "            print(f\"\\nProcessed {tokens_seen:,} tokens in {h:02d}:{m:02d}:{s:02d}\")\n",
    "            print(f\"Saving final model to {model_path}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628d8da-e6fc-4e40-bf7c-5e3a20e22bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test generation\n",
    "prompt = \"The mixture of experts architecture allows\" \n",
    "x = torch.tensor(tokenizer.encode(prompt))\n",
    "\n",
    "model.eval()\n",
    "out = model.generate(\n",
    "    x.unsqueeze(0).to(device),\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95,\n",
    "    top_k=0,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "print(\"\\nPrompt:\", prompt)\n",
    "print(\"Output:\", tokenizer.decode(out[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
