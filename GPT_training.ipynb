{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "First notebook was written from 'scratch', this one leverages existing libraries to experiment with actual training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/harvard-edge/cs249r_book\n",
    "\n",
    "import csv\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism.\n",
    "\n",
    "    Runs multiple attention heads in parallel, each learning different relationships and use RoPE for positional encoding\n",
    "   \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, causal: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Validate configuration\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\"\n",
    "            )\n",
    "        \n",
    "        # 2. Store parameters\n",
    "        self.causal = causal  # tokens can only attend to themselves or past tokens, no cheating !\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # 3. Linear projections for Q, K, V\n",
    "        # Each maps: (B, N, embed_dim) → (B, N, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        # 4. Rotary embeddings\n",
    "        self.rotary_emb = RotaryEmbedding(dim=self.head_dim)\n",
    "\n",
    "        # 4. Output projection (after concatenating heads)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, k_v_cache=None):\n",
    "        B, T, _ = x.shape\n",
    "        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n",
    "    \n",
    "        # 1. Linear projections\n",
    "        if using_cache:\n",
    "            # Only compute projections for the new token\n",
    "            x_q = x[:, -1:, :]  # (B, 1, E)\n",
    "            Q = self.q_proj(x_q)\n",
    "            K = self.k_proj(x_q)\n",
    "            V = self.v_proj(x_q)\n",
    "        else:\n",
    "            # Training or prefill\n",
    "            Q = self.q_proj(x)\n",
    "            K = self.k_proj(x)\n",
    "            V = self.v_proj(x)\n",
    "    \n",
    "    \n",
    "        def split_heads(t):\n",
    "            return (\n",
    "                t.view(B, -1, self.num_heads, self.head_dim)\n",
    "                 .transpose(1, 2)\n",
    "            )\n",
    "    \n",
    "        # 2. Split heads → (B, H, T, D_head)\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "    \n",
    "        # 3. Apply RoPE \n",
    "        if using_cache:\n",
    "           # We need to know how many tokens are already in the past\n",
    "            past_len = k_v_cache[\"K\"].shape[-2]\n",
    "            \n",
    "            # 1. Rotate ONLY the new Q and K\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q, offset=past_len)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K, offset=past_len)\n",
    "            \n",
    "            # 2. Concatenate to the cache -> [Rotated_Past, Rotated_New]\n",
    "            K = torch.cat([k_v_cache[\"K\"], K], dim=-2)\n",
    "            V = torch.cat([k_v_cache[\"V\"], V], dim=-2)\n",
    "        else:\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K)\n",
    "    \n",
    "        # 4. Update cache (only during inference)\n",
    "        if k_v_cache is not None:\n",
    "            k_v_cache[\"K\"] = K\n",
    "            k_v_cache[\"V\"] = V\n",
    "    \n",
    "        # 5. Attention\n",
    "        scores = Q @ K.transpose(-2, -1)\n",
    "        scores = scores / math.sqrt(self.head_dim)\n",
    "    \n",
    "        # Causal mask (only needed when more than one query i.e training or non-cached inference)\n",
    "        if self.causal and Q.shape[-2] > 1:\n",
    "            Tq, Tk = scores.shape[-2:]\n",
    "            mask = torch.triu(\n",
    "                torch.ones(Tq, Tk, device=x.device),\n",
    "                diagonal=1\n",
    "            ).bool()\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = attn @ V  # (B, H, T, D_head)\n",
    "    \n",
    "        # 6. Merge heads\n",
    "        out = (\n",
    "            out.transpose(1, 2)\n",
    "               .contiguous()\n",
    "               .view(B, -1, self.embed_dim)\n",
    "        )\n",
    "    \n",
    "        # 7. Final linear projection\n",
    "        return self.out_proj(out), k_v_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf4ba2d-402d-4eab-86e1-754f9faeaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (Feed-Forward Network) for transformer blocks.\n",
    "\n",
    "    Standard pattern: Linear -> GELU -> Linear with expansion ratio of 4:1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim=None, dropout_prob=0.1):\n",
    "        \"\"\"\n",
    "        Initialize MLP with two linear layers.\n",
    "\n",
    "        APPROACH:\n",
    "        1. First layer expands from embed_dim to hidden_dim (usually 4x larger)\n",
    "        2. Second layer projects back to embed_dim\n",
    "        3. Use GELU activation (smoother than ReLU, preferred in transformers)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * embed_dim  # Standard transformer expansion\n",
    "\n",
    "        # params\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # actual layers, bias on by default, some architectures turn it off here\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.linear1(x))\n",
    "        x = self.linear2(x)  # No GELU here to not mess with residual paths x + MLP(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c04d073-6e3f-469d-98b4-5cda92115923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Block with self-attention, MLP, and residual connections.\n",
    "\n",
    "    This is the core building block of GPT and other transformer models.\n",
    "    Each block processes the input sequence and passes it to the next block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout_prob=0.1, causal=True):\n",
    "        \"\"\"\n",
    "        Initialize a complete transformer block.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Multi-head self-attention for sequence modeling\n",
    "        2. First layer normalization (pre-norm architecture)\n",
    "        3. MLP with specified expansion ratio\n",
    "        4. Second layer normalization\n",
    "\n",
    "        TRANSFORMER BLOCK ARCHITECTURE:\n",
    "        x → LayerNorm → MultiHeadAttention → + (residual) →\n",
    "            LayerNorm → MLP → + (residual) → output\n",
    "\n",
    "        NB: We use pre-norm architecture (LayerNorm before attention/MLP)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, causal)  # causal = masking out tokens\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio * embed_dim, dropout_prob)\n",
    "\n",
    "    def forward(self, x, cache=None):\n",
    "        \"\"\"\n",
    "        Forward pass through transformer block.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Apply layer norm, then self-attention, then add residual\n",
    "        2. Apply layer norm, then MLP, then add residual\n",
    "        3. Return the transformed sequence\n",
    "\n",
    "        COMPUTATION FLOW:\n",
    "        x → ln1 → attention → + x → ln2 → mlp → + → output\n",
    "\n",
    "        RESIDUAL CONNECTIONS:\n",
    "        These are crucial for training deep networks - they allow gradients\n",
    "        to flow directly through the network during backpropagation.\n",
    "\n",
    "        HINT: \n",
    "        - Store intermediate results to add residual connections properly\n",
    "        - Don't forget masking\n",
    "        \"\"\"\n",
    "        x1 = self.layernorm1(x)\n",
    "        x2, cache = self.mha(x1, cache)  # will be used when generating tokens during inference\n",
    "        x2 = x2 + x  # residual path\n",
    "\n",
    "        x3 = self.layernorm2(x2)\n",
    "        x3 = self.mlp(x3) + x2  # residual path\n",
    "        return x3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT (Generative Pre-trained Transformer) model.\n",
    "\n",
    "    This combines embeddings, positional encoding, multiple transformer blocks,\n",
    "    and a language modeling head for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize complete GPT model.\n",
    "\n",
    "        APPROACH:\n",
    "        1. Token embedding layer to convert tokens to vectors\n",
    "        2. Positional embedding to add position information\n",
    "        3. Stack of transformer blocks (the main computation)\n",
    "        4. Final layer norm and language modeling head\n",
    "\n",
    "        GPT ARCHITECTURE:\n",
    "        tokens → embedding → + pos_embedding →\n",
    "                transformer_blocks → layer_norm → lm_head → logits\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> model = GPT(vocab_size=1000, embed_dim=256, num_layers=6, num_heads=8)\n",
    "        >>> tokens = Tensor(np.random.randint(0, 1000, (2, 10)))  # (batch, seq)\n",
    "        >>> logits = model.forward(tokens)\n",
    "        >>> assert logits.shape == (2, 10, 1000)  # (batch, seq, vocab)\n",
    "\n",
    "        HINTS:\n",
    "        - Positional embeddings are learned, not fixed sinusoidal\n",
    "        - Final layer norm stabilizes training\n",
    "        - Language modeling head shares weights with token embedding (tie_weights)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        # weight tying\n",
    "        self.embedding.weight = self.lm_head.weight\n",
    "\n",
    "        # below shamefully stolen from nano-gpt\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.embedding.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "       \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.embedding(tokens)\n",
    "        x = self.dropout(embeddings)\n",
    "        for b in self.blocks:\n",
    "            x, _ = b(x)  # iteratively refines features from initial embeddings\n",
    "        features = self.ln(x)  # normalized to stabilize training\n",
    "        return self.lm_head(features)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 prompt_tokens,\n",
    "                 max_new_tokens=50,\n",
    "                 temperature=1.0,\n",
    "                 use_cache=True,\n",
    "                 use_top_k=False,\n",
    "                ):\n",
    "        self.eval()\n",
    "\n",
    "        tokens_out = prompt_tokens.clone()\n",
    "        current_tokens = prompt_tokens.clone()\n",
    "        tokens_out = tokens_out.to(self.device)\n",
    "        current_tokens = current_tokens.to(self.device)\n",
    "        cache = [{} if use_cache else None for _ in range(len(self.blocks))]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            x = self.embedding(current_tokens)\n",
    "            \n",
    "            for i, b in enumerate(self.blocks):\n",
    "                x, c_i = b(x, cache[i])\n",
    "                cache[i] = c_i\n",
    "            \n",
    "            features = self.ln(x)\n",
    "            logits = self.lm_head(features)\n",
    "                    \n",
    "            last_logits = logits[:, -1, :]\n",
    "    \n",
    "            if temperature > 0:\n",
    "                scaled_logits = last_logits / temperature\n",
    "                # Only sample from top k tokens to avoid garbage prediction derailing whole prediction\n",
    "                # We don't simply take max prob token to allow \"creativity\"\n",
    "                if use_top_k:\n",
    "                    # heuristic that is ok for toy project\n",
    "                    # most of probability mass in on a small amount of tokens\n",
    "                    k = min(max(5, int(0.01 * self.vocab_size)), 100)\n",
    "                    values, indices = torch.topk(scaled_logits, k)\n",
    "                    scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n",
    "                    scaled_logits.scatter_(1, indices, values)\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy decoding if temp is 0 (prevents division by zero)\n",
    "                next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
    "    \n",
    "            tokens_out = torch.cat([tokens_out, next_token], dim=1)\n",
    "\n",
    "            # If caching, we only need to feed the newest token next time, otherwise full sequence\n",
    "            current_tokens = next_token if use_cache else tokens_out\n",
    "       \n",
    "        return tokens_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec6e7e-9bce-467f-9c36-aadb01c2ec5f",
   "metadata": {},
   "source": [
    "## Full Training on Wikipedia\n",
    "\n",
    "Note : we re-use the tokenizer defined at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04a17c6-e96f-4943-827d-649214ae44d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_bf16_supported() True\n"
     ]
    }
   ],
   "source": [
    "#### CONFIG #####\n",
    "\n",
    "# Basically GPT-2 Small\n",
    "block_size = 1024  # context / seq len\n",
    "batch_size = 10\n",
    "embed_dim = 768\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "\n",
    "\n",
    "# Training\n",
    "MAX_STEPS = 115000       # Total number of micro-batches to process\n",
    "GRAD_ACCUM_STEPS = 40    # Accumulate gradients over 40 batches\n",
    "LOG_INTERVAL = 500       # Log every 500 micro-batches\n",
    "num_workers = 4          # For data loading\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "model_path = \"gpt_model_final.pt\"  # where do we store trained model\n",
    "print(\"torch.cuda.is_bf16_supported()\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92cd5fa-1da3-4e79-8760-6c9faadd9516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86579942953d49eb9793809d18816767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de31be06074432ab12e1945ae004992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Train Size: 8238519 rows\n",
      "Tokenizing and chunking (this may take a moment)...\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()  # faster dl\n",
    "\n",
    "\n",
    "# --- 1. Setup Tokenizer (Your specific implementation) ---\n",
    "tokenizer = Tokenizer.from_pretrained(\"GPT2\")\n",
    "eot_id = tokenizer.token_to_id(\"<|endoftext|>\")\n",
    "assert eot_id is not None\n",
    "\n",
    "# --- 2. Load and Normalize Datasets ---\n",
    "# We need to make sure all datasets have a 'text' column and nothing else\n",
    "def clean_columns(ds):\n",
    "    # Some datasets use 'line' or 'sentence', we rename to 'text'\n",
    "    if 'line' in ds.column_names:\n",
    "        ds = ds.rename_column('line', 'text')\n",
    "    # Keep only the 'text' column to ensure concatenation works\n",
    "    return ds.select_columns(['text'])\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Train List\n",
    "train_raw = [\n",
    "    load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\"),\n",
    "    load_dataset(\"tiny_shakespeare\", split=\"train\"),\n",
    "    load_dataset(\"bookcorpus\", split=\"train[:10%]\"),\n",
    "    load_dataset(\"openwebtext\", split=\"train[:10%]\"),\n",
    "]\n",
    "# Clean and Concatenate\n",
    "train_ds = concatenate_datasets([clean_columns(d) for d in train_raw])\n",
    "\n",
    "# Validation List\n",
    "val_raw = [\n",
    "    load_dataset(\"tiny_shakespeare\", split=\"validation\"),\n",
    "    load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\"),\n",
    "]\n",
    "val_ds = concatenate_datasets([clean_columns(d) for d in val_raw])\n",
    "\n",
    "print(f\"Raw Train Size: {len(train_ds)} rows\")\n",
    "\n",
    "# --- 3. Optimized Processing Pipeline ---\n",
    "\n",
    "def process_batch(examples):\n",
    "    \"\"\"\n",
    "    1. Tokenizes text.\n",
    "    2. Appends EOT token to EVERY document.\n",
    "    3. Flattens into a 1D stream.\n",
    "    4. Chunks into block_size + 1 (to allow for shifting).\n",
    "    \"\"\"\n",
    "    all_token_ids = []\n",
    "    \n",
    "    # 1. Tokenize and add EOT (Document Boundary)\n",
    "    # Note: We use tokenizer.encode_batch for speed if possible, \n",
    "    # but here we loop to append EOT easily.\n",
    "    for text in examples[\"text\"]:\n",
    "        # Skip empty strings\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Encode\n",
    "        ids = tokenizer.encode(text).ids\n",
    "        \n",
    "        # Append EOT (Crucial for GPT context separation)\n",
    "        ids.append(eot_id)\n",
    "        all_token_ids.extend(ids)\n",
    "    \n",
    "    # 2. Chunking\n",
    "    # We need chunks of length (block_size + 1)\n",
    "    chunk_len = block_size + 1\n",
    "    \n",
    "    # Truncate remainder\n",
    "    total_len = (len(all_token_ids) // chunk_len) * chunk_len\n",
    "    \n",
    "    # Reshape into list of lists\n",
    "    chunks = [\n",
    "        all_token_ids[i : i + chunk_len] \n",
    "        for i in range(0, total_len, chunk_len)\n",
    "    ]\n",
    "    \n",
    "    # Return dict for HF Dataset\n",
    "    return {\"chunk_ids\": chunks}\n",
    "\n",
    "# Apply the processing\n",
    "# We remove 'text' immediately to free up RAM.\n",
    "# num_proc uses multiple CPU cores to tokenize faster.\n",
    "print(\"Tokenizing and chunking (this may take a moment)...\")\n",
    "train_tokenized = train_ds.map(\n",
    "    process_batch, \n",
    "    batched=True, \n",
    "    batch_size=1000, \n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    "    remove_columns=train_ds.column_names,\n",
    "    desc=\"Processing Train\"\n",
    ")\n",
    "\n",
    "val_tokenized = val_ds.map(\n",
    "    process_batch, \n",
    "    batched=True, \n",
    "    batch_size=1000, \n",
    "    num_proc=max(1, multiprocessing.cpu_count() // 2),\n",
    "    remove_columns=val_ds.column_names,\n",
    "    desc=\"Processing Val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9b9c5e5-977f-4759-854e-06604ef30c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Final Train Batches: 99463\n",
      "Input x shape: torch.Size([10, 1024])\n",
      "Target y shape: torch.Size([10, 1024])\n",
      "\n",
      "Sanity Check (Shifting):\n",
      "x[0, -5:]: [1103, 4639, 13, 843, 4673]\n",
      "y[0, -5:]: [4639, 13, 843, 4673, 703]\n"
     ]
    }
   ],
   "source": [
    "class GPTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.ds = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the chunk of size BLOCK_SIZE + 1\n",
    "        chunk = self.ds[idx][\"chunk_ids\"]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        data = torch.tensor(chunk, dtype=torch.long)\n",
    "\n",
    "        # Shift target, the model needs to learn token_t -> token_t+1\n",
    "        x = data[:-1]\n",
    "        y = data[1:]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Set format to pytorch (optional, but good practice for speed)\n",
    "train_tokenized.set_format(type=\"numpy\", columns=[\"chunk_ids\"])\n",
    "val_tokenized.set_format(type=\"numpy\", columns=[\"chunk_ids\"])\n",
    "\n",
    "train_dataset = GPTDataset(train_tokenized)\n",
    "val_dataset = GPTDataset(val_tokenized)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, # Shuffle chunks\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"-\" * 20)\n",
    "print(f\"Final Train Batches: {len(train_loader)}\")\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"Input x shape: {x.shape}\")  # Should be [Batch, Block_Size]\n",
    "print(f\"Target y shape: {y.shape}\") # Should be [Batch, Block_Size]\n",
    "\n",
    "print(\"\\nSanity Check (Shifting):\")\n",
    "print(f\"x[0, -5:]: {x[0, -5:].tolist()}\") # End of input\n",
    "print(f\"y[0, -5:]: {y[0, -5:].tolist()}\") # End of target\n",
    "del x\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many actual updates we will do (for the scheduler)\n",
    "total_optim_steps = MAX_STEPS // GRAD_ACCUM_STEPS\n",
    "print(f\"Total Micro-batches: {MAX_STEPS}\")\n",
    "print(f\"Gradient Accumulation: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Total Optimizer Updates: {total_optim_steps}\")\n",
    "\n",
    "# --- Model & Optimizer ---\n",
    "model = GPT(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    ").to(device)\n",
    "\n",
    "model = torch.compile(model) \n",
    "model.train()\n",
    "print(\"Model compiled !\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# T_max is now based on actual optimizer updates, not total loops\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_optim_steps, eta_min=3e-5)\n",
    "\n",
    "# --- CSV Logger ---\n",
    "log_file = \"training_log.csv\"\n",
    "file_exists = os.path.isfile(log_file)\n",
    "with open(log_file, \"a\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    if not file_exists:\n",
    "        writer.writerow([\"micro_step\", \"optim_step\", \"loss\", \"lr\", \"tokens_seen\", \"tokens_per_sec\", \"timestamp\"])\n",
    "\n",
    "# --- Training Loop ---\n",
    "micro_step = 0      # Counts every batch seen\n",
    "optim_step = 0      # Counts every weight update\n",
    "tokens_seen = 0\n",
    "running_loss = 0.0\n",
    "start_time = time.time()\n",
    "start_training = time.time()\n",
    "\n",
    "# Initialize gradients once before starting\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n",
    "# Train until we've seen enough tokens\n",
    "while micro_step < MAX_STEPS:\n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        B, T = x.shape\n",
    "        tokens_seen += B * T\n",
    "\n",
    "        # 1. Forward\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        # 2. Scale Loss for Backward (but keep original for logging!)\n",
    "        current_loss_val = loss.item() \n",
    "        scaled_loss = loss / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        # 3. Backward\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        # 4. Step (only every 40 micro-steps)\n",
    "        if (micro_step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            # avoids exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            optim_step += 1\n",
    "\n",
    "        # 5. Bookkeeping\n",
    "        running_loss += current_loss_val\n",
    "        micro_step += 1\n",
    "\n",
    "        # 6. Logging\n",
    "        if micro_step % LOG_INTERVAL == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_loss = running_loss / LOG_INTERVAL\n",
    "            tokens_per_sec = (B * T * LOG_INTERVAL) / elapsed\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            print(\n",
    "                f\"step {micro_step:06d} | \"\n",
    "                f\"opt_step {optim_step:04d} | \"\n",
    "                f\"loss {avg_loss:.3f} | \"\n",
    "                f\"lr {current_lr:.2e} | \"\n",
    "                f\"{tokens_per_sec:,.0f} tok/s\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                with open(log_file, \"a\", newline=\"\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([micro_step, optim_step, f\"{avg_loss:.4f}\", f\"{current_lr:.2e}\", tokens_seen, int(tokens_per_sec), timestamp])\n",
    "            except Exception as e:\n",
    "                print(f\"CSV Error: {e}\")\n",
    "\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "\n",
    "        if micro_step in [10000, 25000, 50000, 75000]:\n",
    "            mid_model_path = model_path.replace(\".pt\", f\"_{micro_step}.pt\")\n",
    "            print(f\"Saving intermediate model in {mid_model_path}\")\n",
    "            torch.save(model.state_dict(), mid_model_path)\n",
    "        \n",
    "        if micro_step >= MAX_STEPS:\n",
    "            elapsed = int(time.time() - start_training)\n",
    "            h = elapsed // 3600\n",
    "            m = (elapsed % 3600) // 60\n",
    "            s = elapsed % 60\n",
    "            print(f\"\\nProcessed {tokens_seen:,} tokens in {h:02d}:{m:02d}:{s:02d}\")\n",
    "            print(f\"Saving final model in {model_path}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba29a505-812a-470b-aed9-1157488fffbe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m x, y = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_dataset))\n\u001b[32m      3\u001b[39m model.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_top_k\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOutput : \u001b[39m\u001b[33m\"\u001b[39m, tokenizer.decode(out[\u001b[32m0\u001b[39m].tolist()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mGPT.generate\u001b[39m\u001b[34m(self, prompt_tokens, max_new_tokens, temperature, use_cache, use_top_k)\u001b[39m\n\u001b[32m    110\u001b[39m x = \u001b[38;5;28mself\u001b[39m.embedding(current_tokens)\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.blocks):\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     x, c_i = \u001b[43mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     cache[i] = c_i\n\u001b[32m    116\u001b[39m features = \u001b[38;5;28mself\u001b[39m.ln(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, cache)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03mForward pass through transformer block.\u001b[39;00m\n\u001b[32m     36\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33;03m- Don't forget masking\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m x1 = \u001b[38;5;28mself\u001b[39m.layernorm1(x)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m x2, cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will be used when generating tokens during inference\u001b[39;00m\n\u001b[32m     55\u001b[39m x2 = x2 + x  \u001b[38;5;66;03m# residual path\u001b[39;00m\n\u001b[32m     57\u001b[39m x3 = \u001b[38;5;28mself\u001b[39m.layernorm2(x2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, k_v_cache)\u001b[39m\n\u001b[32m     94\u001b[39m     mask = torch.triu(\n\u001b[32m     95\u001b[39m         torch.ones(Tq, Tk, device=x.device),\n\u001b[32m     96\u001b[39m         diagonal=\u001b[32m1\u001b[39m\n\u001b[32m     97\u001b[39m     ).bool()\n\u001b[32m     98\u001b[39m     scores = scores.masked_fill(mask, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m attn = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m out = attn @ V  \u001b[38;5;66;03m# (B, H, T, D_head)\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# 6. Merge heads\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7352fb2eeb70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(val_dataset))\n",
    "\n",
    "model.to(\"cpu\")\n",
    "out = model.generate(\n",
    "    x.unsqueeze(0).to(\"cpu\"),\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.9,\n",
    "    use_cache=False,\n",
    "    use_top_k=False,\n",
    ")\n",
    "\n",
    "print(\"\\nOutput : \", tokenizer.decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a028a-7866-40ad-ba90-570cb8bb614f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b4d1452-0749-4415-a850-6138ae435283",
   "metadata": {},
   "source": [
    "## TO DO : \n",
    "\n",
    "- [x] ROPE for K, V, Q\n",
    "- [x] Top k sampling / Temperature\n",
    "- [x] K / V cache\n",
    "- [x] Add stop token / EOS handling\n",
    "- [x] Training on a real problem to see how far we can push current model\n",
    "- [ ] Clean up / Revisit markdown / maths\n",
    "- [ ] Explore hyper connections and manifold constrained HC\n",
    "- [ ] Check newer architectures / design choices (https://github.com/lucidrains git is a gold mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7b86e-2803-4838-840f-e22e311cb940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
