{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "SFT part : teach the model the assistant format\n",
    "\n",
    "Basically : \n",
    "- the prompt tokens are masked (no loss)\n",
    "- the response tokens are trained on (loss applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import csv\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment config\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "from datasets import Dataset as ds, concatenate_datasets, load_dataset\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Torch runtime config\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Custom\n",
    "from utils import count_parameters, load_synthetic_data, strip_compile_prefix, round_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7494176-e827-4d1a-b659-a2c313af239c",
   "metadata": {},
   "source": [
    "## Config & Model Definition\n",
    "\n",
    "Mostly the same code as the pretraining notebook, including Flash Attention, RMSNorm etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa17962-52b5-4773-868c-5c87bae6d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CONFIG #####\n",
    "\n",
    "# Basically GPT-2 Small\n",
    "block_size = 1024\n",
    "batch_size = 32\n",
    "embed_dim = 768\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "dropout_prob = 0  # <!> finetuning, standard practice to disable dropout <!>\n",
    "mlp_ratio = 4  # standard 4x expansion\n",
    "pretrained_weights = \"gpt_full_run.pt\"\n",
    "\n",
    "# Tokenizer\n",
    "ROLE_TOKENS = [\"<|user|>\", \"<|assistant|>\"]\n",
    "IGNORE_INDEX = -100  # to mask out the loss\n",
    "\n",
    "# Training\n",
    "NUM_EPOCHS = 3  # not too many or we're going to overfit our Q/A data\n",
    "num_workers = 4\n",
    "prefetch = 8\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "print(\"torch.cuda.is_bf16_supported()\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 rotary_emb: RotaryEmbedding,\n",
    "                 causal: bool = True,\n",
    "                 dropout: float = 0.1\n",
    "                ):\n",
    "        super().__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "        \n",
    "        self.causal = causal\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout_p = dropout\n",
    "        \n",
    "        # Fused QKV projection: 3x the output size\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        \n",
    "        # Shared rotary embedding (passed from GPT model)\n",
    "        self.rotary_emb = rotary_emb\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, k_v_cache=None):\n",
    "        B, T, _ = x.shape\n",
    "        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n",
    "    \n",
    "        # 1. Single fused projection\n",
    "        if using_cache:\n",
    "            x_q = x[:, -1:, :]\n",
    "            qkv = self.qkv_proj(x_q)  # (B, 1, 3 x embed_dim)\n",
    "        else:\n",
    "            qkv = self.qkv_proj(x)  # (B, T, 3 x embed_dim)\n",
    "        \n",
    "        # 2. Split into Q, K, V\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)  # Each is (B, T, embed_dim)\n",
    "        \n",
    "        def split_heads(t):\n",
    "            return t.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 3. Split heads -> (B, H, T, D_head)\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "    \n",
    "        # 4. Apply RoPE \n",
    "        if using_cache:\n",
    "            past_len = k_v_cache[\"K\"].shape[-2]\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q, offset=past_len)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K, offset=past_len)\n",
    "            \n",
    "            K = torch.cat([k_v_cache[\"K\"], K], dim=-2)\n",
    "            V = torch.cat([k_v_cache[\"V\"], V], dim=-2)\n",
    "            # When using the cache, the \"causality\" is already enforced by the fact that we are passing 1 query token against all valid past keys \n",
    "            # We don't need a mask as we want the current token to attend to everything in the history\n",
    "            is_causal_step = False\n",
    "        else:\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K)\n",
    "            is_causal_step = self.causal\n",
    "    \n",
    "        # 5. Update cache\n",
    "        if k_v_cache is not None:\n",
    "            k_v_cache[\"K\"] = K.detach()  # we will never .backward on these\n",
    "            k_v_cache[\"V\"] = V.detach()  \n",
    "    \n",
    "        # 6. Attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            query=Q,\n",
    "            key=K,\n",
    "            value=V,\n",
    "            attn_mask=None, \n",
    "            dropout_p=self.dropout_p if self.training else 0.0,\n",
    "            is_causal=is_causal_step\n",
    "        )\n",
    "        \n",
    "        # 7. Merge heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, -1, self.embed_dim)\n",
    "\n",
    "        # 8. Linear projection\n",
    "        return self.out_proj(out), k_v_cache\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim=None, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * embed_dim\n",
    "\n",
    "        hidden_dim = round_up(2 * hidden_dim // 3, 8)\n",
    "\n",
    "        # Fused projection\n",
    "        self.gate_up_proj = nn.Linear(embed_dim, 2 * hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        gate_up = self.gate_up_proj(x)\n",
    "        gate, up = gate_up.chunk(2, dim=-1)\n",
    "        return self.dropout(self.down_proj(F.silu(gate) * up))\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 rotary_emb,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 causal=True,\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Initialize a complete transformer block.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Multi-head self-attention for sequence modeling\n",
    "        2. 1st Normalization (pre-norm architecture)\n",
    "        3. MLP with specified expansion ratio\n",
    "        4. 2nd Normalization\n",
    "    \n",
    "        TRANSFORMER BLOCK ARCHITECTURE:\n",
    "        x → Norm → MultiHeadAttention → + (residual) →\n",
    "            Norm → MLP → + (residual) → output\n",
    "    \n",
    "        NB: We use pre-norm architecture (before attention/MLP)\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "        self.norm1 = nn.RMSNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, rotary_emb, causal, dropout_prob)  # causal = masking out tokens\n",
    "        self.norm2 = nn.RMSNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio * embed_dim, dropout_prob)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        x1 = self.norm1(x)\n",
    "        x2, cache = self.mha(x1, cache)  # will be used when generating tokens during inference\n",
    "        x2 = x2 + x  # residual path\n",
    "    \n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.mlp(x3) + x2  # residual path\n",
    "        return x3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT (Generative Pre-trained Transformer) model.\n",
    "\n",
    "    This combines embeddings, positional encoding, multiple transformer blocks,\n",
    "    and a language modeling head for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 is_causal=True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize complete GPT model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Shared rotary embedding across all layers (more efficient for compilation)\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.rotary_emb = RotaryEmbedding(dim=head_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, self.rotary_emb, mlp_ratio, dropout_prob, is_causal) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.RMSNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # weight tying\n",
    "\n",
    "        # below shamefully stolen from nano-gpt\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        # don't forget swiglu variant !\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith((\"out_proj.weight\", \"down_proj.weight\")):\n",
    "                # Residual projections: scale down to prevent variance explosion\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.num_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "       \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.embedding(tokens)\n",
    "        x = self.dropout(embeddings)\n",
    "        for b in self.blocks:\n",
    "            x, _ = b(x)\n",
    "        features = self.norm(x)  # normalized to stabilize training\n",
    "        return self.lm_head(features)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 prompt_tokens,\n",
    "                 max_new_tokens=50,\n",
    "                 temperature=1.0,\n",
    "                 top_k=0,\n",
    "                 top_p=0.0,\n",
    "                 use_cache=True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Auto-regressive text generation loop\n",
    "\n",
    "        prompt_tokens : tensor with input tokens, shape (1, n_tokens)\n",
    "        max_new_tokens : how many new tokens we want to generate\n",
    "        temperature : controls expressivity (lower = less, higher = more funky, degenerate cases : 0 = argmax, +inf = random guess)\n",
    "        top_k : restrict prediction to top_k tokens to avoid sampling low prob garbage, set to 0 to disable, top_k ∈ [0, vocab_size]\n",
    "        top_p : sample from smallest set with cumulative prob >= top_p (adapts to model confidence, usually top_k OR top_p), top_p ∈ [0, 1]\n",
    "        use_cache : set to True to avoid re-computing expensive K, V matrices\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.eval()\n",
    "\n",
    "        tokens_out = prompt_tokens.clone()\n",
    "        current_tokens = prompt_tokens.clone()\n",
    "        tokens_out = tokens_out.to(self.device)\n",
    "        current_tokens = current_tokens.to(self.device)\n",
    "        cache = [{} if use_cache else None for _ in range(len(self.blocks))]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            x = self.embedding(current_tokens)\n",
    "            for i, b in enumerate(self.blocks):\n",
    "                x, c_i = b(x, cache[i])\n",
    "                cache[i] = c_i\n",
    "            \n",
    "            features = self.norm(x)\n",
    "            logits = self.lm_head(features)    \n",
    "            last_logits = logits[:, -1, :]\n",
    "    \n",
    "            if temperature == 0:\n",
    "                # Greedy decoding if temp is 0\n",
    "                next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                # \"reshape the distribution\", i.e crushing logits before softmax ~ uniform distribution etc.\n",
    "                scaled_logits = last_logits / temperature\n",
    "                \n",
    "                # Only sample from top k tokens to avoid garbage prediction derailing whole prediction\n",
    "                if int(top_k) > 0:\n",
    "                    # most of probability mass in on a small amount of tokens, maybe 50 ?\n",
    "                    values, indices = torch.topk(scaled_logits, top_k)\n",
    "                    scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n",
    "                    scaled_logits.scatter_(1, indices, values)\n",
    "\n",
    "                # TODO : DISABLE top_k + top_p ? Modern implementation *usually* only expose top_p\n",
    "                if top_p > 0.0 and top_p < 1.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True, dim=-1)\n",
    "                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    # Remove tokens with cumulative probability above the threshold\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    # Shift right to keep at least one token (the first one)\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "                    # Set logits to -inf for tokens we want to remove\n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                    scaled_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # logits -> distribution probability\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                # Sample from prob distribution, nb : we don't simply take max prob token to allow \"creativity\"\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Stop generating if model thinks the \"document\" is finished\n",
    "            # eot_id = tokenizer.eot_token\n",
    "            if next_token.item() == eot_id:\n",
    "                break\n",
    "            \n",
    "            tokens_out = torch.cat([tokens_out, next_token], dim=1)\n",
    "\n",
    "            # If caching, we only need to feed the newest token next time, otherwise full sequence\n",
    "            current_tokens = next_token if use_cache else tokens_out\n",
    "       \n",
    "        return tokens_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed8616-4de7-4595-afcb-8b36c3dea3f9",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233fbc6d-4607-4bdc-bc51-2bd536a68ba2",
   "metadata": {},
   "source": [
    "Must be the exact same used for pretraining, on top just add 2 extra tokens for assistant / user roles and assign these the same embedding we learnt during pretraining as the end of text token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cbeb30-9756-4a6f-a1a4-a812ba79d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tiktoken.core import Encoding\n",
    "\n",
    "base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "special_tokens = {\n",
    "    \"<|endoftext|>\": base.eot_token,        # must be preserved\n",
    "    \"<|user|>\": base.n_vocab,\n",
    "    \"<|assistant|>\": base.n_vocab + 1,\n",
    "}\n",
    "\n",
    "tokenizer = Encoding(\n",
    "    name=\"gpt2-with-roles\",\n",
    "    pat_str=base._pat_str,\n",
    "    mergeable_ranks=base._mergeable_ranks,\n",
    "    special_tokens=special_tokens,\n",
    ")\n",
    "\n",
    "eot_id = tokenizer.eot_token\n",
    "user_id = tokenizer.encode_single_token(\"<|user|>\")\n",
    "assistant_id = tokenizer.encode_single_token(\"<|assistant|>\")\n",
    "\n",
    "print(user_id, assistant_id, eot_id)\n",
    "print(tokenizer.decode([user_id, assistant_id, eot_id]))\n",
    "\n",
    "vocab_size = round_up(tokenizer.n_vocab, 128)\n",
    "print(\"Vocab size:\", tokenizer.n_vocab, \"→ padded:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925259a-61f9-4bde-aa95-f59755dc5478",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning Dataset\n",
    "\n",
    "### Goal\n",
    "Train the model to generate assistant responses, NOT to predict instructions -> Autoregressive LMs predict the NEXT token at each position. We use this by masking instruction tokens in the loss calculation.\n",
    "\n",
    "---\n",
    "\n",
    "### The Process\n",
    "\n",
    "#### 1. Format the Data\n",
    "```\n",
    "Input text: \"<|user|>\\n{instruction}\\n<|assistant|>\\n{response}\"\n",
    "\n",
    "```\n",
    "\n",
    "#### 2. Tokenize\n",
    "```\n",
    "ids = [user_tok, What, is, 2, +, 2, ?, asst_tok, The, answer, is, 4, eot]\n",
    "idx:   0        1     2   3  4  5  6  7         8    9      10  11 12\n",
    "```\n",
    "\n",
    "#### 3. Create Shifted Labels\n",
    "```python\n",
    "labels[:-1] = ids[1:]  # Each label is the NEXT token to predict\n",
    "\n",
    "labels = [What, is, 2, +, 2, ?, asst_tok, The, answer, is, 4, eot, IGNORE]\n",
    "```\n",
    "**Meaning:** `labels[i]` = what should be predicted after seeing `ids[i]`\n",
    "\n",
    "#### 4. Mask the Instruction\n",
    "```python\n",
    "# Find position of <|assistant|> token (position 7 in example)\n",
    "labels[:assistant_pos+1] = IGNORE_INDEX (-100)\n",
    "\n",
    "labels = [IGN, IGN, IGN, IGN, IGN, IGN, IGN, The, answer, is, 4, eot, IGN]\n",
    "          └─────────instruction masked───────────┘  └────train here────┘\n",
    "```\n",
    "\n",
    "#### 5. Compute Loss (during training)\n",
    "```python\n",
    "logits = model(ids)  # Model predicts next token at each position\n",
    "loss = CrossEntropyLoss(logits, labels, ignore_index=-100)\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- Position 0-6: `labels[i] = -100` → loss ignored (don't train on instruction)\n",
    "- Position 7: predict \"The\" after `<|assistant|>` → **COMPUTE LOSS** ✓\n",
    "- Position 8: predict \"answer\" after \"The\" → **COMPUTE LOSS** ✓\n",
    "- Position 9: predict \"is\" after \"answer\" → **COMPUTE LOSS** ✓\n",
    "- Position 10: predict \"4\" after \"is\" → **COMPUTE LOSS** ✓\n",
    "- Position 11: predict `<eot>` after \"4\" → **COMPUTE LOSS** ✓\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "**Causal Attention Mask**\n",
    "- Prevents the model from \"seeing\" future tokens\n",
    "- At position i, model only attends to tokens 0 to i\n",
    "\n",
    "**Teacher Forcing**\n",
    "- Model sees correct previous tokens during training\n",
    "- Learns to predict the next one\n",
    "\n",
    "**Masking with -100**\n",
    "- `CrossEntropyLoss` ignores these positions\n",
    "- Gradients only flow through response tokens\n",
    "\n",
    "**Result:** Model learns \"given instruction X, generate response Y\" without wasting compute trying to predict the instruction itself.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Facts\n",
    "\n",
    "- `IGNORE_INDEX = -100` (standard PyTorch convention)\n",
    "- Only ~5-20% of tokens typically contribute to loss (just the responses)\n",
    "- The shift (`labels[i] = ids[i+1]`) aligns predictions with targets\n",
    "- The masking + smaller size dataset is going to finetune behavior but not (or barely) knowledge !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841eb3d5-48d3-4403-aa9f-20331683a61d",
   "metadata": {},
   "outputs": [],
   "source": "# Load multiple datasets\nprint(\"Loading datasets...\")\nalpaca_cleaned = load_dataset(\"yahma/alpaca-cleaned\")\nplatypus = load_dataset(\"garage-bAInd/Open-Platypus\")\nno_robots = load_dataset(\"HuggingFaceH4/no_robots\")\n\n# Format functions for each dataset\ndef format_alpaca(example):\n    if example[\"input\"].strip():\n        user_text = (\n            f\"{example['instruction']}\\n\\n\"\n            f\"{example['input']}\"\n        )\n    else:\n        user_text = example[\"instruction\"]\n    return {\n        \"user\": user_text,\n        \"assistant\": example[\"output\"]\n    }\n\ndef format_platypus(example):\n    return {\n        \"user\": example[\"instruction\"],\n        \"assistant\": example[\"output\"]\n    }\n\ndef format_no_robots(example):\n    return {\n        \"user\": example[\"prompt\"],\n        \"assistant\": example[\"messages\"][1][\"content\"]\n    }\n\n# Map each dataset to common format\nprint(\"Formatting datasets...\")\nalpaca_formatted = alpaca_cleaned.map(\n    format_alpaca, \n    remove_columns=alpaca_cleaned[\"train\"].column_names\n)\nplatypus_formatted = platypus.map(\n    format_platypus, \n    remove_columns=platypus[\"train\"].column_names\n)\nno_robots_formatted = no_robots.map(\n    format_no_robots, \n    remove_columns=no_robots[\"train\"].column_names\n)\n\ncombined_datasets = [\n    alpaca_formatted[\"train\"],\n    platypus_formatted[\"train\"],\n    no_robots_formatted[\"train\"]\n]\n\n# OPTIONAL : can use any modern LLM to generate custom SFT data\nif os.path.isfile(\"synthetic_sft_data.jsonl\"):\n    print(\"Loading synthetic data...\")\n    synthetic_data = load_synthetic_data(\"synthetic_sft_data.jsonl\")\n    print(f\"Loaded {len(synthetic_data)} synthetic examples\")\n    \n    # Transform list of dicts to dict of lists\n    data_dict = {}\n    for key in synthetic_data[0].keys():\n        data_dict[key] = [item[key] for item in synthetic_data]\n    \n    synthetic_dataset = ds.from_dict(data_dict)\n    \n    # Print example to verify format\n    print(\"\\nExample from synthetic dataset:\")\n    pprint(synthetic_dataset[0])\n\n    # Add to all datasets\n    combined_datasets.append(synthetic_dataset)\n\n# Combine all datasets\nprint(\"\\n\\nCombining datasets...\")\ncombined_train = concatenate_datasets(combined_datasets)\n\nprint(f\"Total training examples: {len(combined_train)}\")\nprint(\"\\nExample from combined dataset:\")\npprint(next(iter(combined_train)))\n\ndef tokenize_sft(example):\n    text = (\n        \"<|user|>\\n\"\n        f\"{example['user']}\\n\"\n        \"<|assistant|>\\n\"\n        f\"{example['assistant']}\"\n    )\n\n    ids = tokenizer.encode(text, allowed_special=set(special_tokens.keys()))\n    ids.append(eot_id)\n\n    # Find assistant token\n    try:\n        assistant_pos = ids.index(assistant_id)\n    except ValueError:\n        # Return empty tensors that will be filtered out\n        return {\"input_ids\": [], \"labels\": []}\n\n    # Create labels as shifted version of ids\n    labels = [IGNORE_INDEX] * len(ids)\n    labels[:-1] = ids[1:]\n\n    # Mask out everything before assistant response\n    labels[:assistant_pos + 1] = [IGNORE_INDEX] * (assistant_pos + 1)\n\n    # Truncate\n    ids = ids[:block_size]\n    labels = labels[:block_size]\n\n    return {\n        \"input_ids\": ids,\n        \"labels\": labels,\n    }\n\n# Tokenize combined dataset\nprint(\"Tokenizing combined dataset...\")\ncombined_tokenized = combined_train.map(\n    tokenize_sft,\n    remove_columns=combined_train.column_names,\n    num_proc=4,\n)\n\n# Filter out empty examples (failed tokenization)\npre_filter_len = len(combined_tokenized)\ncombined_tokenized = combined_tokenized.filter(lambda x: len(x[\"input_ids\"]) > 0)\nprint(f\"Filtered {pre_filter_len - len(combined_tokenized)} invalid examples\")\nprint(f\"Final dataset size: {len(combined_tokenized)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2dc3f0-4902-4404-b046-416806c20f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Inputs from user / assistant conversations are of variable length -> pad for training\n",
    "    # To squeeze out performance, can assign inputs to buckets or pad with fixed len -> compile model\n",
    "    # Here the data is reasonably sized so we can skip\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    \n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    \n",
    "    for x in batch:\n",
    "        pad_len = max_len - len(x[\"input_ids\"])\n",
    "        input_ids.append(\n",
    "            x[\"input_ids\"] + [eot_id] * pad_len\n",
    "        )\n",
    "        labels.append(\n",
    "            x[\"labels\"] + [IGNORE_INDEX] * pad_len\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    combined_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=prefetch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with {len(train_loader)} batches of {batch_size} seqs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237d148-f21d-45e7-81dc-7b7d5543a14d",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Start from pretrained model, set `dropout` to 0, extend embedding table to the new role tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae4514-68d7-466c-b69e-3480dc0b14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"vocab_size\": vocab_size,  # This should already be 50304 from both pretraining and SFT\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"mlp_ratio\": mlp_ratio,\n",
    "    \"dropout_prob\": dropout_prob,\n",
    "}\n",
    "\n",
    "print(\"Initializing model with config:\")\n",
    "pprint(model_config)\n",
    "model = GPT(**model_config).to(device)\n",
    "\n",
    "print(f\"Loading: {pretrained_weights}...\")\n",
    "trained_weights = strip_compile_prefix(torch.load(pretrained_weights, map_location=device))\n",
    "model.load_state_dict(trained_weights, strict=True)\n",
    "print(\"Loaded pretrained weights\")\n",
    "\n",
    "# Initialize the new special token embeddings (they exist but were untrained padding)\n",
    "# Indices 50257 (<|user|>) and 50258 (<|assistant|>) are within the padded vocab\n",
    "with torch.no_grad():\n",
    "    # Option 1: Copy from <|endoftext|> token\n",
    "    model.embedding.weight[user_id] = model.embedding.weight[eot_id].clone()\n",
    "    model.embedding.weight[assistant_id] = model.embedding.weight[eot_id].clone()\n",
    "    \n",
    "    # Option 2 (alternative): Use mean of all trained embeddings\n",
    "    # mean_emb = model.embedding.weight[:base.n_vocab].mean(dim=0)\n",
    "    # model.embedding.weight[user_id] = mean_emb\n",
    "    # model.embedding.weight[assistant_id] = mean_emb\n",
    "\n",
    "# If using weight tying, lm_head already shares weights, no action needed\n",
    "# If NOT using weight tying, you'd need to update lm_head separately\n",
    "\n",
    "print(f\"Initialized special tokens: <|user|>={user_id}, <|assistant|>={assistant_id}\")\n",
    "\n",
    "# Ready to train!\n",
    "model.train()\n",
    "_, _ = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ff922-468a-49b1-a8c7-26230af36f68",
   "metadata": {},
   "source": [
    "Weight decay is quite huge compared to habitual CNN but seems to be the standard for LLMs (empirical evidence), usually 0.1 for pretraining, 0.01 for SFT, helps avoiding memorization etc. Small lr because we're finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e581bf-67ec-4db3-9bcd-a1516f736f0f",
   "metadata": {},
   "outputs": [],
   "source": "# Separate parameters into decay and no-decay groups (same logic as pretraining)\ndecay_params = []\nno_decay_params = []\n\nfor name, param in model.named_parameters():\n    if not param.requires_grad:\n        continue\n    # Don't apply weight decay to:\n    # - Norm parameters (RMSNorm weights)\n    # - Embedding table (tied with lm_head)\n    # - Any bias terms\n    if any(kw in name.lower() for kw in ['norm', 'bias', 'embed', 'embedding', 'lm_head']):\n        no_decay_params.append(param)\n    else:\n        decay_params.append(param)\n\nprint(f\"Decay params: {len(decay_params)}, No decay params: {len(no_decay_params)}\")\n\noptimizer = torch.optim.AdamW([\n    {'params': decay_params, 'weight_decay': 0.01},\n    {'params': no_decay_params, 'weight_decay': 0.0}\n],\n    lr=2e-5,\n    betas=(0.9, 0.95),\n)\n\n# Learning rate scheduler: linear warmup + cosine decay\ntotal_steps = NUM_EPOCHS * len(train_loader)\nwarmup_steps = int(total_steps * 0.05)  # 5% warmup\n\nfrom torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n\nwarmup_scheduler = LinearLR(\n    optimizer,\n    start_factor=0.1,  # start at 0.1 * lr\n    total_iters=warmup_steps\n)\ncosine_scheduler = CosineAnnealingLR(\n    optimizer,\n    T_max=total_steps - warmup_steps,\n    eta_min=1e-6  # min lr\n)\nscheduler = SequentialLR(\n    optimizer,\n    schedulers=[warmup_scheduler, cosine_scheduler],\n    milestones=[warmup_steps]\n)\n\nprint(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\")"
  },
  {
   "cell_type": "markdown",
   "id": "ade3a017-b03e-4e16-9e46-169a2e6fc1d4",
   "metadata": {},
   "source": [
    "We train over `epochs` because SFT data is 1) manageable 2) fixed Q/A samples while pretraining data is intractable and random contiguous chunk of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [],
   "source": "import time\n\nglobal_step = 0\ntotal_batches = len(train_loader)\nlog_interval = 100\n\nprint(f\"Starting SFT training: {NUM_EPOCHS} epochs, {total_batches} batches/epoch, {total_batches * NUM_EPOCHS} total steps\")\nprint(\"-\" * 80)\n\ntraining_start = time.time()\n\nfor epoch in range(NUM_EPOCHS):\n    epoch_start = time.time()\n    epoch_loss_sum = 0.0\n    epoch_loss_count = 0\n\n    for step, batch in enumerate(train_loader):\n        input_ids = batch[\"input_ids\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        with torch.amp.autocast('cuda', dtype=dtype):\n            logits = model(input_ids)\n            loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                labels.view(-1),\n                ignore_index=IGNORE_INDEX,\n            )\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        global_step += 1\n\n        # Track epoch loss\n        epoch_loss_sum += loss.item()\n        epoch_loss_count += 1\n\n        if step % log_interval == 0:\n            current_lr = optimizer.param_groups[0][\"lr\"]\n            pct_complete = 100 * (step + 1) / total_batches\n            elapsed = time.time() - epoch_start\n            \n            # Estimate time remaining for epoch\n            if step > 0:\n                steps_per_sec = step / elapsed\n                remaining_steps = total_batches - step\n                eta_sec = remaining_steps / steps_per_sec\n                eta_str = f\"{int(eta_sec // 60):02d}:{int(eta_sec % 60):02d}\"\n            else:\n                eta_str = \"--:--\"\n            \n            avg_loss = epoch_loss_sum / epoch_loss_count\n            \n            print(\n                f\"epoch {epoch+1}/{NUM_EPOCHS} | \"\n                f\"step {step:>5}/{total_batches} ({pct_complete:5.1f}%) | \"\n                f\"loss {loss.item():.4f} (avg {avg_loss:.4f}) | \"\n                f\"lr {current_lr:.2e} | \"\n                f\"ETA {eta_str}\"\n            )\n\n    # End of epoch summary\n    epoch_elapsed = time.time() - epoch_start\n    epoch_avg_loss = epoch_loss_sum / epoch_loss_count\n    print(\"-\" * 80)\n    print(\n        f\"Epoch {epoch+1} complete | \"\n        f\"avg loss: {epoch_avg_loss:.4f} | \"\n        f\"time: {int(epoch_elapsed // 60):02d}:{int(epoch_elapsed % 60):02d}\"\n    )\n    print(\"-\" * 80)\n\n    # Save model in full precision\n    torch.save(model.state_dict(), f\"GPT_SFT_epoch_{epoch}.pt\")\n    print(f\"Saved checkpoint: GPT_SFT_epoch_{epoch}.pt\")\n\n# Final summary\ntotal_time = time.time() - training_start\nprint(\"=\" * 80)\nprint(f\"Training complete! Total time: {int(total_time // 3600):02d}:{int((total_time % 3600) // 60):02d}:{int(total_time % 60):02d}\")\nprint(f\"Final checkpoint: GPT_SFT_epoch_{NUM_EPOCHS - 1}.pt\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67998ad-5c05-424b-9358-fc72fe99d35b",
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef generate(\n    model,\n    prompt_text,\n    max_new_tokens=150,\n    temperature=0.7,\n    top_p=0.9,\n    stop_token_id=eot_id,\n):\n    \"\"\"Generate response from a prompt string.\"\"\"\n    model.eval()\n    \n    enc = tokenizer.encode(prompt_text, allowed_special=set(special_tokens.keys()))\n    input_ids = torch.tensor(enc, dtype=torch.long).unsqueeze(0).to(device)\n    \n    for _ in range(max_new_tokens):\n        # Crop to block_size if needed\n        input_ids_cond = input_ids if input_ids.shape[1] <= block_size else input_ids[:, -block_size:]\n        \n        logits = model(input_ids_cond)\n        next_token_logits = logits[:, -1, :]\n        \n        # Apply temperature\n        next_token_logits = next_token_logits / temperature\n        \n        # Top-p sampling\n        if top_p < 1.0:\n            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n            sorted_indices_to_remove = cumulative_probs > top_p\n            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n            sorted_indices_to_remove[..., 0] = 0\n            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n            next_token_logits[indices_to_remove] = float('-inf')\n        \n        probs = F.softmax(next_token_logits, dim=-1)\n        next_token = torch.multinomial(probs, num_samples=1)\n        \n        input_ids = torch.cat([input_ids, next_token], dim=1)\n        \n        if stop_token_id is not None and next_token.item() == stop_token_id:\n            break\n    \n    return tokenizer.decode(input_ids[0].tolist())\n\n\ndef format_prompt(user_message):\n    \"\"\"Format a user message into the chat template.\"\"\"\n    return f\"<|user|>\\n{user_message}\\n<|assistant|>\\n\"\n\n\n# =============================================================================\n# Quick Evaluation Suite\n# =============================================================================\nprint(\"=\" * 80)\nprint(\"POST-TRAINING EVALUATION\")\nprint(\"=\" * 80)\n\neval_prompts = [\n    # Basic instruction following\n    (\"Simple instruction\", \"Write a short greeting message.\"),\n    \n    # Knowledge recall (don't expect accuracy, just coherence)\n    (\"Knowledge\", \"What is the capital of France?\"),\n    \n    # Reasoning (basic)\n    (\"Basic reasoning\", \"If I have 3 apples and buy 2 more, how many do I have?\"),\n    \n    # Creative\n    (\"Creative\", \"Write a haiku about programming.\"),\n    \n    # Explanation\n    (\"Explanation\", \"Explain what a neural network is in simple terms.\"),\n    \n    # Code (if trained on code data)\n    (\"Code\", \"Write a Python function that adds two numbers.\"),\n    \n    # Multi-step\n    (\"Multi-step\", \"List 3 benefits of exercise.\"),\n    \n    # Refusal/boundary (interesting to see behavior)\n    (\"Edge case\", \"Summarize the following text: \"),\n]\n\nmodel.eval()\nfor category, user_msg in eval_prompts:\n    prompt = format_prompt(user_msg)\n    \n    print(f\"\\n[{category}]\")\n    print(f\"User: {user_msg}\")\n    print(\"-\" * 40)\n    \n    try:\n        response = generate(model, prompt, max_new_tokens=150, temperature=0.7)\n        # Extract just the assistant response\n        if \"<|assistant|>\" in response:\n            assistant_part = response.split(\"<|assistant|>\\n\")[-1]\n            # Clean up any trailing special tokens\n            assistant_part = assistant_part.replace(\"<|endoftext|>\", \"\").strip()\n        else:\n            assistant_part = response\n        print(f\"Assistant: {assistant_part}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n    \n    print(\"-\" * 40)\n\n# =============================================================================\n# Quantitative checks\n# =============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FORMAT COMPLIANCE CHECK\")\nprint(\"=\" * 80)\n\n# Check if model properly terminates with EOT\ntest_prompts = [format_prompt(p) for _, p in eval_prompts[:3]]\neot_count = 0\ntotal_length = 0\n\nfor prompt in test_prompts:\n    enc = tokenizer.encode(prompt, allowed_special=set(special_tokens.keys()))\n    input_ids = torch.tensor(enc, dtype=torch.long).unsqueeze(0).to(device)\n    \n    # Generate with greedy decoding for consistency\n    model.eval()\n    for _ in range(200):\n        logits = model(input_ids[:, -block_size:] if input_ids.shape[1] > block_size else input_ids)\n        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n        input_ids = torch.cat([input_ids, next_token], dim=1)\n        if next_token.item() == eot_id:\n            eot_count += 1\n            break\n    \n    total_length += input_ids.shape[1] - len(enc)\n\navg_response_len = total_length / len(test_prompts)\neot_rate = 100 * eot_count / len(test_prompts)\n\nprint(f\"EOT termination rate: {eot_count}/{len(test_prompts)} ({eot_rate:.0f}%)\")\nprint(f\"Avg response length: {avg_response_len:.0f} tokens\")\nprint(f\"  → {'Good: Model learns to stop' if eot_rate > 50 else 'Warning: Model may ramble'}\")\nprint(f\"  → {'Good: Reasonable length' if 20 < avg_response_len < 150 else 'Check: Unusual response length'}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Evaluation complete!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628d8da-e6fc-4e40-bf7c-5e3a20e22bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}