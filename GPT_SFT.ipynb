{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "SFT part : teach the model the assistant format\n",
    "\n",
    "Basically : \n",
    "- the prompt tokens are masked (no loss)\n",
    "- the response tokens are trained on (loss applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import csv\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment config\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "from datasets import Dataset as ds, concatenate_datasets, load_dataset\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Torch runtime config\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Custom\n",
    "from utils import count_parameters, load_synthetic_data, strip_compile_prefix, round_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7494176-e827-4d1a-b659-a2c313af239c",
   "metadata": {},
   "source": [
    "## Config & Model Definition\n",
    "\n",
    "Mostly the same code as the pretraining notebook, including Flash Attention, RMSNorm etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa17962-52b5-4773-868c-5c87bae6d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_bf16_supported() True\n"
     ]
    }
   ],
   "source": [
    "#### CONFIG #####\n",
    "\n",
    "# Basically GPT-2 Small\n",
    "block_size = 1024\n",
    "batch_size = 16\n",
    "embed_dim = 768\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "dropout_prob = 0  # <!> finetuning, standard practice to disable dropout <!>\n",
    "mlp_ratio = 4  # standard 4x expansion\n",
    "pretrained_weights = \"gpt_full_run.pt\"\n",
    "\n",
    "# Tokenizer\n",
    "ROLE_TOKENS = [\"<|user|>\", \"<|assistant|>\"]\n",
    "IGNORE_INDEX = -100  # to mask out the loss\n",
    "\n",
    "# Training\n",
    "NUM_EPOCHS = 3  # not too many or we're going to overfit our Q/A data\n",
    "num_workers = 4\n",
    "prefetch = 4\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "print(\"torch.cuda.is_bf16_supported()\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 rotary_emb: RotaryEmbedding,\n",
    "                 causal: bool = True,\n",
    "                 dropout: float = 0.1\n",
    "                ):\n",
    "        super().__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "        \n",
    "        self.causal = causal\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout_p = dropout\n",
    "        \n",
    "        # Fused QKV projection: 3x the output size\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        \n",
    "        # Shared rotary embedding (passed from GPT model)\n",
    "        self.rotary_emb = rotary_emb\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, k_v_cache=None):\n",
    "        B, T, _ = x.shape\n",
    "        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n",
    "    \n",
    "        # 1. Single fused projection\n",
    "        if using_cache:\n",
    "            x_q = x[:, -1:, :]\n",
    "            qkv = self.qkv_proj(x_q)  # (B, 1, 3 x embed_dim)\n",
    "        else:\n",
    "            qkv = self.qkv_proj(x)  # (B, T, 3 x embed_dim)\n",
    "        \n",
    "        # 2. Split into Q, K, V\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)  # Each is (B, T, embed_dim)\n",
    "        \n",
    "        def split_heads(t):\n",
    "            return t.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 3. Split heads -> (B, H, T, D_head)\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "    \n",
    "        # 4. Apply RoPE \n",
    "        if using_cache:\n",
    "            past_len = k_v_cache[\"K\"].shape[-2]\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q, offset=past_len)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K, offset=past_len)\n",
    "            \n",
    "            K = torch.cat([k_v_cache[\"K\"], K], dim=-2)\n",
    "            V = torch.cat([k_v_cache[\"V\"], V], dim=-2)\n",
    "            # When using the cache, the \"causality\" is already enforced by the fact that we are passing 1 query token against all valid past keys \n",
    "            # We don't need a mask as we want the current token to attend to everything in the history\n",
    "            is_causal_step = False\n",
    "        else:\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K)\n",
    "            is_causal_step = self.causal\n",
    "    \n",
    "        # 5. Update cache\n",
    "        if k_v_cache is not None:\n",
    "            k_v_cache[\"K\"] = K.detach()  # we will never .backward on these\n",
    "            k_v_cache[\"V\"] = V.detach()  \n",
    "    \n",
    "        # 6. Attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            query=Q,\n",
    "            key=K,\n",
    "            value=V,\n",
    "            attn_mask=None, \n",
    "            dropout_p=self.dropout_p if self.training else 0.0,\n",
    "            is_causal=is_causal_step\n",
    "        )\n",
    "        \n",
    "        # 7. Merge heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, -1, self.embed_dim)\n",
    "\n",
    "        # 8. Linear projection\n",
    "        return self.out_proj(out), k_v_cache\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim=None, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * embed_dim\n",
    "\n",
    "        hidden_dim = round_up(2 * hidden_dim // 3, 8)\n",
    "\n",
    "        # Fused projection\n",
    "        self.gate_up_proj = nn.Linear(embed_dim, 2 * hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        gate_up = self.gate_up_proj(x)\n",
    "        gate, up = gate_up.chunk(2, dim=-1)\n",
    "        return self.dropout(self.down_proj(F.silu(gate) * up))\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 rotary_emb,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 causal=True,\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Initialize a complete transformer block.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Multi-head self-attention for sequence modeling\n",
    "        2. 1st Normalization (pre-norm architecture)\n",
    "        3. MLP with specified expansion ratio\n",
    "        4. 2nd Normalization\n",
    "    \n",
    "        TRANSFORMER BLOCK ARCHITECTURE:\n",
    "        x → Norm → MultiHeadAttention → + (residual) →\n",
    "            Norm → MLP → + (residual) → output\n",
    "    \n",
    "        NB: We use pre-norm architecture (before attention/MLP)\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "        self.norm1 = nn.RMSNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, rotary_emb, causal, dropout_prob)  # causal = masking out tokens\n",
    "        self.norm2 = nn.RMSNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio * embed_dim, dropout_prob)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        x1 = self.norm1(x)\n",
    "        x2, cache = self.mha(x1, cache)  # will be used when generating tokens during inference\n",
    "        x2 = x2 + x  # residual path\n",
    "    \n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.mlp(x3) + x2  # residual path\n",
    "        return x3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT (Generative Pre-trained Transformer) model.\n",
    "\n",
    "    This combines embeddings, positional encoding, multiple transformer blocks,\n",
    "    and a language modeling head for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 is_causal=True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize complete GPT model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Shared rotary embedding across all layers (more efficient for compilation)\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.rotary_emb = RotaryEmbedding(dim=head_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, self.rotary_emb, mlp_ratio, dropout_prob, is_causal) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.RMSNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # weight tying\n",
    "\n",
    "        # below shamefully stolen from nano-gpt\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        # don't forget swiglu variant !\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith((\"out_proj.weight\", \"down_proj.weight\")):\n",
    "                # Residual projections: scale down to prevent variance explosion\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.num_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "       \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.embedding(tokens)\n",
    "        x = self.dropout(embeddings)\n",
    "        for b in self.blocks:\n",
    "            x, _ = b(x)\n",
    "        features = self.norm(x)  # normalized to stabilize training\n",
    "        return self.lm_head(features)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 prompt_tokens,\n",
    "                 max_new_tokens=50,\n",
    "                 temperature=1.0,\n",
    "                 top_k=0,\n",
    "                 top_p=0.0,\n",
    "                 use_cache=True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Auto-regressive text generation loop\n",
    "\n",
    "        prompt_tokens : tensor with input tokens, shape (1, n_tokens)\n",
    "        max_new_tokens : how many new tokens we want to generate\n",
    "        temperature : controls expressivity (lower = less, higher = more funky, degenerate cases : 0 = argmax, +inf = random guess)\n",
    "        top_k : restrict prediction to top_k tokens to avoid sampling low prob garbage, set to 0 to disable, top_k ∈ [0, vocab_size]\n",
    "        top_p : sample from smallest set with cumulative prob >= top_p (adapts to model confidence, usually top_k OR top_p), top_p ∈ [0, 1]\n",
    "        use_cache : set to True to avoid re-computing expensive K, V matrices\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.eval()\n",
    "\n",
    "        tokens_out = prompt_tokens.clone()\n",
    "        current_tokens = prompt_tokens.clone()\n",
    "        tokens_out = tokens_out.to(self.device)\n",
    "        current_tokens = current_tokens.to(self.device)\n",
    "        cache = [{} if use_cache else None for _ in range(len(self.blocks))]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            x = self.embedding(current_tokens)\n",
    "            for i, b in enumerate(self.blocks):\n",
    "                x, c_i = b(x, cache[i])\n",
    "                cache[i] = c_i\n",
    "            \n",
    "            features = self.norm(x)\n",
    "            logits = self.lm_head(features)    \n",
    "            last_logits = logits[:, -1, :]\n",
    "    \n",
    "            if temperature == 0:\n",
    "                # Greedy decoding if temp is 0\n",
    "                next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                # \"reshape the distribution\", i.e crushing logits before softmax ~ uniform distribution etc.\n",
    "                scaled_logits = last_logits / temperature\n",
    "                \n",
    "                # Only sample from top k tokens to avoid garbage prediction derailing whole prediction\n",
    "                if int(top_k) > 0:\n",
    "                    # most of probability mass in on a small amount of tokens, maybe 50 ?\n",
    "                    values, indices = torch.topk(scaled_logits, top_k)\n",
    "                    scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n",
    "                    scaled_logits.scatter_(1, indices, values)\n",
    "\n",
    "                # TODO : DISABLE top_k + top_p ? Modern implementation *usually* only expose top_p\n",
    "                if top_p > 0.0 and top_p < 1.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True, dim=-1)\n",
    "                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    # Remove tokens with cumulative probability above the threshold\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    # Shift right to keep at least one token (the first one)\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "                    # Set logits to -inf for tokens we want to remove\n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                    scaled_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # logits -> distribution probability\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                # Sample from prob distribution, nb : we don't simply take max prob token to allow \"creativity\"\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Stop generating if model thinks the \"document\" is finished\n",
    "            # eot_id = tokenizer.eot_token\n",
    "            if next_token.item() == eot_id:\n",
    "                break\n",
    "            \n",
    "            tokens_out = torch.cat([tokens_out, next_token], dim=1)\n",
    "\n",
    "            # If caching, we only need to feed the newest token next time, otherwise full sequence\n",
    "            current_tokens = next_token if use_cache else tokens_out\n",
    "       \n",
    "        return tokens_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed8616-4de7-4595-afcb-8b36c3dea3f9",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233fbc6d-4607-4bdc-bc51-2bd536a68ba2",
   "metadata": {},
   "source": [
    "Must be the exact same used for pretraining, on top just add 2 extra tokens for assistant / user roles and assign these the same embedding we learnt during pretraining as the end of text token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cbeb30-9756-4a6f-a1a4-a812ba79d320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257 50258 50256\n",
      "<|user|><|assistant|><|endoftext|>\n",
      "Vocab size: 50259 → padded: 50304\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from tiktoken.core import Encoding\n",
    "\n",
    "base = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "special_tokens = {\n",
    "    \"<|endoftext|>\": base.eot_token,        # must be preserved\n",
    "    \"<|user|>\": base.n_vocab,\n",
    "    \"<|assistant|>\": base.n_vocab + 1,\n",
    "}\n",
    "\n",
    "tokenizer = Encoding(\n",
    "    name=\"gpt2-with-roles\",\n",
    "    pat_str=base._pat_str,\n",
    "    mergeable_ranks=base._mergeable_ranks,\n",
    "    special_tokens=special_tokens,\n",
    ")\n",
    "\n",
    "eot_id = tokenizer.eot_token\n",
    "user_id = tokenizer.encode_single_token(\"<|user|>\")\n",
    "assistant_id = tokenizer.encode_single_token(\"<|assistant|>\")\n",
    "\n",
    "print(user_id, assistant_id, eot_id)\n",
    "print(tokenizer.decode([user_id, assistant_id, eot_id]))\n",
    "\n",
    "vocab_size = round_up(tokenizer.n_vocab, 128)\n",
    "print(\"Vocab size:\", tokenizer.n_vocab, \"→ padded:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925259a-61f9-4bde-aa95-f59755dc5478",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning Dataset\n",
    "\n",
    "### Goal\n",
    "Train the model to generate assistant responses, NOT to predict instructions -> Autoregressive LMs predict the NEXT token at each position. We use this by masking instruction tokens in the loss calculation.\n",
    "\n",
    "---\n",
    "\n",
    "### The Process\n",
    "\n",
    "#### 1. Format the Data\n",
    "```\n",
    "Input text: \"<|user|>\\n{instruction}\\n<|assistant|>\\n{response}\"\n",
    "\n",
    "```\n",
    "\n",
    "#### 2. Tokenize\n",
    "```\n",
    "ids = [user_tok, What, is, 2, +, 2, ?, asst_tok, The, answer, is, 4, eot]\n",
    "idx:   0        1     2   3  4  5  6  7         8    9      10  11 12\n",
    "```\n",
    "\n",
    "#### 3. Create Shifted Labels\n",
    "```python\n",
    "labels[:-1] = ids[1:]  # Each label is the NEXT token to predict\n",
    "\n",
    "labels = [What, is, 2, +, 2, ?, asst_tok, The, answer, is, 4, eot, IGNORE]\n",
    "```\n",
    "**Meaning:** `labels[i]` = what should be predicted after seeing `ids[i]`\n",
    "\n",
    "#### 4. Mask the Instruction\n",
    "```python\n",
    "# Find position of <|assistant|> token (position 7 in example)\n",
    "labels[:assistant_pos+1] = IGNORE_INDEX (-100)\n",
    "\n",
    "labels = [IGN, IGN, IGN, IGN, IGN, IGN, IGN, The, answer, is, 4, eot, IGN]\n",
    "          └─────────instruction masked───────────┘  └────train here────┘\n",
    "```\n",
    "\n",
    "#### 5. Compute Loss (during training)\n",
    "```python\n",
    "logits = model(ids)  # Model predicts next token at each position\n",
    "loss = CrossEntropyLoss(logits, labels, ignore_index=-100)\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- Position 0-6: `labels[i] = -100` → loss ignored (don't train on instruction)\n",
    "- Position 7: predict \"The\" after `<|assistant|>` → **COMPUTE LOSS** ✓\n",
    "- Position 8: predict \"answer\" after \"The\" → **COMPUTE LOSS** ✓\n",
    "- Position 9: predict \"is\" after \"answer\" → **COMPUTE LOSS** ✓\n",
    "- Position 10: predict \"4\" after \"is\" → **COMPUTE LOSS** ✓\n",
    "- Position 11: predict `<eot>` after \"4\" → **COMPUTE LOSS** ✓\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "**Causal Attention Mask**\n",
    "- Prevents the model from \"seeing\" future tokens\n",
    "- At position i, model only attends to tokens 0 to i\n",
    "\n",
    "**Teacher Forcing**\n",
    "- Model sees correct previous tokens during training\n",
    "- Learns to predict the next one\n",
    "\n",
    "**Masking with -100**\n",
    "- `CrossEntropyLoss` ignores these positions\n",
    "- Gradients only flow through response tokens\n",
    "\n",
    "**Result:** Model learns \"given instruction X, generate response Y\" without wasting compute trying to predict the instruction itself.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Facts\n",
    "\n",
    "- `IGNORE_INDEX = -100` (standard PyTorch convention)\n",
    "- Only ~5-20% of tokens typically contribute to loss (just the responses)\n",
    "- The shift (`labels[i] = ids[i+1]`) aligns predictions with targets\n",
    "- The masking + smaller size dataset is going to finetune behavior but not (or barely) knowledge !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "841eb3d5-48d3-4403-aa9f-20331683a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Formatting datasets...\n",
      "Loading synthetic data...\n",
      "Loaded 21 synthetic examples\n",
      "\n",
      "\n",
      "Combining datasets...\n",
      "Total training examples: 286242\n",
      "\n",
      "Example from combined dataset:\n",
      "{'assistant': '1. Eat a balanced and nutritious diet: Make sure your meals are '\n",
      "              'inclusive of a variety of fruits and vegetables, lean protein, '\n",
      "              'whole grains, and healthy fats. This helps to provide your body '\n",
      "              'with the essential nutrients to function at its best and can '\n",
      "              'help prevent chronic diseases.\\n'\n",
      "              '\\n'\n",
      "              '2. Engage in regular physical activity: Exercise is crucial for '\n",
      "              'maintaining strong bones, muscles, and cardiovascular health. '\n",
      "              'Aim for at least 150 minutes of moderate aerobic exercise or 75 '\n",
      "              'minutes of vigorous exercise each week.\\n'\n",
      "              '\\n'\n",
      "              '3. Get enough sleep: Getting enough quality sleep is crucial '\n",
      "              'for physical and mental well-being. It helps to regulate mood, '\n",
      "              'improve cognitive function, and supports healthy growth and '\n",
      "              'immune function. Aim for 7-9 hours of sleep each night.',\n",
      " 'user': 'Give three tips for staying healthy.'}\n",
      "Tokenizing combined dataset...\n",
      "Filtered 0 invalid examples\n",
      "Final dataset size: 286242\n"
     ]
    }
   ],
   "source": [
    "# Load multiple datasets\n",
    "print(\"Loading datasets...\")\n",
    "alpaca_cleaned = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "platypus = load_dataset(\"garage-bAInd/Open-Platypus\")\n",
    "no_robots = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    "maths_qa = load_dataset(\"microsoft/orca-math-word-problems-200k\") \n",
    "    \n",
    "   \n",
    "# Format functions for each dataset\n",
    "def format_alpaca(example):\n",
    "    if example[\"input\"].strip():\n",
    "        user_text = (\n",
    "            f\"{example['instruction']}\\n\\n\"\n",
    "            f\"{example['input']}\"\n",
    "        )\n",
    "    else:\n",
    "        user_text = example[\"instruction\"]\n",
    "    return {\n",
    "        \"user\": user_text,\n",
    "        \"assistant\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "def format_platypus(example):\n",
    "    return {\n",
    "        \"user\": example[\"instruction\"],\n",
    "        \"assistant\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "def format_no_robots(example):\n",
    "    return {\n",
    "        \"user\": example[\"prompt\"],\n",
    "        \"assistant\": example[\"messages\"][1][\"content\"]\n",
    "    }\n",
    "\n",
    "def format_maths_qa(example): \n",
    "    return { \n",
    "        \"user\": example[\"question\"], \n",
    "        \"assistant\": example[\"answer\"],\n",
    "    } \n",
    "\n",
    "# Map each dataset to common format\n",
    "print(\"Formatting datasets...\")\n",
    "alpaca_formatted = alpaca_cleaned.map(\n",
    "    format_alpaca, \n",
    "    remove_columns=alpaca_cleaned[\"train\"].column_names\n",
    ")\n",
    "platypus_formatted = platypus.map(\n",
    "    format_platypus, \n",
    "    remove_columns=platypus[\"train\"].column_names\n",
    ")\n",
    "no_robots_formatted = no_robots.map(\n",
    "    format_no_robots, \n",
    "    remove_columns=no_robots[\"train\"].column_names\n",
    ")\n",
    "maths_qa_formatted = maths_qa.map(\n",
    "    format_maths_qa,\n",
    "    remove_columns=maths_qa['train'].column_names\n",
    ")\n",
    "\n",
    "combined_datasets = [\n",
    "    alpaca_formatted[\"train\"],\n",
    "    platypus_formatted[\"train\"],\n",
    "    no_robots_formatted[\"train\"],\n",
    "    maths_qa_formatted[\"train\"],\n",
    "]\n",
    "\n",
    "# OPTIONAL : can use any modern LLM to generate custom SFT data\n",
    "if os.path.isfile(\"synthetic_sft_data.jsonl\"):\n",
    "    print(\"Loading synthetic data...\")\n",
    "    synthetic_data = load_synthetic_data(\"synthetic_sft_data.jsonl\")\n",
    "    print(f\"Loaded {len(synthetic_data)} synthetic examples\")\n",
    "    \n",
    "    # Transform list of dicts to dict of lists\n",
    "    data_dict = {}\n",
    "    for key in synthetic_data[0].keys():\n",
    "        data_dict[key] = [item[key] for item in synthetic_data]\n",
    "    \n",
    "    synthetic_dataset = ds.from_dict(data_dict)\n",
    "\n",
    "    # Add to all datasets\n",
    "    combined_datasets.append(synthetic_dataset)\n",
    "\n",
    "# Combine all datasets\n",
    "print(\"\\n\\nCombining datasets...\")\n",
    "combined_train = concatenate_datasets(combined_datasets)\n",
    "\n",
    "print(f\"Total training examples: {len(combined_train)}\")\n",
    "print(\"\\nExample from combined dataset:\")\n",
    "pprint(next(iter(combined_train)))\n",
    "\n",
    "def tokenize_sft(example):\n",
    "    text = (\n",
    "        \"<|user|>\\n\"\n",
    "        f\"{example['user']}\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        f\"{example['assistant']}\"\n",
    "    )\n",
    "\n",
    "    ids = tokenizer.encode(text, allowed_special=set(special_tokens.keys()))\n",
    "    ids.append(eot_id)\n",
    "\n",
    "    # Find assistant token\n",
    "    try:\n",
    "        assistant_pos = ids.index(assistant_id)\n",
    "    except ValueError:\n",
    "        # Return empty tensors that will be filtered out\n",
    "        return {\"input_ids\": [], \"labels\": []}\n",
    "\n",
    "    # Create labels as shifted version of ids\n",
    "    labels = [IGNORE_INDEX] * len(ids)\n",
    "    labels[:-1] = ids[1:]\n",
    "\n",
    "    # Mask out everything before assistant response\n",
    "    labels[:assistant_pos + 1] = [IGNORE_INDEX] * (assistant_pos + 1)\n",
    "\n",
    "    # Truncate\n",
    "    ids = ids[:block_size]\n",
    "    labels = labels[:block_size]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": ids,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# Tokenize combined dataset\n",
    "print(\"Tokenizing combined dataset...\")\n",
    "combined_tokenized = combined_train.map(\n",
    "    tokenize_sft,\n",
    "    remove_columns=combined_train.column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "# Filter out empty examples (failed tokenization)\n",
    "pre_filter_len = len(combined_tokenized)\n",
    "combined_tokenized = combined_tokenized.filter(lambda x: len(x[\"input_ids\"]) > 0)\n",
    "print(f\"Filtered {pre_filter_len - len(combined_tokenized)} invalid examples\")\n",
    "print(f\"Final dataset size: {len(combined_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2dc3f0-4902-4404-b046-416806c20f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created with 5388 batches of 16 seqs\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    # Inputs from user / assistant conversations are of variable length -> pad for training\n",
    "    # To squeeze out performance, can assign inputs to buckets or pad with fixed len -> compile model\n",
    "    # Here the data is reasonably sized so we can skip\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    \n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    \n",
    "    for x in batch:\n",
    "        pad_len = max_len - len(x[\"input_ids\"])\n",
    "        input_ids.append(\n",
    "            x[\"input_ids\"] + [eot_id] * pad_len\n",
    "        )\n",
    "        labels.append(\n",
    "            x[\"labels\"] + [IGNORE_INDEX] * pad_len\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    combined_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=prefetch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with {len(train_loader)} batches of {batch_size} seqs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237d148-f21d-45e7-81dc-7b7d5543a14d",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Start from pretrained model, set `dropout` to 0, extend embedding table to the new role tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27ae4514-68d7-466c-b69e-3480dc0b14c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with config:\n",
      "{'dropout_prob': 0,\n",
      " 'embed_dim': 768,\n",
      " 'mlp_ratio': 4,\n",
      " 'num_heads': 12,\n",
      " 'num_layers': 12,\n",
      " 'vocab_size': 50304}\n",
      "Loading: gpt_full_run.pt...\n",
      "Loaded pretrained weights\n",
      "Initialized special tokens: <|user|>=50257, <|assistant|>=50258\n",
      "Parameter Breakdown:\n",
      "==================================================\n",
      "embeddings          :   38,633,472 (31.26%)\n",
      "other               :   28,311,584 (22.91%)\n",
      "norms               :       19,200 ( 0.02%)\n",
      "mlp                 :   56,623,104 (45.82%)\n",
      "==================================================\n",
      "TOTAL               :  123,587,360\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"vocab_size\": vocab_size,  # This should already be 50304 from both pretraining and SFT\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"mlp_ratio\": mlp_ratio,\n",
    "    \"dropout_prob\": dropout_prob,\n",
    "}\n",
    "\n",
    "print(\"Initializing model with config:\")\n",
    "pprint(model_config)\n",
    "model = GPT(**model_config).to(device)\n",
    "\n",
    "print(f\"Loading: {pretrained_weights}...\")\n",
    "trained_weights = strip_compile_prefix(torch.load(pretrained_weights, map_location=device))\n",
    "model.load_state_dict(trained_weights, strict=True)\n",
    "print(\"Loaded pretrained weights\")\n",
    "\n",
    "# Initialize the new special token embeddings (they exist but were untrained padding)\n",
    "# Indices 50257 (<|user|>) and 50258 (<|assistant|>) are within the padded vocab\n",
    "with torch.no_grad():\n",
    "    # Option 1: Copy from <|endoftext|> token\n",
    "    model.embedding.weight[user_id] = model.embedding.weight[eot_id].clone()\n",
    "    model.embedding.weight[assistant_id] = model.embedding.weight[eot_id].clone()\n",
    "\n",
    "print(f\"Initialized special tokens: <|user|>={user_id}, <|assistant|>={assistant_id}\")\n",
    "\n",
    "# Ready to train!\n",
    "model.train()\n",
    "_, _ = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ff922-468a-49b1-a8c7-26230af36f68",
   "metadata": {},
   "source": [
    "Weight decay is quite huge compared to habitual CNN but seems to be the standard for LLMs (empirical evidence), usually 0.1 for pretraining, 0.01 for SFT, helps avoiding memorization etc. Small lr because we're finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2e581bf-67ec-4db3-9bcd-a1516f736f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decay params: 48, No decay params: 26\n",
      "Total steps: 16164, Warmup steps: 808\n"
     ]
    }
   ],
   "source": [
    "# Separate parameters into decay and no-decay groups (same logic as pretraining)\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        continue\n",
    "    # Don't apply weight decay to:\n",
    "    # - Norm parameters (RMSNorm weights)\n",
    "    # - Embedding table (tied with lm_head)\n",
    "    # - Any bias terms\n",
    "    if any(kw in name.lower() for kw in ['norm', 'bias', 'embed', 'embedding', 'lm_head']):\n",
    "        no_decay_params.append(param)\n",
    "    else:\n",
    "        decay_params.append(param)\n",
    "\n",
    "print(f\"Decay params: {len(decay_params)}, No decay params: {len(no_decay_params)}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params, 'weight_decay': 0.01},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "],\n",
    "    lr=2e-5,\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "# Learning rate scheduler: linear warmup + cosine decay\n",
    "total_steps = NUM_EPOCHS * len(train_loader)\n",
    "warmup_steps = int(total_steps * 0.05)  # 5% warmup\n",
    "\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,  # start at 0.1 * lr\n",
    "    total_iters=warmup_steps\n",
    ")\n",
    "cosine_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps - warmup_steps,\n",
    "    eta_min=1e-6  # min lr\n",
    ")\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "    milestones=[warmup_steps]\n",
    ")\n",
    "\n",
    "print(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3a017-b03e-4e16-9e46-169a2e6fc1d4",
   "metadata": {},
   "source": [
    "We train over `epochs` because SFT data is 1) manageable 2) fixed Q/A samples while pretraining data is intractable and random contiguous chunk of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SFT training: 3 epochs, 5388 batches/epoch, 16164 total steps\n",
      "--------------------------------------------------------------------------------\n",
      "epoch 1/3 | step     0/5388 (  0.0%) | loss 2.0774 (avg 2.0774) | lr 2.02e-06 | ETA --:--\n",
      "epoch 1/3 | step   100/5388 (  1.9%) | loss 2.3841 (avg 2.1575) | lr 4.25e-06 | ETA 19:25\n",
      "epoch 1/3 | step   200/5388 (  3.7%) | loss 2.2457 (avg 2.1023) | lr 6.48e-06 | ETA 18:19\n",
      "epoch 1/3 | step   300/5388 (  5.6%) | loss 1.6772 (avg 2.0780) | lr 8.71e-06 | ETA 18:00\n",
      "epoch 1/3 | step   400/5388 (  7.4%) | loss 2.2998 (avg 2.0520) | lr 1.09e-05 | ETA 17:25\n",
      "epoch 1/3 | step   500/5388 (  9.3%) | loss 1.9112 (avg 2.0323) | lr 1.32e-05 | ETA 16:53\n",
      "epoch 1/3 | step   600/5388 ( 11.2%) | loss 2.0248 (avg 2.0170) | lr 1.54e-05 | ETA 16:27\n",
      "epoch 1/3 | step   700/5388 ( 13.0%) | loss 1.7973 (avg 2.0056) | lr 1.76e-05 | ETA 16:04\n",
      "epoch 1/3 | step   800/5388 ( 14.9%) | loss 1.8609 (avg 1.9970) | lr 1.98e-05 | ETA 15:41\n",
      "epoch 1/3 | step   900/5388 ( 16.7%) | loss 1.8380 (avg 1.9816) | lr 2.00e-05 | ETA 15:14\n",
      "epoch 1/3 | step  1000/5388 ( 18.6%) | loss 1.7404 (avg 1.9688) | lr 2.00e-05 | ETA 14:50\n",
      "epoch 1/3 | step  1100/5388 ( 20.4%) | loss 1.8733 (avg 1.9582) | lr 2.00e-05 | ETA 14:25\n",
      "epoch 1/3 | step  1200/5388 ( 22.3%) | loss 1.8031 (avg 1.9505) | lr 2.00e-05 | ETA 14:03\n",
      "epoch 1/3 | step  1300/5388 ( 24.1%) | loss 1.5692 (avg 1.9433) | lr 2.00e-05 | ETA 13:44\n",
      "epoch 1/3 | step  1400/5388 ( 26.0%) | loss 2.0386 (avg 1.9372) | lr 1.99e-05 | ETA 13:21\n",
      "epoch 1/3 | step  1500/5388 ( 27.9%) | loss 2.0107 (avg 1.9308) | lr 1.99e-05 | ETA 13:04\n",
      "epoch 1/3 | step  1600/5388 ( 29.7%) | loss 2.2350 (avg 1.9266) | lr 1.99e-05 | ETA 12:44\n",
      "epoch 1/3 | step  1700/5388 ( 31.6%) | loss 2.1122 (avg 1.9198) | lr 1.98e-05 | ETA 12:25\n",
      "epoch 1/3 | step  1800/5388 ( 33.4%) | loss 1.8152 (avg 1.9141) | lr 1.98e-05 | ETA 12:04\n",
      "epoch 1/3 | step  1900/5388 ( 35.3%) | loss 1.6215 (avg 1.9072) | lr 1.98e-05 | ETA 11:43\n",
      "epoch 1/3 | step  2000/5388 ( 37.1%) | loss 1.9925 (avg 1.9022) | lr 1.97e-05 | ETA 11:23\n",
      "epoch 1/3 | step  2100/5388 ( 39.0%) | loss 1.4131 (avg 1.8994) | lr 1.97e-05 | ETA 11:04\n",
      "epoch 1/3 | step  2200/5388 ( 40.9%) | loss 1.6710 (avg 1.8953) | lr 1.96e-05 | ETA 10:44\n",
      "epoch 1/3 | step  2300/5388 ( 42.7%) | loss 1.4346 (avg 1.8898) | lr 1.96e-05 | ETA 10:24\n",
      "epoch 1/3 | step  2400/5388 ( 44.6%) | loss 1.4889 (avg 1.8867) | lr 1.95e-05 | ETA 10:03\n",
      "epoch 1/3 | step  2500/5388 ( 46.4%) | loss 1.9634 (avg 1.8817) | lr 1.94e-05 | ETA 09:43\n",
      "epoch 1/3 | step  2600/5388 ( 48.3%) | loss 1.5647 (avg 1.8781) | lr 1.94e-05 | ETA 09:23\n",
      "epoch 1/3 | step  2700/5388 ( 50.1%) | loss 1.8127 (avg 1.8746) | lr 1.93e-05 | ETA 09:03\n",
      "epoch 1/3 | step  2800/5388 ( 52.0%) | loss 1.7850 (avg 1.8729) | lr 1.92e-05 | ETA 08:42\n",
      "epoch 1/3 | step  2900/5388 ( 53.8%) | loss 2.0727 (avg 1.8709) | lr 1.91e-05 | ETA 08:22\n",
      "epoch 1/3 | step  3000/5388 ( 55.7%) | loss 1.7517 (avg 1.8674) | lr 1.91e-05 | ETA 08:02\n",
      "epoch 1/3 | step  3100/5388 ( 57.6%) | loss 1.9469 (avg 1.8650) | lr 1.90e-05 | ETA 07:42\n",
      "epoch 1/3 | step  3200/5388 ( 59.4%) | loss 1.7916 (avg 1.8618) | lr 1.89e-05 | ETA 07:22\n",
      "epoch 1/3 | step  3300/5388 ( 61.3%) | loss 1.8736 (avg 1.8595) | lr 1.88e-05 | ETA 07:02\n",
      "epoch 1/3 | step  3400/5388 ( 63.1%) | loss 1.5197 (avg 1.8576) | lr 1.87e-05 | ETA 06:41\n",
      "epoch 1/3 | step  3500/5388 ( 65.0%) | loss 1.6711 (avg 1.8549) | lr 1.86e-05 | ETA 06:21\n",
      "epoch 1/3 | step  3600/5388 ( 66.8%) | loss 1.6915 (avg 1.8532) | lr 1.85e-05 | ETA 06:01\n",
      "epoch 1/3 | step  3700/5388 ( 68.7%) | loss 1.9817 (avg 1.8507) | lr 1.84e-05 | ETA 05:41\n",
      "epoch 1/3 | step  3800/5388 ( 70.5%) | loss 1.6949 (avg 1.8472) | lr 1.83e-05 | ETA 05:21\n",
      "epoch 1/3 | step  3900/5388 ( 72.4%) | loss 1.5052 (avg 1.8463) | lr 1.82e-05 | ETA 05:01\n",
      "epoch 1/3 | step  4000/5388 ( 74.3%) | loss 1.6084 (avg 1.8440) | lr 1.80e-05 | ETA 04:40\n",
      "epoch 1/3 | step  4100/5388 ( 76.1%) | loss 1.4375 (avg 1.8415) | lr 1.79e-05 | ETA 04:19\n",
      "epoch 1/3 | step  4200/5388 ( 78.0%) | loss 1.7941 (avg 1.8406) | lr 1.78e-05 | ETA 03:59\n",
      "epoch 1/3 | step  4300/5388 ( 79.8%) | loss 2.2125 (avg 1.8390) | lr 1.77e-05 | ETA 03:39\n",
      "epoch 1/3 | step  4400/5388 ( 81.7%) | loss 1.8339 (avg 1.8379) | lr 1.75e-05 | ETA 03:18\n",
      "epoch 1/3 | step  4500/5388 ( 83.5%) | loss 1.4135 (avg 1.8369) | lr 1.74e-05 | ETA 02:58\n",
      "epoch 1/3 | step  4600/5388 ( 85.4%) | loss 1.5630 (avg 1.8351) | lr 1.73e-05 | ETA 02:38\n",
      "epoch 1/3 | step  4700/5388 ( 87.2%) | loss 1.9872 (avg 1.8333) | lr 1.71e-05 | ETA 02:18\n",
      "epoch 1/3 | step  4800/5388 ( 89.1%) | loss 1.7511 (avg 1.8318) | lr 1.70e-05 | ETA 01:58\n",
      "epoch 1/3 | step  4900/5388 ( 91.0%) | loss 1.5290 (avg 1.8303) | lr 1.69e-05 | ETA 01:38\n",
      "epoch 1/3 | step  5000/5388 ( 92.8%) | loss 1.7678 (avg 1.8283) | lr 1.67e-05 | ETA 01:17\n",
      "epoch 1/3 | step  5100/5388 ( 94.7%) | loss 1.8166 (avg 1.8268) | lr 1.66e-05 | ETA 00:57\n",
      "epoch 1/3 | step  5200/5388 ( 96.5%) | loss 1.8637 (avg 1.8254) | lr 1.64e-05 | ETA 00:37\n",
      "epoch 1/3 | step  5300/5388 ( 98.4%) | loss 1.9851 (avg 1.8242) | lr 1.63e-05 | ETA 00:17\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 complete | avg loss: 1.8231 | time: 17:59\n",
      "--------------------------------------------------------------------------------\n",
      "Saved checkpoint: GPT_SFT_epoch_0.pt\n",
      "epoch 2/3 | step     0/5388 (  0.0%) | loss 1.4653 (avg 1.4653) | lr 1.61e-05 | ETA --:--\n",
      "epoch 2/3 | step   100/5388 (  1.9%) | loss 1.5160 (avg 1.6168) | lr 1.60e-05 | ETA 19:05\n",
      "epoch 2/3 | step   200/5388 (  3.7%) | loss 1.5862 (avg 1.6180) | lr 1.58e-05 | ETA 18:04\n",
      "epoch 2/3 | step   300/5388 (  5.6%) | loss 1.6085 (avg 1.6294) | lr 1.56e-05 | ETA 17:13\n",
      "epoch 2/3 | step   400/5388 (  7.4%) | loss 1.9874 (avg 1.6307) | lr 1.55e-05 | ETA 16:50\n",
      "epoch 2/3 | step   500/5388 (  9.3%) | loss 1.6290 (avg 1.6258) | lr 1.53e-05 | ETA 16:23\n",
      "epoch 2/3 | step   600/5388 ( 11.2%) | loss 1.8785 (avg 1.6340) | lr 1.51e-05 | ETA 16:05\n",
      "epoch 2/3 | step   700/5388 ( 13.0%) | loss 2.3964 (avg 1.6368) | lr 1.50e-05 | ETA 15:44\n",
      "epoch 2/3 | step   800/5388 ( 14.9%) | loss 1.4793 (avg 1.6321) | lr 1.48e-05 | ETA 15:19\n",
      "epoch 2/3 | step   900/5388 ( 16.7%) | loss 1.3036 (avg 1.6302) | lr 1.46e-05 | ETA 15:04\n",
      "epoch 2/3 | step  1000/5388 ( 18.6%) | loss 1.5687 (avg 1.6272) | lr 1.45e-05 | ETA 14:47\n",
      "epoch 2/3 | step  1100/5388 ( 20.4%) | loss 1.8563 (avg 1.6257) | lr 1.43e-05 | ETA 14:27\n",
      "epoch 2/3 | step  1200/5388 ( 22.3%) | loss 1.2921 (avg 1.6286) | lr 1.41e-05 | ETA 14:06\n",
      "epoch 2/3 | step  1300/5388 ( 24.1%) | loss 1.9874 (avg 1.6290) | lr 1.39e-05 | ETA 13:45\n",
      "epoch 2/3 | step  1400/5388 ( 26.0%) | loss 1.8191 (avg 1.6287) | lr 1.37e-05 | ETA 13:25\n",
      "epoch 2/3 | step  1500/5388 ( 27.9%) | loss 2.1109 (avg 1.6304) | lr 1.35e-05 | ETA 13:02\n",
      "epoch 2/3 | step  1600/5388 ( 29.7%) | loss 1.7807 (avg 1.6283) | lr 1.34e-05 | ETA 12:43\n",
      "epoch 2/3 | step  1700/5388 ( 31.6%) | loss 1.6186 (avg 1.6264) | lr 1.32e-05 | ETA 12:20\n",
      "epoch 2/3 | step  1800/5388 ( 33.4%) | loss 1.2089 (avg 1.6278) | lr 1.30e-05 | ETA 12:01\n",
      "epoch 2/3 | step  1900/5388 ( 35.3%) | loss 1.4111 (avg 1.6275) | lr 1.28e-05 | ETA 11:42\n",
      "epoch 2/3 | step  2000/5388 ( 37.1%) | loss 1.6741 (avg 1.6253) | lr 1.26e-05 | ETA 11:23\n",
      "epoch 2/3 | step  2100/5388 ( 39.0%) | loss 1.4963 (avg 1.6261) | lr 1.24e-05 | ETA 11:03\n",
      "epoch 2/3 | step  2200/5388 ( 40.9%) | loss 1.2839 (avg 1.6274) | lr 1.22e-05 | ETA 10:44\n",
      "epoch 2/3 | step  2300/5388 ( 42.7%) | loss 1.6561 (avg 1.6257) | lr 1.20e-05 | ETA 10:24\n",
      "epoch 2/3 | step  2400/5388 ( 44.6%) | loss 1.8850 (avg 1.6250) | lr 1.19e-05 | ETA 10:03\n",
      "epoch 2/3 | step  2500/5388 ( 46.4%) | loss 1.8230 (avg 1.6255) | lr 1.17e-05 | ETA 09:42\n",
      "epoch 2/3 | step  2600/5388 ( 48.3%) | loss 1.8009 (avg 1.6249) | lr 1.15e-05 | ETA 09:22\n",
      "epoch 2/3 | step  2700/5388 ( 50.1%) | loss 2.3911 (avg 1.6246) | lr 1.13e-05 | ETA 09:01\n",
      "epoch 2/3 | step  2800/5388 ( 52.0%) | loss 1.6459 (avg 1.6253) | lr 1.11e-05 | ETA 08:42\n",
      "epoch 2/3 | step  2900/5388 ( 53.8%) | loss 1.6548 (avg 1.6271) | lr 1.09e-05 | ETA 08:21\n",
      "epoch 2/3 | step  3000/5388 ( 55.7%) | loss 1.4259 (avg 1.6275) | lr 1.07e-05 | ETA 08:00\n",
      "epoch 2/3 | step  3100/5388 ( 57.6%) | loss 1.7437 (avg 1.6291) | lr 1.05e-05 | ETA 07:40\n",
      "epoch 2/3 | step  3200/5388 ( 59.4%) | loss 1.5315 (avg 1.6281) | lr 1.03e-05 | ETA 07:20\n",
      "epoch 2/3 | step  3300/5388 ( 61.3%) | loss 1.8090 (avg 1.6286) | lr 1.01e-05 | ETA 06:59\n",
      "epoch 2/3 | step  3400/5388 ( 63.1%) | loss 1.6196 (avg 1.6291) | lr 9.91e-06 | ETA 06:39\n",
      "epoch 2/3 | step  3500/5388 ( 65.0%) | loss 1.5107 (avg 1.6282) | lr 9.72e-06 | ETA 06:19\n",
      "epoch 2/3 | step  3600/5388 ( 66.8%) | loss 1.4345 (avg 1.6283) | lr 9.52e-06 | ETA 05:58\n",
      "epoch 2/3 | step  3700/5388 ( 68.7%) | loss 1.4703 (avg 1.6293) | lr 9.33e-06 | ETA 05:39\n",
      "epoch 2/3 | step  3800/5388 ( 70.5%) | loss 1.3931 (avg 1.6292) | lr 9.14e-06 | ETA 05:18\n",
      "epoch 2/3 | step  3900/5388 ( 72.4%) | loss 1.7227 (avg 1.6296) | lr 8.95e-06 | ETA 04:58\n",
      "epoch 2/3 | step  4000/5388 ( 74.3%) | loss 1.6116 (avg 1.6295) | lr 8.75e-06 | ETA 04:38\n",
      "epoch 2/3 | step  4100/5388 ( 76.1%) | loss 1.5786 (avg 1.6298) | lr 8.56e-06 | ETA 04:18\n",
      "epoch 2/3 | step  4200/5388 ( 78.0%) | loss 1.8632 (avg 1.6296) | lr 8.37e-06 | ETA 03:58\n",
      "epoch 2/3 | step  4300/5388 ( 79.8%) | loss 1.8575 (avg 1.6293) | lr 8.19e-06 | ETA 03:38\n",
      "epoch 2/3 | step  4400/5388 ( 81.7%) | loss 1.7073 (avg 1.6288) | lr 8.00e-06 | ETA 03:18\n",
      "epoch 2/3 | step  4500/5388 ( 83.5%) | loss 1.4456 (avg 1.6286) | lr 7.81e-06 | ETA 02:58\n",
      "epoch 2/3 | step  4600/5388 ( 85.4%) | loss 1.4315 (avg 1.6282) | lr 7.62e-06 | ETA 02:37\n",
      "epoch 2/3 | step  4700/5388 ( 87.2%) | loss 1.9951 (avg 1.6293) | lr 7.44e-06 | ETA 02:17\n",
      "epoch 2/3 | step  4800/5388 ( 89.1%) | loss 1.4462 (avg 1.6302) | lr 7.26e-06 | ETA 01:57\n",
      "epoch 2/3 | step  4900/5388 ( 91.0%) | loss 2.0010 (avg 1.6294) | lr 7.07e-06 | ETA 01:37\n",
      "epoch 2/3 | step  5000/5388 ( 92.8%) | loss 1.7353 (avg 1.6293) | lr 6.89e-06 | ETA 01:17\n",
      "epoch 2/3 | step  5100/5388 ( 94.7%) | loss 1.7949 (avg 1.6297) | lr 6.72e-06 | ETA 00:57\n",
      "epoch 2/3 | step  5200/5388 ( 96.5%) | loss 2.1543 (avg 1.6306) | lr 6.54e-06 | ETA 00:37\n",
      "epoch 2/3 | step  5300/5388 ( 98.4%) | loss 2.1616 (avg 1.6308) | lr 6.36e-06 | ETA 00:17\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 complete | avg loss: 1.6316 | time: 17:57\n",
      "--------------------------------------------------------------------------------\n",
      "Saved checkpoint: GPT_SFT_epoch_1.pt\n",
      "epoch 3/3 | step     0/5388 (  0.0%) | loss 1.4431 (avg 1.4431) | lr 6.21e-06 | ETA --:--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "global_step = 0\n",
    "total_batches = len(train_loader)\n",
    "log_interval = 100\n",
    "\n",
    "print(f\"Starting SFT training: {NUM_EPOCHS} epochs, {total_batches} batches/epoch, {total_batches * NUM_EPOCHS} total steps\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss_sum = 0.0\n",
    "    epoch_loss_count = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.amp.autocast('cuda', dtype=dtype):\n",
    "            logits = model(input_ids)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels.view(-1),\n",
    "                ignore_index=IGNORE_INDEX,\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        # Track epoch loss\n",
    "        epoch_loss_sum += loss.item()\n",
    "        epoch_loss_count += 1\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            pct_complete = 100 * (step + 1) / total_batches\n",
    "            elapsed = time.time() - epoch_start\n",
    "            \n",
    "            # Estimate time remaining for epoch\n",
    "            if step > 0:\n",
    "                steps_per_sec = step / elapsed\n",
    "                remaining_steps = total_batches - step\n",
    "                eta_sec = remaining_steps / steps_per_sec\n",
    "                eta_str = f\"{int(eta_sec // 60):02d}:{int(eta_sec % 60):02d}\"\n",
    "            else:\n",
    "                eta_str = \"--:--\"\n",
    "            \n",
    "            avg_loss = epoch_loss_sum / epoch_loss_count\n",
    "            \n",
    "            print(\n",
    "                f\"epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "                f\"step {step:>5}/{total_batches} ({pct_complete:5.1f}%) | \"\n",
    "                f\"loss {loss.item():.4f} (avg {avg_loss:.4f}) | \"\n",
    "                f\"lr {current_lr:.2e} | \"\n",
    "                f\"ETA {eta_str}\"\n",
    "            )\n",
    "\n",
    "    # End of epoch summary\n",
    "    epoch_elapsed = time.time() - epoch_start\n",
    "    epoch_avg_loss = epoch_loss_sum / epoch_loss_count\n",
    "    print(\"-\" * 80)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1} complete | \"\n",
    "        f\"avg loss: {epoch_avg_loss:.4f} | \"\n",
    "        f\"time: {int(epoch_elapsed // 60):02d}:{int(epoch_elapsed % 60):02d}\"\n",
    "    )\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Save model in full precision\n",
    "    torch.save(model.state_dict(), f\"GPT_SFT_epoch_{epoch}.pt\")\n",
    "    print(f\"Saved checkpoint: GPT_SFT_epoch_{epoch}.pt\")\n",
    "\n",
    "# Final summary\n",
    "total_time = time.time() - training_start\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training complete! Total time: {int(total_time // 3600):02d}:{int((total_time % 3600) // 60):02d}:{int(total_time % 60):02d}\")\n",
    "print(f\"Final checkpoint: GPT_SFT_epoch_{NUM_EPOCHS - 1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d67998ad-5c05-424b-9358-fc72fe99d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "POST-TRAINING EVALUATION\n",
      "================================================================================\n",
      "\n",
      "[Simple instruction]\n",
      "User: Write a short greeting message.\n",
      "----------------------------------------\n",
      "Assistant: Hello everyone!\n",
      "----------------------------------------\n",
      "\n",
      "[Knowledge]\n",
      "User: What is the capital of France?\n",
      "----------------------------------------\n",
      "Assistant: The capital of France is Paris.\n",
      "----------------------------------------\n",
      "\n",
      "[Basic reasoning]\n",
      "User: If I have 3 apples and buy 2 more, how many do I have?\n",
      "----------------------------------------\n",
      "Assistant: I need to find the total number of apples, which is the product of the number of apples and the number of apples each person has, given that I want to count the number of apples and apples per person. The problem says that I have 3 apples and buy 2 more, so I can write an equation: 3 * 2 = 3. Solving for the number of apples, I get 2 = 3. This means that 2 people have 3 apples and 2 apples per person. Now I have 3 apples and 2 apples per person, so I can write another equation: 2 * 3 = 6. Solving for the number of apples, I get 3 = 6. This means that 6 people have 6 apples and 6 apples per person, so\n",
      "----------------------------------------\n",
      "\n",
      "[Creative]\n",
      "User: Write a haiku about programming.\n",
      "----------------------------------------\n",
      "Assistant: Code it, code it, code it\n",
      "Design it, design it, design it\n",
      "Code it, code it, code it\n",
      "Code it, code it, code it\n",
      "Code it, code it, code it\n",
      "----------------------------------------\n",
      "\n",
      "[Explanation]\n",
      "User: Explain what a neural network is in simple terms.\n",
      "----------------------------------------\n",
      "Assistant: A neural network is a type of artificial intelligence model that uses a set of interconnected nodes to process and analyze data. The main goal of a neural network is to learn from data and make predictions or decisions based on that data. The network is designed to work well with large amounts of data and can learn to recognize patterns and relationships within it.\n",
      "----------------------------------------\n",
      "\n",
      "[Code]\n",
      "User: Write a Python function that adds two numbers.\n",
      "----------------------------------------\n",
      "Assistant: Here is a Python function that adds two numbers:\n",
      "\n",
      "```python\n",
      "def add_number(num):\n",
      "    # Initialize a variable to add the number\n",
      "    num = num\n",
      "   \n",
      "    # Iterate through the numbers\n",
      "    for i in range(1,num):\n",
      "        # If the number is greater than or equal to the number, add it to the number\n",
      "        return num\n",
      "   \n",
      "    # If the number is less than or equal to the number, subtract it from the number\n",
      "       return num\n",
      "```\n",
      "\n",
      "This function takes in\n",
      "----------------------------------------\n",
      "\n",
      "[Multi-step]\n",
      "User: List 3 benefits of exercise.\n",
      "----------------------------------------\n",
      "Assistant: 1. Improved physical health: Regular exercise can help improve overall physical health, including strength, flexibility, and flexibility, as well as reducing the risk of chronic diseases such as heart disease, type 2 diabetes, and certain types of cancer.\n",
      "\n",
      "2. Improved mental health: Exercise can improve mood and cognitive function, making it a great tool for managing stress, anxiety, and depression. It can also reduce the risk of developing chronic diseases such as obesity, diabetes, and heart disease.\n",
      "\n",
      "3. Enhanced energy levels: Regular exercise can help increase energy levels, which can improve focus and concentration. This can also help reduce feelings of fatigue and improve mood.\n",
      "----------------------------------------\n",
      "\n",
      "[Edge case]\n",
      "User: Summarize the following text: \n",
      "----------------------------------------\n",
      "Assistant: The text provides an overview of the main points and ideas of the article. It discusses the background and purpose of the article, its main arguments and the arguments and perspectives presented in it.\n",
      "----------------------------------------\n",
      "\n",
      "================================================================================\n",
      "FORMAT COMPLIANCE CHECK\n",
      "================================================================================\n",
      "EOT termination rate: 2/3 (67%)\n",
      "Avg response length: 73 tokens\n",
      "  → Good: Model learns to stop\n",
      "  → Good: Reasonable length\n",
      "\n",
      "================================================================================\n",
      "Evaluation complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    prompt_text,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    stop_token_id=eot_id,\n",
    "):\n",
    "    \"\"\"Generate response from a prompt string.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    enc = tokenizer.encode(prompt_text, allowed_special=set(special_tokens.keys()))\n",
    "    input_ids = torch.tensor(enc, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop to block_size if needed\n",
    "        input_ids_cond = input_ids if input_ids.shape[1] <= block_size else input_ids[:, -block_size:]\n",
    "        \n",
    "        logits = model(input_ids_cond)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        \n",
    "        # Top-p sampling\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            next_token_logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        if stop_token_id is not None and next_token.item() == stop_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0].tolist())\n",
    "\n",
    "\n",
    "def format_prompt(user_message):\n",
    "    \"\"\"Format a user message into the chat template.\"\"\"\n",
    "    return f\"<|user|>\\n{user_message}\\n<|assistant|>\\n\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Quick Evaluation Suite\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"POST-TRAINING EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "eval_prompts = [\n",
    "    # Basic instruction following\n",
    "    (\"Simple instruction\", \"Write a short greeting message.\"),\n",
    "    \n",
    "    # Knowledge recall (don't expect accuracy, just coherence)\n",
    "    (\"Knowledge\", \"What is the capital of France?\"),\n",
    "    \n",
    "    # Reasoning (basic)\n",
    "    (\"Basic reasoning\", \"If I have 3 apples and buy 2 more, how many do I have?\"),\n",
    "    \n",
    "    # Creative\n",
    "    (\"Creative\", \"Write a haiku about programming.\"),\n",
    "    \n",
    "    # Explanation\n",
    "    (\"Explanation\", \"Explain what a neural network is in simple terms.\"),\n",
    "    \n",
    "    # Code (if trained on code data)\n",
    "    (\"Code\", \"Write a Python function that adds two numbers.\"),\n",
    "    \n",
    "    # Multi-step\n",
    "    (\"Multi-step\", \"List 3 benefits of exercise.\"),\n",
    "    \n",
    "    # Refusal/boundary (interesting to see behavior)\n",
    "    (\"Edge case\", \"Summarize the following text: \"),\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "for category, user_msg in eval_prompts:\n",
    "    prompt = format_prompt(user_msg)\n",
    "    \n",
    "    print(f\"\\n[{category}]\")\n",
    "    print(f\"User: {user_msg}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        response = generate(model, prompt, max_new_tokens=150, temperature=0.7)\n",
    "        # Extract just the assistant response\n",
    "        if \"<|assistant|>\" in response:\n",
    "            assistant_part = response.split(\"<|assistant|>\\n\")[-1]\n",
    "            # Clean up any trailing special tokens\n",
    "            assistant_part = assistant_part.replace(\"<|endoftext|>\", \"\").strip()\n",
    "        else:\n",
    "            assistant_part = response\n",
    "        print(f\"Assistant: {assistant_part}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# =============================================================================\n",
    "# Quantitative checks\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FORMAT COMPLIANCE CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if model properly terminates with EOT\n",
    "test_prompts = [format_prompt(p) for _, p in eval_prompts[:3]]\n",
    "eot_count = 0\n",
    "total_length = 0\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    enc = tokenizer.encode(prompt, allowed_special=set(special_tokens.keys()))\n",
    "    input_ids = torch.tensor(enc, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate with greedy decoding for consistency\n",
    "    model.eval()\n",
    "    for _ in range(200):\n",
    "        logits = model(input_ids[:, -block_size:] if input_ids.shape[1] > block_size else input_ids)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        if next_token.item() == eot_id:\n",
    "            eot_count += 1\n",
    "            break\n",
    "    \n",
    "    total_length += input_ids.shape[1] - len(enc)\n",
    "\n",
    "avg_response_len = total_length / len(test_prompts)\n",
    "eot_rate = 100 * eot_count / len(test_prompts)\n",
    "\n",
    "print(f\"EOT termination rate: {eot_count}/{len(test_prompts)} ({eot_rate:.0f}%)\")\n",
    "print(f\"Avg response length: {avg_response_len:.0f} tokens\")\n",
    "print(f\"  → {'Good: Model learns to stop' if eot_rate > 50 else 'Warning: Model may ramble'}\")\n",
    "print(f\"  → {'Good: Reasonable length' if 20 < avg_response_len < 150 else 'Check: Unusual response length'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Evaluation complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5628d8da-e6fc-4e40-bf7c-5e3a20e22bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: No, it is not a good idea to jump from a bridge. The bridge is not a physical structure and it is not designed to be used as a place to jump. Instead, it is a place to rest and take a breath, and to get some fresh air and rest.\n"
     ]
    }
   ],
   "source": [
    "prompt = format_prompt(\"Is it a good idea to jump from a bridge ?\")\n",
    "\n",
    "response = generate(model, prompt, max_new_tokens=150, temperature=0.7)\n",
    "# Extract just the assistant response\n",
    "if \"<|assistant|>\" in response:\n",
    "    assistant_part = response.split(\"<|assistant|>\\n\")[-1]\n",
    "    # Clean up any trailing special tokens\n",
    "    assistant_part = assistant_part.replace(\"<|endoftext|>\", \"\").strip()\n",
    "else:\n",
    "    assistant_part = response\n",
    "print(f\"Assistant: {assistant_part}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc4c0f-ee94-4100-a2fe-66cc7967f8f4",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Even from a tiny model (~120M parameters), we can get something that LOOKS reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc0f1e-6fac-495b-92ee-8308cad1b41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
