{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "SFT part : teach the model the assistant format\n",
    "\n",
    "Basically : \n",
    "- the prompt tokens are masked (no loss)\n",
    "- the response tokens are trained on (loss applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import csv\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment config\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Torch runtime config\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, causal: bool = True, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "        \n",
    "        self.causal = causal\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout_p = dropout\n",
    "        \n",
    "        # Fused QKV projection: 3x the output size\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        \n",
    "        self.rotary_emb = RotaryEmbedding(dim=self.head_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, k_v_cache=None):\n",
    "        B, T, _ = x.shape\n",
    "        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n",
    "    \n",
    "        # 1. Single fused projection\n",
    "        if using_cache:\n",
    "            x_q = x[:, -1:, :]\n",
    "            qkv = self.qkv_proj(x_q)  # (B, 1, 3*embed_dim)\n",
    "        else:\n",
    "            qkv = self.qkv_proj(x)  # (B, T, 3*embed_dim)\n",
    "        \n",
    "        # 2. Split into Q, K, V\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)  # Each is (B, T, embed_dim)\n",
    "        \n",
    "        def split_heads(t):\n",
    "            return t.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 3. Split heads -> (B, H, T, D_head)\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "    \n",
    "        # 4. Apply RoPE \n",
    "        if using_cache:\n",
    "            past_len = k_v_cache[\"K\"].shape[-2]\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q, offset=past_len)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K, offset=past_len)\n",
    "            \n",
    "            K = torch.cat([k_v_cache[\"K\"], K], dim=-2)\n",
    "            V = torch.cat([k_v_cache[\"V\"], V], dim=-2)\n",
    "        else:\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K)\n",
    "    \n",
    "        # 5. Update cache\n",
    "        if k_v_cache is not None:\n",
    "            k_v_cache[\"K\"] = K\n",
    "            k_v_cache[\"V\"] = V\n",
    "    \n",
    "        # 6. Attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            query=Q,\n",
    "            key=K,\n",
    "            value=V,\n",
    "            attn_mask=None, \n",
    "            dropout_p=self.dropout_p if self.training else 0.0,\n",
    "            is_causal=self.causal and (Q.shape[-2] > 1)\n",
    "        )\n",
    "        \n",
    "        # 7. Merge heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, -1, self.embed_dim)\n",
    "    \n",
    "        return self.out_proj(out), k_v_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf4ba2d-402d-4eab-86e1-754f9faeaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim=None, dropout_prob=0.1, use_swiglu=True):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * embed_dim\n",
    "        \n",
    "        self.use_swiglu = use_swiglu\n",
    "        \n",
    "        if use_swiglu:\n",
    "            # Adjust hidden_dim for param count matching\n",
    "            hidden_dim = int(2 * hidden_dim / 3)\n",
    "            self.gate_proj = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.up_proj = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.act = nn.GELU()\n",
    "            self.linear2 = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_swiglu:\n",
    "            return self.dropout(self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x)))\n",
    "        else:\n",
    "            return self.dropout(self.linear2(self.act(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c04d073-6e3f-469d-98b4-5cda92115923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 causal=True,\n",
    "                 use_swiglu=True,\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Initialize a complete transformer block.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Multi-head self-attention for sequence modeling\n",
    "        2. 1st Normalization (pre-norm architecture)\n",
    "        3. MLP with specified expansion ratio\n",
    "        4. 2nd Normalization\n",
    "    \n",
    "        TRANSFORMER BLOCK ARCHITECTURE:\n",
    "        x → Norm → MultiHeadAttention → + (residual) →\n",
    "            Norm → MLP → + (residual) → output\n",
    "    \n",
    "        NB: We use pre-norm architecture (before attention/MLP)\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "        self.norm1 = nn.RMSNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, causal, dropout_prob)  # causal = masking out tokens\n",
    "        self.norm2 = nn.RMSNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio * embed_dim, dropout_prob, use_swiglu)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        x1 = self.norm1(x)\n",
    "        x2, cache = self.mha(x1, cache)  # will be used when generating tokens during inference\n",
    "        x2 = x2 + x  # residual path\n",
    "    \n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.mlp(x3) + x2  # residual path\n",
    "        return x3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT (Generative Pre-trained Transformer) model.\n",
    "\n",
    "    This combines embeddings, positional encoding, multiple transformer blocks,\n",
    "    and a language modeling head for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 use_swiglu=True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize complete GPT model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout_prob, use_swiglu) for _ in range(num_layers)])\n",
    "        self.norm = nn.RMSNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        # weight tying\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        # below shamefully stolen from nano-gpt\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * self.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"Number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.embedding.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "       \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.embedding(tokens)\n",
    "        x = self.dropout(embeddings)\n",
    "        for b in self.blocks:\n",
    "            x, _ = b(x)  # iteratively refines features from initial embeddings\n",
    "        features = self.norm(x)  # normalized to stabilize training\n",
    "        return self.lm_head(features)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 prompt_tokens,\n",
    "                 max_new_tokens=50,\n",
    "                 temperature=1.0,\n",
    "                 use_cache=True,\n",
    "                 use_top_k=False,\n",
    "                ):\n",
    "        self.eval()\n",
    "\n",
    "        tokens_out = prompt_tokens.clone()\n",
    "        current_tokens = prompt_tokens.clone()\n",
    "        tokens_out = tokens_out.to(self.device)\n",
    "        current_tokens = current_tokens.to(self.device)\n",
    "        cache = [{} if use_cache else None for _ in range(len(self.blocks))]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            x = self.embedding(current_tokens)\n",
    "            for i, b in enumerate(self.blocks):\n",
    "                x, c_i = b(x, cache[i])\n",
    "                cache[i] = c_i\n",
    "            \n",
    "            features = self.norm(x)\n",
    "            logits = self.lm_head(features)\n",
    "                    \n",
    "            last_logits = logits[:, -1, :]\n",
    "    \n",
    "            if temperature > 0:\n",
    "                scaled_logits = last_logits / temperature\n",
    "                # Only sample from top k tokens to avoid garbage prediction derailing whole prediction\n",
    "                # We don't simply take max prob token to allow \"creativity\"\n",
    "                if use_top_k:\n",
    "                    # heuristic that is ok for toy project\n",
    "                    # most of probability mass in on a small amount of tokens\n",
    "                    k = min(max(5, int(0.01 * self.vocab_size)), 100)\n",
    "                    values, indices = torch.topk(scaled_logits, k)\n",
    "                    scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n",
    "                    scaled_logits.scatter_(1, indices, values)\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                # Greedy decoding if temp is 0 (prevents division by zero)\n",
    "                next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
    "    \n",
    "            tokens_out = torch.cat([tokens_out, next_token], dim=1)\n",
    "\n",
    "            # If caching, we only need to feed the newest token next time, otherwise full sequence\n",
    "            current_tokens = next_token if use_cache else tokens_out\n",
    "       \n",
    "        return tokens_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0cbeb30-9756-4a6f-a1a4-a812ba79d320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257 50258 50256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|user|><|assistant|><|endoftext|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to re-use the exact same tokenizer for SFT\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"GPT2\")\n",
    "eot_id = tokenizer.token_to_id(\"<|endoftext|>\")\n",
    "\n",
    "ROLE_TOKENS = [\"<|user|>\", \"<|assistant|>\"]\n",
    "\n",
    "tokenizer.add_special_tokens(ROLE_TOKENS)\n",
    "\n",
    "user_id = tokenizer.token_to_id(\"<|user|>\")\n",
    "assistant_id = tokenizer.token_to_id(\"<|assistant|>\")\n",
    "\n",
    "print(user_id, assistant_id, eot_id)  # 50257 50258 50256\n",
    "\n",
    "tokenizer.decode([user_id, assistant_id, eot_id], skip_special_tokens=False)  # '<|user|><|assistant|><|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841eb3d5-48d3-4403-aa9f-20331683a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Formatting datasets...\n",
      "Combining datasets...\n",
      "Total training examples: 86186\n",
      "\n",
      "Example from combined dataset:\n",
      "{'assistant': '1. Eat a balanced and nutritious diet: Make sure your meals are '\n",
      "              'inclusive of a variety of fruits and vegetables, lean protein, '\n",
      "              'whole grains, and healthy fats. This helps to provide your body '\n",
      "              'with the essential nutrients to function at its best and can '\n",
      "              'help prevent chronic diseases.\\n'\n",
      "              '\\n'\n",
      "              '2. Engage in regular physical activity: Exercise is crucial for '\n",
      "              'maintaining strong bones, muscles, and cardiovascular health. '\n",
      "              'Aim for at least 150 minutes of moderate aerobic exercise or 75 '\n",
      "              'minutes of vigorous exercise each week.\\n'\n",
      "              '\\n'\n",
      "              '3. Get enough sleep: Getting enough quality sleep is crucial '\n",
      "              'for physical and mental well-being. It helps to regulate mood, '\n",
      "              'improve cognitive function, and supports healthy growth and '\n",
      "              'immune function. Aim for 7-9 hours of sleep each night.',\n",
      " 'user': 'Give three tips for staying healthy.'}\n",
      "Tokenizing combined dataset...\n"
     ]
    }
   ],
   "source": [
    "IGNORE_INDEX = -100\n",
    "block_size = 1024\n",
    "\n",
    "# Load multiple datasets\n",
    "print(\"Loading datasets...\")\n",
    "alpaca_cleaned = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "platypus = load_dataset(\"garage-bAInd/Open-Platypus\")\n",
    "no_robots = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    "\n",
    "# Format functions for each dataset\n",
    "def format_alpaca(example):\n",
    "    if example[\"input\"].strip():\n",
    "        user_text = (\n",
    "            f\"{example['instruction']}\\n\\n\"\n",
    "            f\"{example['input']}\"\n",
    "        )\n",
    "    else:\n",
    "        user_text = example[\"instruction\"]\n",
    "    return {\n",
    "        \"user\": user_text,\n",
    "        \"assistant\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "def format_platypus(example):\n",
    "    return {\n",
    "        \"user\": example[\"instruction\"],\n",
    "        \"assistant\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "def format_no_robots(example):\n",
    "    return {\n",
    "        \"user\": example[\"prompt\"],\n",
    "        \"assistant\": example[\"messages\"][1][\"content\"]  # Fixed: assistant message is in messages list\n",
    "    }\n",
    "\n",
    "# Map each dataset to common format\n",
    "print(\"Formatting datasets...\")\n",
    "alpaca_formatted = alpaca_cleaned.map(\n",
    "    format_alpaca, \n",
    "    remove_columns=alpaca_cleaned[\"train\"].column_names\n",
    ")\n",
    "platypus_formatted = platypus.map(\n",
    "    format_platypus, \n",
    "    remove_columns=platypus[\"train\"].column_names\n",
    ")\n",
    "no_robots_formatted = no_robots.map(\n",
    "    format_no_robots, \n",
    "    remove_columns=no_robots[\"train\"].column_names\n",
    ")\n",
    "\n",
    "combined_datasets = [\n",
    "    alpaca_formatted[\"train\"],\n",
    "    platypus_formatted[\"train\"],\n",
    "    no_robots_formatted[\"train\"]\n",
    "]\n",
    "\n",
    "# OPTIONAL : can use any modern LLM to generate custom SFT data\n",
    "if os.path.isfile(\"synthetic_sft_data.jsonl\"):\n",
    "    def load_synthetic_data(file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "        return data\n",
    "    \n",
    "    print(\"Loading synthetic data...\")\n",
    "    synthetic_data = load_synthetic_data(\"synthetic_sft_data.jsonl\")\n",
    "    print(f\"Loaded {len(synthetic_data)} synthetic examples\")\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    synthetic_dataset = Dataset.from_list(synthetic_data)\n",
    "    \n",
    "    # Print example to verify format\n",
    "    print(\"\\nExample from synthetic dataset:\")\n",
    "    pprint(synthetic_dataset[0])\n",
    "\n",
    "    # Add to all datasets\n",
    "    combined_datasets.append(synthetic_dataset)\n",
    "\n",
    "# Combine all datasets\n",
    "print(\"\\n\\nCombining datasets...\")\n",
    "combined_train = concatenate_datasets(combined_datasets)\n",
    "\n",
    "print(f\"Total training examples: {len(combined_train)}\")\n",
    "print(\"\\nExample from combined dataset:\")\n",
    "pprint(next(iter(combined_train)))\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_sft(example):\n",
    "    text = (\n",
    "        \"<|user|>\\n\"\n",
    "        f\"{example['user']}\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        f\"{example['assistant']}\"\n",
    "    )\n",
    "    ids = tokenizer.encode(text).ids\n",
    "    ids.append(eot_id)\n",
    "    \n",
    "    # Find assistant token\n",
    "    try:\n",
    "        assistant_pos = ids.index(assistant_id)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "    # Create labels as shifted version of ids\n",
    "    labels = [IGNORE_INDEX] * len(ids)\n",
    "    labels[:-1] = ids[1:]  # Shift: label[i] = ids[i+1]\n",
    "    \n",
    "    # Now mask out everything before assistant response\n",
    "    labels[:assistant_pos + 1] = [IGNORE_INDEX] * (assistant_pos + 1)\n",
    "    \n",
    "    # Truncate\n",
    "    ids = ids[:block_size]\n",
    "    labels = labels[:block_size]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": ids,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# Tokenize combined dataset\n",
    "print(\"Tokenizing combined dataset...\")\n",
    "combined_tokenized = combined_train.map(\n",
    "    tokenize_sft,\n",
    "    remove_columns=combined_train.column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch):\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    \n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    \n",
    "    for x in batch:\n",
    "        pad_len = max_len - len(x[\"input_ids\"])\n",
    "        input_ids.append(\n",
    "            x[\"input_ids\"] + [eot_id] * pad_len\n",
    "        )\n",
    "        labels.append(\n",
    "            x[\"labels\"] + [IGNORE_INDEX] * pad_len\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a208c59-6353-44f6-8659-f13a1f777738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_bf16_supported() True\n"
     ]
    }
   ],
   "source": [
    "#### CONFIG #####\n",
    "\n",
    "# Basically GPT-2 Small\n",
    "block_size = 1024  # 512 for faster convergence then 1024 to finish training\n",
    "batch_size = 16\n",
    "embed_dim = 768\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "dropout_prob = 0.1\n",
    "mlp_ratio = 4  # standard 4x expansion\n",
    "\n",
    "\n",
    "# Training\n",
    "MAX_STEPS = 500000       # Total number of micro-batches to process\n",
    "GRAD_ACCUM_STEPS = 40    # Accumulate gradients over 40 batches\n",
    "LOG_INTERVAL = 500       # Log every 500 micro-batches\n",
    "num_workers = 4          # For data loading\n",
    "prefetch = 4\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "model_path = f\"gpt_model_{block_size}_final.pt\"  # where do we store trained model\n",
    "print(\"torch.cuda.is_bf16_supported()\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2dc3f0-4902-4404-b046-416806c20f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created with 5387 batches\n"
     ]
    }
   ],
   "source": [
    "# Create dataloader\n",
    "train_loader = DataLoader(\n",
    "    combined_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=prefetch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with {len(train_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27ae4514-68d7-466c-b69e-3480dc0b14c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with config : \n",
      "{'dropout_prob': 0.1,\n",
      " 'embed_dim': 768,\n",
      " 'mlp_ratio': 4,\n",
      " 'num_heads': 12,\n",
      " 'num_layers': 12,\n",
      " 'use_swiglu': True,\n",
      " 'vocab_size': 50257}\n",
      "Number of parameters: 84.95M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(50259, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (norm1): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (norm2): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "        (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_compile_prefix(state_dict, prefix=\"_orig_mod.\"):\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(prefix):\n",
    "            new_state_dict[k[len(prefix):]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"vocab_size\": tokenizer.get_vocab_size() - len(ROLE_TOKENS),\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"mlp_ratio\": mlp_ratio,\n",
    "    \"dropout_prob\": dropout_prob,\n",
    "    \"use_swiglu\": True,\n",
    "}\n",
    "\n",
    "print(\"Initializing model with config : \")\n",
    "pprint(model_config)\n",
    "\n",
    "model = GPT(**model_config).to(device)\n",
    "\n",
    "trained_weights = strip_compile_prefix(torch.load(\"gpt_model_1024_final_60000.pt\", map_location=device))\n",
    "\n",
    "model.load_state_dict(trained_weights, strict=True)\n",
    "\n",
    "# Need to handle the embeddings for the 2 new tokens we added\n",
    "old_vocab_size, dim = model.embedding.weight.shape\n",
    "new_vocab_size = old_vocab_size + len(ROLE_TOKENS)\n",
    "\n",
    "new_embedding = torch.nn.Embedding(new_vocab_size, dim).to(device)\n",
    "new_embedding.weight.data[:old_vocab_size] = model.embedding.weight.data\n",
    "new_embedding.weight.data[old_vocab_size:] = model.embedding.weight.data[eot_id]  # copy paste embedding of end of text to usr/assistant\n",
    "\n",
    "model.embedding = new_embedding\n",
    "model.lm_head.weight = model.embedding.weight\n",
    "\n",
    "# Ready to train ! \n",
    "# model = torch.compile(model)  ## bad idea unless we implement bucketing or fixed size padding\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e581bf-67ec-4db3-9bcd-a1516f736f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=2e-5,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114128/2291286227.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | step 0 | loss 3.6935\n",
      "epoch 0 | step 100 | loss 3.2384\n",
      "epoch 0 | step 200 | loss 2.4780\n",
      "epoch 0 | step 300 | loss 3.1288\n",
      "epoch 0 | step 400 | loss 2.6858\n",
      "epoch 0 | step 500 | loss 2.5710\n",
      "epoch 0 | step 600 | loss 2.7187\n",
      "epoch 0 | step 700 | loss 2.2620\n",
      "epoch 0 | step 800 | loss 2.7491\n",
      "epoch 0 | step 900 | loss 2.7041\n",
      "epoch 0 | step 1000 | loss 2.8460\n",
      "epoch 0 | step 1100 | loss 2.8642\n",
      "epoch 0 | step 1200 | loss 3.2335\n",
      "epoch 0 | step 1300 | loss 2.7360\n",
      "epoch 0 | step 1400 | loss 2.9145\n",
      "epoch 0 | step 1500 | loss 2.9781\n",
      "epoch 0 | step 1600 | loss 3.2000\n",
      "epoch 0 | step 1700 | loss 3.0878\n",
      "epoch 0 | step 1800 | loss 2.8833\n",
      "epoch 0 | step 1900 | loss 2.3682\n",
      "epoch 0 | step 2000 | loss 2.7500\n",
      "epoch 0 | step 2100 | loss 2.8032\n",
      "epoch 0 | step 2200 | loss 2.7443\n",
      "epoch 0 | step 2300 | loss 2.5857\n",
      "epoch 0 | step 2400 | loss 2.7358\n",
      "epoch 0 | step 2500 | loss 2.7681\n",
      "epoch 0 | step 2600 | loss 2.5352\n",
      "epoch 0 | step 2700 | loss 2.5798\n",
      "epoch 0 | step 2800 | loss 3.0915\n",
      "epoch 0 | step 2900 | loss 3.1561\n",
      "epoch 0 | step 3000 | loss 2.9453\n",
      "epoch 0 | step 3100 | loss 2.8691\n",
      "epoch 0 | step 3200 | loss 2.6227\n",
      "epoch 0 | step 3300 | loss 2.7992\n",
      "epoch 0 | step 3400 | loss 2.4706\n",
      "epoch 0 | step 3500 | loss 2.6948\n",
      "epoch 0 | step 3600 | loss 3.0576\n",
      "epoch 0 | step 3700 | loss 2.6945\n",
      "epoch 0 | step 3800 | loss 2.7633\n",
      "epoch 0 | step 3900 | loss 2.4175\n",
      "epoch 0 | step 4000 | loss 2.9774\n",
      "epoch 0 | step 4100 | loss 3.1372\n",
      "epoch 0 | step 4200 | loss 2.6781\n",
      "epoch 0 | step 4300 | loss 2.6855\n",
      "epoch 0 | step 4400 | loss 2.9741\n",
      "epoch 0 | step 4500 | loss 1.9767\n",
      "epoch 0 | step 4600 | loss 2.5929\n",
      "epoch 0 | step 4700 | loss 2.5695\n",
      "epoch 0 | step 4800 | loss 2.2171\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        with autocast(dtype=dtype):\n",
    "            logits = model(input_ids)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels.view(-1),\n",
    "                ignore_index=IGNORE_INDEX,\n",
    "            )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"epoch {epoch} | step {step} | loss {loss.item():.4f}\"\n",
    "            )\n",
    "    \n",
    "    # Save model in full precision\n",
    "    torch.save(model.state_dict(), f\"GPT_SFT_epoch_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d67998ad-5c05-424b-9358-fc72fe99d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Give an example of a hard data mining task.\n",
      "<|assistant|>\n",
      "An example of a hard data mining task is by the following steps: \n",
      "1. What is needed to store the data? \n",
      "2. What is needed to store the data? \n",
      "3. How can you store the data? \n",
      "4. What gives you the right model?<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"<|user|>\\n\"\n",
    "    \"Give an example of a hard data mining task.\\n\"\n",
    "    \"<|assistant|>\\n\"\n",
    ")\n",
    "\n",
    "enc = tokenizer.encode(\n",
    "    prompt,\n",
    ")\n",
    "\n",
    "input_ids = torch.tensor(enc.ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_greedy(\n",
    "    model,\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    stop_token_id=eot_id,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1] / 0.7\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == stop_token_id:\n",
    "            break\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "out = generate_greedy(model, input_ids, max_new_tokens=150)\n",
    "\n",
    "print(tokenizer.decode(out[0].tolist(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b7b8b-ed71-42d5-b703-77662c4dea6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
