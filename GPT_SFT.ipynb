{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2082cb7-8981-48dc-9807-0ee0f47c3e59",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "SFT part : teach the model the assistant format\n",
    "\n",
    "Basically : \n",
    "- the prompt tokens are masked (no loss)\n",
    "- the response tokens are trained on (loss applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107f7396-2d85-483f-90b2-34724d757a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import csv\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment config\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "from datasets import Dataset as ds, concatenate_datasets, load_dataset\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Torch runtime config\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Custom\n",
    "from utils import count_parameters, load_synthetic_data, strip_compile_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7494176-e827-4d1a-b659-a2c313af239c",
   "metadata": {},
   "source": [
    "## Config & Model Definition\n",
    "\n",
    "Mostly the same code as the pretraining notebook, including Flash Attention, RMSNorm etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa17962-52b5-4773-868c-5c87bae6d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_bf16_supported() True\n"
     ]
    }
   ],
   "source": [
    "#### CONFIG #####\n",
    "\n",
    "# Basically GPT-2 Small\n",
    "block_size = 1024\n",
    "batch_size = 16\n",
    "embed_dim = 768\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "dropout_prob = 0  # <!> finetuning, standard practice to disable dropout <!>\n",
    "mlp_ratio = 4  # standard 4x expansion\n",
    "pretrained_weights = \"gpt_model_1024_158417.pt\"\n",
    "\n",
    "# Tokenizer\n",
    "ROLE_TOKENS = [\"<|user|>\", \"<|assistant|>\"]\n",
    "IGNORE_INDEX = -100  # to mask out the loss\n",
    "\n",
    "# Training\n",
    "NUM_EPOCHS = 3  # not too many or we're going to overfit our Q/A data\n",
    "num_workers = 4\n",
    "prefetch = 4\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "print(\"torch.cuda.is_bf16_supported()\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77258703-a407-43dd-ac3b-5f1b9ed2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, causal: bool = True, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "        \n",
    "        self.causal = causal\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout_p = dropout\n",
    "        \n",
    "        # Fused QKV projection: 3x the output size\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        \n",
    "        self.rotary_emb = RotaryEmbedding(dim=self.head_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, k_v_cache=None):\n",
    "        B, T, _ = x.shape\n",
    "        using_cache = k_v_cache is not None and \"K\" in k_v_cache\n",
    "    \n",
    "        # 1. Single fused projection\n",
    "        if using_cache:\n",
    "            x_q = x[:, -1:, :]\n",
    "            qkv = self.qkv_proj(x_q)  # (B, 1, 3 x embed_dim)\n",
    "        else:\n",
    "            qkv = self.qkv_proj(x)  # (B, T, 3 x embed_dim)\n",
    "        \n",
    "        # 2. Split into Q, K, V\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)  # Each is (B, T, embed_dim)\n",
    "        \n",
    "        def split_heads(t):\n",
    "            return t.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 3. Split heads -> (B, H, T, D_head)\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "    \n",
    "        # 4. Apply RoPE \n",
    "        if using_cache:\n",
    "            past_len = k_v_cache[\"K\"].shape[-2]\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q, offset=past_len)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K, offset=past_len)\n",
    "            \n",
    "            K = torch.cat([k_v_cache[\"K\"], K], dim=-2)\n",
    "            V = torch.cat([k_v_cache[\"V\"], V], dim=-2)\n",
    "        else:\n",
    "            Q = self.rotary_emb.rotate_queries_or_keys(Q)\n",
    "            K = self.rotary_emb.rotate_queries_or_keys(K)\n",
    "    \n",
    "        # 5. Update cache\n",
    "        if k_v_cache is not None:\n",
    "            k_v_cache[\"K\"] = K\n",
    "            k_v_cache[\"V\"] = V\n",
    "    \n",
    "        # 6. Flash Attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            query=Q,\n",
    "            key=K,\n",
    "            value=V,\n",
    "            attn_mask=None, \n",
    "            dropout_p=self.dropout_p if self.training else 0.0,\n",
    "            is_causal=self.causal and (Q.shape[-2] > 1)\n",
    "        )\n",
    "        \n",
    "        # 7. Merge heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, -1, self.embed_dim)\n",
    "\n",
    "        # 8. Linear projection\n",
    "        return self.out_proj(out), k_v_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf4ba2d-402d-4eab-86e1-754f9faeaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim=None, dropout_prob=0.1, use_swiglu=True):\n",
    "        super().__init__()\n",
    "        self.use_swiglu = use_swiglu\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * embed_dim\n",
    "        # https://arxiv.org/pdf/2002.05202\n",
    "        # We offer no explanation as to why these\n",
    "        # architectures seem to work; \n",
    "        # we attribute their success, as all else, to divine benevolence.\n",
    "        if self.use_swiglu:\n",
    "            # Adjust hidden_dim to ~ match baseline # of parameters\n",
    "            hidden_dim = int(2 * hidden_dim / 3)\n",
    "            self.gate_proj = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.up_proj = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "            self.act = nn.GELU()\n",
    "            self.linear2 = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_swiglu:\n",
    "            out = self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "        else:\n",
    "            out = self.linear2(self.act(self.linear1(x)))\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c04d073-6e3f-469d-98b4-5cda92115923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 causal=True,\n",
    "                 use_swiglu=True,\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Initialize a complete transformer block.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Multi-head self-attention for sequence modeling\n",
    "        2. 1st Normalization (pre-norm architecture)\n",
    "        3. MLP with specified expansion ratio\n",
    "        4. 2nd Normalization\n",
    "    \n",
    "        TRANSFORMER BLOCK ARCHITECTURE:\n",
    "        x → Norm → MultiHeadAttention → + (residual) →\n",
    "            Norm → MLP → + (residual) → output\n",
    "    \n",
    "        NB: We use pre-norm architecture (before attention/MLP)\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "        self.norm1 = nn.RMSNorm(embed_dim)  # Nb : modern architectures seem to use rmsnorm instead\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads, causal, dropout_prob)  # causal = masking out tokens\n",
    "        self.norm2 = nn.RMSNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio * embed_dim, dropout_prob, use_swiglu)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        x1 = self.norm1(x)\n",
    "        x2, cache = self.mha(x1, cache)  # will be used when generating tokens during inference\n",
    "        x2 = x2 + x  # residual path\n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.mlp(x3) + x2  # residual path\n",
    "        return x3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d94e28-950a-4561-a935-a2a0145ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Generative Pre-trained Transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 dropout_prob=0.1,\n",
    "                 use_swiglu=True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize complete GPT model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout_prob, use_swiglu) for _ in range(num_layers)])\n",
    "        self.norm = nn.RMSNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # weight tying\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.embedding(tokens)\n",
    "        x = self.dropout(embeddings)\n",
    "        for b in self.blocks:\n",
    "            x, _ = b(x)\n",
    "        features = self.norm(x)\n",
    "        return self.lm_head(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed8616-4de7-4595-afcb-8b36c3dea3f9",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233fbc6d-4607-4bdc-bc51-2bd536a68ba2",
   "metadata": {},
   "source": [
    "Must be the exact same used for pretraining, on top just add 2 extra tokens for assistant / user roles and assign these the same embedding we learnt during pretraining as the end of text token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cbeb30-9756-4a6f-a1a4-a812ba79d320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257 50258 50256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|user|><|assistant|><|endoftext|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to re-use the exact same tokenizer for SFT + 2 new tokens for conversation\n",
    "tokenizer = Tokenizer.from_pretrained(\"GPT2\")\n",
    "eot_id = tokenizer.token_to_id(\"<|endoftext|>\")\n",
    "\n",
    "tokenizer.add_special_tokens(ROLE_TOKENS)\n",
    "user_id = tokenizer.token_to_id(\"<|user|>\")\n",
    "assistant_id = tokenizer.token_to_id(\"<|assistant|>\")\n",
    "\n",
    "print(user_id, assistant_id, eot_id)  # 50257 50258 50256\n",
    "tokenizer.decode([user_id, assistant_id, eot_id], skip_special_tokens=False)  # '<|user|><|assistant|><|endoftext|>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925259a-61f9-4bde-aa95-f59755dc5478",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning Dataset\n",
    "\n",
    "### Goal\n",
    "Train the model to generate assistant responses, NOT to predict instructions -> Autoregressive LMs predict the NEXT token at each position. We use this by masking instruction tokens in the loss calculation.\n",
    "\n",
    "---\n",
    "\n",
    "### The Process\n",
    "\n",
    "#### 1. Format the Data\n",
    "```\n",
    "Input text: \"<|user|>\\n{instruction}\\n<|assistant|>\\n{response}\"\n",
    "\n",
    "```\n",
    "\n",
    "#### 2. Tokenize\n",
    "```\n",
    "ids = [user_tok, What, is, 2, +, 2, ?, asst_tok, The, answer, is, 4, eot]\n",
    "idx:   0        1     2   3  4  5  6  7         8    9      10  11 12\n",
    "```\n",
    "\n",
    "#### 3. Create Shifted Labels\n",
    "```python\n",
    "labels[:-1] = ids[1:]  # Each label is the NEXT token to predict\n",
    "\n",
    "labels = [What, is, 2, +, 2, ?, asst_tok, The, answer, is, 4, eot, IGNORE]\n",
    "```\n",
    "**Meaning:** `labels[i]` = what should be predicted after seeing `ids[i]`\n",
    "\n",
    "#### 4. Mask the Instruction\n",
    "```python\n",
    "# Find position of <|assistant|> token (position 7 in example)\n",
    "labels[:assistant_pos+1] = IGNORE_INDEX (-100)\n",
    "\n",
    "labels = [IGN, IGN, IGN, IGN, IGN, IGN, IGN, The, answer, is, 4, eot, IGN]\n",
    "          └─────────instruction masked───────────┘  └────train here────┘\n",
    "```\n",
    "\n",
    "#### 5. Compute Loss (during training)\n",
    "```python\n",
    "logits = model(ids)  # Model predicts next token at each position\n",
    "loss = CrossEntropyLoss(logits, labels, ignore_index=-100)\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- Position 0-6: `labels[i] = -100` → loss ignored (don't train on instruction)\n",
    "- Position 7: predict \"The\" after `<|assistant|>` → **COMPUTE LOSS** ✓\n",
    "- Position 8: predict \"answer\" after \"The\" → **COMPUTE LOSS** ✓\n",
    "- Position 9: predict \"is\" after \"answer\" → **COMPUTE LOSS** ✓\n",
    "- Position 10: predict \"4\" after \"is\" → **COMPUTE LOSS** ✓\n",
    "- Position 11: predict `<eot>` after \"4\" → **COMPUTE LOSS** ✓\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "**Causal Attention Mask**\n",
    "- Prevents the model from \"seeing\" future tokens\n",
    "- At position i, model only attends to tokens 0 to i\n",
    "\n",
    "**Teacher Forcing**\n",
    "- Model sees correct previous tokens during training\n",
    "- Learns to predict the next one\n",
    "\n",
    "**Masking with -100**\n",
    "- `CrossEntropyLoss` ignores these positions\n",
    "- Gradients only flow through response tokens\n",
    "\n",
    "**Result:** Model learns \"given instruction X, generate response Y\" without wasting compute trying to predict the instruction itself.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Facts\n",
    "\n",
    "- `IGNORE_INDEX = -100` (standard PyTorch convention)\n",
    "- Only ~5-20% of tokens typically contribute to loss (just the responses)\n",
    "- The shift (`labels[i] = ids[i+1]`) aligns predictions with targets\n",
    "- The masking + smaller size dataset is going to finetune behavior but not (or barely) knowledge !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841eb3d5-48d3-4403-aa9f-20331683a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Formatting datasets...\n",
      "Loading synthetic data...\n",
      "Loaded 21 synthetic examples\n",
      "\n",
      "Example from synthetic dataset:\n",
      "{'assistant': '**\\n'\n",
      "              '\\n'\n",
      "              'To solve for the initial investment $X$, we must break the '\n",
      "              'problem down into sequential steps, calculating the value of '\n",
      "              'each component in terms of $X$.\\n'\n",
      "              '\\n'\n",
      "              '### **Step 1: Calculate Revenue from Inventory Sales**\\n'\n",
      "              '*   Initial Inventory Cost = $0.60X$\\n'\n",
      "              '*   Value of sellable inventory (after $10\\\\%$ loss) = $0.60X '\n",
      "              '\\\\times (1 - 0.10) = 0.54X$\\n'\n",
      "              '*   The markup is $150\\\\%$, which means the sales multiplier is '\n",
      "              '$2.5$ ($100\\\\%$ original cost + $150\\\\%$ markup).\\n'\n",
      "              '*   **Total Sales Revenue** = $0.54X \\\\times 2.5 = 1.35X$\\n'\n",
      "              '\\n'\n",
      "              '### **Step 2: Calculate the Marketing Rebate**\\n'\n",
      "              '*   Initial Marketing Cost = $0.40X$\\n'\n",
      "              '*   Rebate percentage = $15\\\\%$\\n'\n",
      "              '*   **Marketing Rebate** = $0.40X \\\\times 0.15 = 0.06X$\\n'\n",
      "              '\\n'\n",
      "              '### **Step 3: Calculate Total Cash Inflow before Taxes**\\n'\n",
      "              '*   Total Cash = Sales Revenue + Marketing Rebate\\n'\n",
      "              '*   **Total Cash Inflow** = $1.35X + 0.06X = 1.41X$\\n'\n",
      "              '\\n'\n",
      "              '### **Step 4: Determine Gross Profit and Tax Amount**\\n'\n",
      "              'As defined in the problem, Gross Profit is the Total Cash '\n",
      "              'Inflow minus the Total Initial Investment.\\n'\n",
      "              '*   Gross Profit = $1.41X - X = 0.41X$\\n'\n",
      "              '*   The tax is $20\\\\%$ of the Gross Profit.\\n'\n",
      "              '*   **Tax Amount** = $0.20 \\\\times 0.41X = 0.082X$\\n'\n",
      "              '\\n'\n",
      "              '### **Step 5: Set up the equation for Final Remaining Cash**\\n'\n",
      "              'The entrepreneur’s final cash is the Total Cash Inflow (from '\n",
      "              'Step 3) minus the Tax Amount (from Step 4).\\n'\n",
      "              '*   Final Cash = $1.41X - 0.082X$\\n'\n",
      "              '*   Final Cash = $1.328X$\\n'\n",
      "              '\\n'\n",
      "              'We are given that the final cash is $166,000:\\n'\n",
      "              '*   $1.328X = 166,000$\\n'\n",
      "              '\\n'\n",
      "              '### **Step 6: Solve for X**\\n'\n",
      "              '*   $X = \\\\frac{166,000}{1.328}$\\n'\n",
      "              '*   $X = 125,000$\\n'\n",
      "              '\\n'\n",
      "              '**Final Answer:**\\n'\n",
      "              'The initial investment was **$125,000**.',\n",
      " 'user': '**** \\n'\n",
      "         '\\n'\n",
      "         'An entrepreneur starts a boutique electronics company with an '\n",
      "         'initial investment of $X$ dollars. He allocates the capital as '\n",
      "         'follows: $60\\\\%$ is spent on manufacturing inventory and $40\\\\%$ is '\n",
      "         'spent on a global marketing campaign. \\n'\n",
      "         '\\n'\n",
      "         'The business experiences the following outcomes by the end of the '\n",
      "         'first quarter:\\n'\n",
      "         '1.  **Inventory:** Due to a warehouse accident, $10\\\\%$ of the '\n",
      "         'manufactured inventory is damaged and becomes unsellable. The '\n",
      "         'remaining $90\\\\%$ of the inventory is sold at a markup of $150\\\\%$ '\n",
      "         '(meaning the selling price is $2.5$ times the manufacturing cost).\\n'\n",
      "         '2.  **Marketing:** The marketing campaign is highly efficient, and '\n",
      "         'the company receives a \"performance rebate\" equal to $15\\\\%$ of the '\n",
      "         'original marketing spend.\\n'\n",
      "         '3.  **Taxation:** The government levies a $20\\\\%$ \"Success Tax\" '\n",
      "         \"specifically on the company's **Gross Profit**. Gross Profit is \"\n",
      "         'defined as: *(Total Revenue from Sales + Marketing Rebate) – (Total '\n",
      "         'Initial Investment)*.\\n'\n",
      "         '\\n'\n",
      "         'After paying the tax, the entrepreneur has a total of **$166,000** '\n",
      "         'in cash remaining. What was the initial investment ($X$)?\\n'\n",
      "         '\\n'\n",
      "         '***\\n'\n",
      "         '\\n'\n",
      "         '**'}\n",
      "\n",
      "\n",
      "Combining datasets...\n",
      "Total training examples: 86207\n",
      "\n",
      "Example from combined dataset:\n",
      "{'assistant': '1. Eat a balanced and nutritious diet: Make sure your meals are '\n",
      "              'inclusive of a variety of fruits and vegetables, lean protein, '\n",
      "              'whole grains, and healthy fats. This helps to provide your body '\n",
      "              'with the essential nutrients to function at its best and can '\n",
      "              'help prevent chronic diseases.\\n'\n",
      "              '\\n'\n",
      "              '2. Engage in regular physical activity: Exercise is crucial for '\n",
      "              'maintaining strong bones, muscles, and cardiovascular health. '\n",
      "              'Aim for at least 150 minutes of moderate aerobic exercise or 75 '\n",
      "              'minutes of vigorous exercise each week.\\n'\n",
      "              '\\n'\n",
      "              '3. Get enough sleep: Getting enough quality sleep is crucial '\n",
      "              'for physical and mental well-being. It helps to regulate mood, '\n",
      "              'improve cognitive function, and supports healthy growth and '\n",
      "              'immune function. Aim for 7-9 hours of sleep each night.',\n",
      " 'user': 'Give three tips for staying healthy.'}\n",
      "Tokenizing combined dataset...\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "# Load multiple datasets\n",
    "print(\"Loading datasets...\")\n",
    "alpaca_cleaned = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "platypus = load_dataset(\"garage-bAInd/Open-Platypus\")\n",
    "no_robots = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    "\n",
    "# Format functions for each dataset\n",
    "def format_alpaca(example):\n",
    "    if example[\"input\"].strip():\n",
    "        user_text = (\n",
    "            f\"{example['instruction']}\\n\\n\"\n",
    "            f\"{example['input']}\"\n",
    "        )\n",
    "    else:\n",
    "        user_text = example[\"instruction\"]\n",
    "    return {\n",
    "        \"user\": user_text,\n",
    "        \"assistant\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "def format_platypus(example):\n",
    "    return {\n",
    "        \"user\": example[\"instruction\"],\n",
    "        \"assistant\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "def format_no_robots(example):\n",
    "    return {\n",
    "        \"user\": example[\"prompt\"],\n",
    "        \"assistant\": example[\"messages\"][1][\"content\"]\n",
    "    }\n",
    "\n",
    "# Map each dataset to common format\n",
    "print(\"Formatting datasets...\")\n",
    "alpaca_formatted = alpaca_cleaned.map(\n",
    "    format_alpaca, \n",
    "    remove_columns=alpaca_cleaned[\"train\"].column_names\n",
    ")\n",
    "platypus_formatted = platypus.map(\n",
    "    format_platypus, \n",
    "    remove_columns=platypus[\"train\"].column_names\n",
    ")\n",
    "no_robots_formatted = no_robots.map(\n",
    "    format_no_robots, \n",
    "    remove_columns=no_robots[\"train\"].column_names\n",
    ")\n",
    "\n",
    "combined_datasets = [\n",
    "    alpaca_formatted[\"train\"],\n",
    "    platypus_formatted[\"train\"],\n",
    "    no_robots_formatted[\"train\"]\n",
    "]\n",
    "\n",
    "# OPTIONAL : can use any modern LLM to generate custom SFT data\n",
    "if os.path.isfile(\"synthetic_sft_data.jsonl\"):\n",
    "    print(\"Loading synthetic data...\")\n",
    "    synthetic_data = load_synthetic_data(\"synthetic_sft_data.jsonl\")\n",
    "    print(f\"Loaded {len(synthetic_data)} synthetic examples\")\n",
    "    \n",
    "    # Transform list of dicts to dict of lists\n",
    "    data_dict = {}\n",
    "    for key in synthetic_data[0].keys():\n",
    "        data_dict[key] = [item[key] for item in synthetic_data]\n",
    "    \n",
    "    synthetic_dataset = ds.from_dict(data_dict)\n",
    "    \n",
    "    # Print example to verify format\n",
    "    print(\"\\nExample from synthetic dataset:\")\n",
    "    pprint(synthetic_dataset[0])\n",
    "\n",
    "    # Add to all datasets\n",
    "    combined_datasets.append(synthetic_dataset)\n",
    "\n",
    "# Combine all datasets\n",
    "print(\"\\n\\nCombining datasets...\")\n",
    "combined_train = concatenate_datasets(combined_datasets)\n",
    "\n",
    "print(f\"Total training examples: {len(combined_train)}\")\n",
    "print(\"\\nExample from combined dataset:\")\n",
    "pprint(next(iter(combined_train)))\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_sft(example):\n",
    "    text = (\n",
    "        \"<|user|>\\n\"\n",
    "        f\"{example['user']}\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        f\"{example['assistant']}\"\n",
    "    )\n",
    "    ids = tokenizer.encode(text).ids\n",
    "    ids.append(eot_id)\n",
    "    \n",
    "    # Find assistant token\n",
    "    try:\n",
    "        assistant_pos = ids.index(assistant_id)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "    # Create labels as shifted version of ids\n",
    "    labels = [IGNORE_INDEX] * len(ids)\n",
    "    labels[:-1] = ids[1:]  # Shift: label[i] = ids[i+1]\n",
    "    \n",
    "    # Now mask out everything before assistant response\n",
    "    labels[:assistant_pos + 1] = [IGNORE_INDEX] * (assistant_pos + 1)\n",
    "    \n",
    "    # Truncate\n",
    "    ids = ids[:block_size]\n",
    "    labels = labels[:block_size]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": ids,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# Tokenize combined dataset\n",
    "print(\"Tokenizing combined dataset...\")\n",
    "combined_tokenized = combined_train.map(\n",
    "    tokenize_sft,\n",
    "    remove_columns=combined_train.column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2dc3f0-4902-4404-b046-416806c20f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created with 5388 batches of 16 seqs\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    # Inputs from user / assistant conversations are of variable length -> pad for training\n",
    "    # To squeeze out performance, can assign inputs to buckets or pad with fixed len -> compile model\n",
    "    # Here the data is reasonably sized so we can skip\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    \n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    \n",
    "    for x in batch:\n",
    "        pad_len = max_len - len(x[\"input_ids\"])\n",
    "        input_ids.append(\n",
    "            x[\"input_ids\"] + [eot_id] * pad_len\n",
    "        )\n",
    "        labels.append(\n",
    "            x[\"labels\"] + [IGNORE_INDEX] * pad_len\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    combined_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=prefetch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with {len(train_loader)} batches of {batch_size} seqs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237d148-f21d-45e7-81dc-7b7d5543a14d",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Start from pretrained model, set `dropout` to 0, extend embedding table to the new role tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27ae4514-68d7-466c-b69e-3480dc0b14c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with config : \n",
      "{'dropout_prob': 0,\n",
      " 'embed_dim': 768,\n",
      " 'mlp_ratio': 4,\n",
      " 'num_heads': 12,\n",
      " 'num_layers': 12,\n",
      " 'use_swiglu': True,\n",
      " 'vocab_size': 50257}\n",
      "Loading : gpt_model_1024_158417.pt...\n",
      "Loaded pretrained weights\n",
      "Parameter Breakdown:\n",
      "==================================================\n",
      "embeddings          :   38,598,912 (31.24%)\n",
      "norms               :       19,200 ( 0.02%)\n",
      "other               :   28,311,936 (22.91%)\n",
      "mlp                 :   56,623,104 (45.83%)\n",
      "==================================================\n",
      "TOTAL               :  123,553,152\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"vocab_size\": tokenizer.get_vocab_size() - len(ROLE_TOKENS),\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"mlp_ratio\": mlp_ratio,\n",
    "    \"dropout_prob\": dropout_prob,\n",
    "    \"use_swiglu\": True,\n",
    "}\n",
    "\n",
    "print(\"Initializing model with config : \")\n",
    "pprint(model_config)\n",
    "\n",
    "model = GPT(**model_config).to(device)\n",
    "\n",
    "print(f\"Loading : {pretrained_weights}...\")\n",
    "trained_weights = strip_compile_prefix(torch.load(pretrained_weights, map_location=device))  # handles naming in case of compiled model\n",
    "\n",
    "model.load_state_dict(trained_weights, strict=True)\n",
    "print(\"Loaded pretrained weights\")\n",
    "\n",
    "# Need to handle the embeddings for the 2 new tokens we added\n",
    "old_vocab_size, dim = model.embedding.weight.shape\n",
    "new_vocab_size = old_vocab_size + len(ROLE_TOKENS)\n",
    "\n",
    "new_embedding = torch.nn.Embedding(new_vocab_size, dim).to(device)\n",
    "new_embedding.weight.data[:old_vocab_size] = model.embedding.weight.data\n",
    "new_embedding.weight.data[old_vocab_size:] = model.embedding.weight.data[eot_id]  # copy paste embedding of end of text to usr/assistant\n",
    "\n",
    "model.embedding = new_embedding\n",
    "model.lm_head.weight = model.embedding.weight\n",
    "\n",
    "# Ready to train ! \n",
    "# model = torch.compile(model)  ## bad idea unless we implement bucketing or fixed size padding\n",
    "model.train()\n",
    "_, _ = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ff922-468a-49b1-a8c7-26230af36f68",
   "metadata": {},
   "source": [
    "Weight decay is quite huge compared to habitual CNN but seems to be the standard for LLMs (empirical evidence), usually 0.1 for pretraining, 0.01 for SFT, helps avoiding memorization etc. Small lr because we're finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e581bf-67ec-4db3-9bcd-a1516f736f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=2e-5,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=0.01,  # good practice\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3a017-b03e-4e16-9e46-169a2e6fc1d4",
   "metadata": {},
   "source": [
    "We train over `epochs` because SFT data is 1) manageable 2) fixed Q/A samples while pretraining data is intractable and random contiguous chunk of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7202c6b-3c1d-4fe2-a5ac-e47318851137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | step 0 | loss 2.7657\n",
      "epoch 0 | step 100 | loss 2.2716\n",
      "epoch 0 | step 200 | loss 2.2547\n",
      "epoch 0 | step 300 | loss 2.2833\n",
      "epoch 0 | step 400 | loss 2.5284\n",
      "epoch 0 | step 500 | loss 2.1692\n",
      "epoch 0 | step 600 | loss 2.1431\n",
      "epoch 0 | step 700 | loss 2.4482\n",
      "epoch 0 | step 800 | loss 2.5566\n",
      "epoch 0 | step 900 | loss 2.0463\n",
      "epoch 0 | step 1000 | loss 2.4977\n",
      "epoch 0 | step 1100 | loss 2.1114\n",
      "epoch 0 | step 1200 | loss 2.3105\n",
      "epoch 0 | step 1300 | loss 2.1581\n",
      "epoch 0 | step 1400 | loss 2.3696\n",
      "epoch 0 | step 1500 | loss 2.4273\n",
      "epoch 0 | step 1600 | loss 2.4766\n",
      "epoch 0 | step 1700 | loss 2.1687\n",
      "epoch 0 | step 1800 | loss 2.4495\n",
      "epoch 0 | step 1900 | loss 2.5453\n",
      "epoch 0 | step 2000 | loss 2.2902\n",
      "epoch 0 | step 2100 | loss 1.8036\n",
      "epoch 0 | step 2200 | loss 1.8021\n",
      "epoch 0 | step 2300 | loss 1.9774\n",
      "epoch 0 | step 2400 | loss 2.0191\n",
      "epoch 0 | step 2500 | loss 1.9997\n",
      "epoch 0 | step 2600 | loss 1.9025\n",
      "epoch 0 | step 2700 | loss 2.6555\n",
      "epoch 0 | step 2800 | loss 2.2636\n",
      "epoch 0 | step 2900 | loss 2.2412\n",
      "epoch 0 | step 3000 | loss 1.8183\n",
      "epoch 0 | step 3100 | loss 1.7996\n",
      "epoch 0 | step 3200 | loss 2.6079\n",
      "epoch 0 | step 3300 | loss 1.8475\n",
      "epoch 0 | step 3400 | loss 2.1349\n",
      "epoch 0 | step 3500 | loss 1.9058\n",
      "epoch 0 | step 3600 | loss 1.8081\n",
      "epoch 0 | step 3700 | loss 2.4101\n",
      "epoch 0 | step 3800 | loss 2.1448\n",
      "epoch 0 | step 3900 | loss 2.4242\n",
      "epoch 0 | step 4000 | loss 2.3253\n",
      "epoch 0 | step 4100 | loss 2.3683\n",
      "epoch 0 | step 4200 | loss 2.3189\n",
      "epoch 0 | step 4300 | loss 2.0630\n",
      "epoch 0 | step 4400 | loss 2.4755\n",
      "epoch 0 | step 4500 | loss 2.0363\n",
      "epoch 0 | step 4600 | loss 2.6059\n",
      "epoch 0 | step 4700 | loss 1.9836\n",
      "epoch 0 | step 4800 | loss 2.3246\n",
      "epoch 0 | step 4900 | loss 2.1741\n",
      "epoch 0 | step 5000 | loss 2.2022\n",
      "epoch 0 | step 5100 | loss 2.1338\n",
      "epoch 0 | step 5200 | loss 2.1518\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         input_ids = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m         labels = batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, dtype=dtype):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=dtype):\n",
    "            logits = model(input_ids)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels.view(-1),\n",
    "                ignore_index=IGNORE_INDEX,\n",
    "            )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"epoch {epoch} | step {step} | loss {loss.item():.4f}\"\n",
    "            )\n",
    "    \n",
    "    # Save model in full precision\n",
    "    torch.save(model.state_dict(), f\"GPT_SFT_epoch_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67998ad-5c05-424b-9358-fc72fe99d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Explain why sorting by hash is a bad idea.\n",
      "<|assistant|>\n",
      "Dictionaries can be unethical, depending on their reasoning. While hash is a good idea, it can be harmful for many people. For example, if you try to sell hash books to someone else, you can sell them to someone else. This makes it difficult for both your likely and future users to know how much they have in store for you.\n",
      "\n",
      "Dictionaries can also be harmful to certain groups of people, such as children or teenagers. For instance, if you try to sell your family's home to someone else, you can sell it to someone else for the same purpose. This can make it difficult for many people to know the exact value you want them to know about your decision.\n",
      "\n",
      "Dictionaries are also harmful\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"<|user|>\\n\"\n",
    "    \"Explain why sorting by hash is a bad idea.\\n\"\n",
    "    \"<|assistant|>\\n\"\n",
    ")\n",
    "\n",
    "enc = tokenizer.encode(\n",
    "    prompt,\n",
    ")\n",
    "\n",
    "input_ids = torch.tensor(enc.ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    stop_token_id=eot_id,\n",
    "    temperature=0.7,\n",
    "    greedy=False,\n",
    "):\n",
    "    model.eval()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop to block_size if needed\n",
    "        input_ids_cond = input_ids if input_ids.shape[1] <= block_size else input_ids[:, -block_size:]\n",
    "        \n",
    "        # Get logits for next token\n",
    "        logits = model(input_ids_cond)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        if greedy:\n",
    "            # Greedy decoding\n",
    "            next_token = next_token_logits.argmax(dim=-1, keepdim=True)\n",
    "        else:\n",
    "            # Sample with temperature\n",
    "            probs = F.softmax(next_token_logits / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append sampled token\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        # Stop if we hit the stop token\n",
    "        if stop_token_id is not None and (next_token == stop_token_id).all():\n",
    "            break\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# Hopefully 1) it follows the conversation workflow 2) it's not slop !\n",
    "out = generate(model, input_ids, max_new_tokens=150)\n",
    "print(tokenizer.decode(out[0].tolist(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628d8da-e6fc-4e40-bf7c-5e3a20e22bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
